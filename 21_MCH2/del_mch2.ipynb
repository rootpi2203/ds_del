{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH2\n",
    "Fachdozent: Martin Melchior     \n",
    "Student: Manuel Schwarz   \n",
    "HS23\n",
    "\n",
    "Dieses Notebook bearbeitet die Mini-Challenge 2 des Moduls Deep Learning (del).   \n",
    "Die Performance der Modelle wurde mit **wandb.ai** aufgezeichnet und kann [hier](https://wandb.ai/manuel-schwarz/del-mc2/workspace?workspace=user-manuel-schwarz) eingesehen werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Aufgabenstellung:</b> Eine Blaue Box beschreibt die Aufgabe aus der Aufgabenstellung 'SGDS_DEL_MC1.pdf' \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Antworte:</b> Eine Grüne Box beschreibt die Bearbeitung / Reflektion der Aufgabenstellung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import spacy  # conda install -c conda-forge spacy + python -m spacy download en_core_web_sm\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau Modellierung und Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Überlege Dir, welche Modell-Architektur Sinn machen könnte. Mindestens zwei Modell-Varianten sollen aufgebaut werden, die miteinander verglichen werden sollen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Für die del-MC2 Challenge wird das Modell vom Paper Vinyals et al `Show and Tell: A Neural Image Caption Generator` nachgebaut. Das Paper entwickelte ein Modell welches für Bilder eine Bildbeschreibung erstellt. Für die verwendeten Daten wird das `Flickr 8k` Datenset verwendet. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Flickr 8k lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = './data/Images'\n",
    "captions_file = './data/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_captions = pd.read_csv('./data/captions.txt', sep='\\t', header=None)\n",
    "pd_captions.columns = ['full_caption']\n",
    "pd_captions[['image_name', 'caption']] = pd_captions['full_caption'].str.split(',', n=1, expand=True)\n",
    "pd_captions.to_csv('./data/pd_captions.csv', index=False)\n",
    "pd_captions.drop('full_caption', axis=1, inplace=True)\n",
    "pd_captions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `caption.txt` File ist der Bildnamen und die Bildbeschreibung (caption) hinterlegt. Pro Bild stehen fünf Captions zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{pd_captions.image_name[image_id]}'\n",
    "example_caption1 = pd_captions.caption[image_id+0]\n",
    "example_caption2 = pd_captions.caption[image_id+1]\n",
    "example_caption3 = pd_captions.caption[image_id+2]\n",
    "example_caption4 = pd_captions.caption[image_id+3]\n",
    "example_caption5 = pd_captions.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bilddaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder_path = './data/Images/'\n",
    "\n",
    "resolutions = []\n",
    "for image_filename in os.listdir(images_folder_path):\n",
    "    if image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        image_path = os.path.join(images_folder_path, image_filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            resolutions.append(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_labels = [f\"{w}x{h}\" for w, h in resolutions]\n",
    "resolution_counts = Counter(dimension_labels)\n",
    "sorted_resolution_counts = dict(resolution_counts.most_common(50))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(sorted_resolution_counts.keys(), sorted_resolution_counts.values(), color='skyblue')\n",
    "\n",
    "plt.title('Verteilung der Bildauflösungen von Flickr 8k (Top 50)')\n",
    "plt.xlabel('Dimension (b x w)')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths, heights = zip(*resolutions)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(widths, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildbreiten')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['b'])\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.boxplot(heights, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildhöhen')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['h'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bildbeschreibungen\n",
    "\n",
    "Textdaten werden folgend in Tokens konvergiert:\n",
    "[doku torchtext](http://man.hubwiz.com/docset/torchtext.docset/Contents/Resources/Documents/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show torchtext\n",
    "# Version: 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "special_cases = [(\"<start>\", [{ORTH: \"<start>\"}]), (\"<end>\", [{ORTH: \"<end>\"}]), (\"<pad>\", [{ORTH: \"<pad>\"}])]\n",
    "for case in special_cases:\n",
    "    spacy_en.tokenizer.add_special_case(*case)\n",
    "\n",
    "def tokenize_en(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(caption)]\n",
    "    else:\n",
    "        return [tok.text for tok in spacy_en.tokenizer(caption)]    \n",
    "\n",
    "print(f'Test tokenize: {example_caption1}')\n",
    "tokens = tokenize_en(example_caption1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste mit allen vorhandenen Tokens \n",
    "token_series = pd_captions['caption'].apply(tokenize_en).explode()\n",
    "count_token = token_series.value_counts()\n",
    "count_token.index.name = 'token'\n",
    "count_token = count_token.reset_index()\n",
    "# count_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = 50\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.bar(count_token.head(num_token).token, count_token.head(num_token).caption, color='skyblue')\n",
    "plt.title(f'Total Tokens: {len(count_token)} Tokens, dargestellt {num_token} Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en_len(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return len([tok.text.lower() for tok in spacy_en.tokenizer(caption)])\n",
    "    else:\n",
    "        return len([tok.text for tok in spacy_en.tokenizer(caption)])\n",
    "\n",
    "token_series = pd_captions['caption'].apply(tokenize_en_len)\n",
    "# token_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3))\n",
    "token_mean = token_series.mean()\n",
    "token_std = token_series.std() / 2\n",
    "plt.hist(token_series, color='skyblue', bins=40)\n",
    "plt.axvline(token_mean, color='red', alpha=0.6, label=f'mean {token_mean:0.2f}')\n",
    "plt.axvspan(token_mean-token_std, token_mean+token_std, color='grey', alpha=0.2, label=f'std {token_std:0.2f}')\n",
    "plt.suptitle('Verteilung der Anzahl Tokens je Caption')\n",
    "plt.title(f'min bei: {token_series.min()}, max bei: {token_series.max()}', fontsize=8)\n",
    "plt.xlabel('Anzahl Tokens')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling\n",
    "\n",
    "- Im Paper werden Tokens die weniger als fünf mal vorkommen entfernt\n",
    "- Im Paper werden `start` und `end` Token eingeführt, diese sollen dem Modell helfen zu erkennen, wann der Beginn der Generierung einer Beschreibung ist und wann sie beendet werden sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_token = 5\n",
    "count_token_filtered = count_token[count_token.caption >= min_num_token]\n",
    "print(f'Total Tokens {len(count_token)}')\n",
    "print(f'Anzahl Tokens die mehr als {min_num_token} vorkommen: {len(count_token_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "\n",
    "pd_caption_mod = pd_captions.copy()\n",
    "pd_caption_mod['caption'] = pd_caption_mod['caption'].apply(lambda x: f\"{START_TOKEN} {x} {END_TOKEN}\")\n",
    "pd_captions.to_csv('./data/pd_captions_mod.csv', index=False)\n",
    "\n",
    "print(pd_captions['caption'][0])\n",
    "print(pd_captions['caption'][1])\n",
    "print(pd_captions['caption'][2])\n",
    "print()\n",
    "print(pd_caption_mod['caption'][0])\n",
    "print(pd_caption_mod['caption'][1])\n",
    "print(pd_caption_mod['caption'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Captions: {len(pd_caption_mod)}')\n",
    "unique_images = pd_caption_mod.image_name.unique()\n",
    "print(f'Anzahl Bilder: {len(unique_images)}')\n",
    "\n",
    "unique_images = list(pd_caption_mod.image_name.unique())\n",
    "train_images = random.sample(unique_images, k=int(len(unique_images) * 0.8))\n",
    "test_images = list(set(unique_images) - set(train_images))\n",
    "\n",
    "print(f'Länge Trainingsset: {len(train_images)}')\n",
    "print(f'Länge Testset: {len(test_images)}')\n",
    "\n",
    "pd_train_set = pd_caption_mod[pd_caption_mod.image_name.isin(train_images)]\n",
    "pd_test_set = pd_caption_mod[pd_caption_mod.image_name.isin(test_images)]\n",
    "pd_train_set.to_csv('./data/train_captions.csv', index=False)\n",
    "pd_test_set.to_csv('./data/test_captions.csv', index=False)\n",
    "\n",
    "print(f'Länge Trainingsset: {len(pd_train_set)}')\n",
    "print(f'Länge Testset: {len(pd_test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train_captions.csv')\n",
    "test_set = pd.read_csv('./data/test_captions.csv')\n",
    "\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{train_set.image_name[image_id]}'\n",
    "example_caption1 = train_set.caption[image_id+0]\n",
    "example_caption2 = train_set.caption[image_id+1]\n",
    "example_caption3 = train_set.caption[image_id+2]\n",
    "example_caption4 = train_set.caption[image_id+3]\n",
    "example_caption5 = train_set.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling (Trainingsset)\n",
    "- caption in matrix ablegen, gleiche länge der captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = train_set['caption'].apply(tokenize_en_len)\n",
    "print(f'maximale caption länge: {token_series.max()} Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_captions, min_freq):\n",
    "    # Zähle die Häufigkeit der Tokens in allen Captions\n",
    "    token_counts = Counter(token for caption in tokenized_captions for token in caption)\n",
    "\n",
    "    # Erstelle das Vokabular nur mit Tokens, die min_freq oder mehr Mal vorkommen\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<start>\": 1,\n",
    "        \"<end>\": 2\n",
    "    }\n",
    "    token_id = 3\n",
    "    for token, count in token_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[token] = token_id\n",
    "            token_id += 1\n",
    "    return vocab\n",
    "\n",
    "def create_matrices_from_captions(train_set):\n",
    "    captions = train_set['caption']\n",
    "    tokenized_captions = [tokenize_en(caption, lower_text=True) for caption in captions]\n",
    "\n",
    "    # Erstellen des Vokabular mit einer Mindesthäufigkeit von x\n",
    "    vocab = build_vocab(tokenized_captions, min_freq=5)\n",
    "\n",
    "    # Vokabular verwenden, um Ihre Captions in Indizes umzuwandeln\n",
    "    indexed_captions = [[vocab.get(token, vocab[\"<pad>\"]) for token in caption] for caption in tokenized_captions]\n",
    "\n",
    "    caption_tensors = [torch.tensor(caption) for caption in indexed_captions]\n",
    "\n",
    "    # Bestimmen der maximale Länge für das Padding\n",
    "    max_length = max(len(caption) for caption in caption_tensors)\n",
    "\n",
    "    # Padding hinzufügen, damit alle Captions die gleiche Länge haben\n",
    "    padded_captions = pad_sequence(caption_tensors, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "    return padded_captions, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)\n",
    "print(f'Matrix dim: {padded_captions.shape}')\n",
    "print(f'Länge Wörterbuch: {len(vocab)}')\n",
    "padded_captions[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, csv_file_name, root_dir, transform=None):\n",
    "        self.captions_frame = pd.read_csv(csv_file_name)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.captions_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        caption = self.captions_frame.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, caption\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    # transforms.RandomCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "test_set = FlickrDataset(\n",
    "    csv_file_name='./data/test_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataloader\n",
    "for i_batch, (image, caption) in enumerate(train_dataloader):\n",
    "    # print(f'{i_batch}: {image.shape}, {caption}')\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    img_pil = to_pil(image.squeeze(0))\n",
    "    plt.imshow(img_pil)\n",
    "    plt.title(caption)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda117_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
