{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH2\n",
    "Fachdozent: Martin Melchior     \n",
    "Student: Manuel Schwarz   \n",
    "HS23\n",
    "\n",
    "Dieses Notebook bearbeitet die Mini-Challenge 2 des Moduls Deep Learning (del).   \n",
    "Die Performance der Modelle wurde mit **wandb.ai** aufgezeichnet und kann [hier](https://wandb.ai/manuel-schwarz/del-mc2/workspace?workspace=user-manuel-schwarz) eingesehen werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Aufgabenstellung:</b> Eine Blaue Box beschreibt die Aufgabe aus der Aufgabenstellung 'SGDS_DEL_MC1.pdf' \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Antworte:</b> Eine Grüne Box beschreibt die Bearbeitung / Reflektion der Aufgabenstellung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import spacy  # conda install -c conda-forge spacy + python -m spacy download en_core_web_sm\n",
    "import torchvision\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "# from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchvision import datasets, models, transforms\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau Modellierung und Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Überlege Dir, welche Modell-Architektur Sinn machen könnte. Mindestens zwei Modell-Varianten sollen aufgebaut werden, die miteinander verglichen werden sollen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Für die del-MC2 Challenge wird das Modell vom Paper Vinyals et al `Show and Tell: A Neural Image Caption Generator` nachgebaut. Das Paper entwickelte ein Modell welches für Bilder eine Bildbeschreibung erstellt. Für die verwendeten Daten wird das `Flickr 8k` Datenset verwendet. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Flickr 8k lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = './data/Images'\n",
    "captions_file = './data/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_captions = pd.read_csv('./data/captions.txt', sep='\\t', header=None)\n",
    "pd_captions.columns = ['full_caption']\n",
    "pd_captions[['image_name', 'caption']] = pd_captions['full_caption'].str.split(',', n=1, expand=True)\n",
    "pd_captions.to_csv('./data/pd_captions.csv', index=False)\n",
    "pd_captions.drop('full_caption', axis=1, inplace=True)\n",
    "pd_captions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `caption.txt` File ist der Bildnamen und die Bildbeschreibung (caption) hinterlegt. Pro Bild stehen fünf Captions zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{pd_captions.image_name[image_id]}'\n",
    "example_caption1 = pd_captions.caption[image_id+0]\n",
    "example_caption2 = pd_captions.caption[image_id+1]\n",
    "example_caption3 = pd_captions.caption[image_id+2]\n",
    "example_caption4 = pd_captions.caption[image_id+3]\n",
    "example_caption5 = pd_captions.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bilddaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder_path = './data/Images/'\n",
    "\n",
    "resolutions = []\n",
    "for image_filename in os.listdir(images_folder_path):\n",
    "    if image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        image_path = os.path.join(images_folder_path, image_filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            resolutions.append(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_labels = [f\"{w}x{h}\" for w, h in resolutions]\n",
    "resolution_counts = Counter(dimension_labels)\n",
    "sorted_resolution_counts = dict(resolution_counts.most_common(50))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(sorted_resolution_counts.keys(), sorted_resolution_counts.values(), color='skyblue')\n",
    "\n",
    "plt.title('Verteilung der Bildauflösungen von Flickr 8k (Top 50)')\n",
    "plt.xlabel('Dimension (b x w)')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths, heights = zip(*resolutions)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(widths, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildbreiten')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['b'])\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.boxplot(heights, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildhöhen')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['h'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bildbeschreibungen\n",
    "\n",
    "Textdaten werden folgend in Tokens konvergiert:\n",
    "[doku torchtext](http://man.hubwiz.com/docset/torchtext.docset/Contents/Resources/Documents/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show torchtext\n",
    "# Version: 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "special_cases = [(\"<start>\", [{ORTH: \"<start>\"}]), (\"<end>\", [{ORTH: \"<end>\"}]), (\"<pad>\", [{ORTH: \"<pad>\"}])]\n",
    "for case in special_cases:\n",
    "    spacy_en.tokenizer.add_special_case(*case)\n",
    "\n",
    "def tokenize_en(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(caption)]\n",
    "    else:\n",
    "        return [tok.text for tok in spacy_en.tokenizer(caption)]    \n",
    "\n",
    "print(f'Test tokenize: {example_caption1}')\n",
    "tokens = tokenize_en(example_caption1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste mit allen vorhandenen Tokens \n",
    "token_series = pd_captions['caption'].apply(tokenize_en).explode()\n",
    "count_token = token_series.value_counts()\n",
    "count_token.index.name = 'token'\n",
    "count_token = count_token.reset_index()\n",
    "# count_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = 50\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.bar(count_token.head(num_token).token, count_token.head(num_token).iloc[:,1], color='skyblue')\n",
    "plt.title(f'Total Tokens: {len(count_token)} Tokens, dargestellt {num_token} Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en_len(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return len([tok.text.lower() for tok in spacy_en.tokenizer(caption)])\n",
    "    else:\n",
    "        return len([tok.text for tok in spacy_en.tokenizer(caption)])\n",
    "\n",
    "token_series = pd_captions['caption'].apply(tokenize_en_len)\n",
    "# token_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3))\n",
    "token_mean = token_series.mean()\n",
    "token_std = token_series.std() / 2\n",
    "plt.hist(token_series, color='skyblue', bins=40)\n",
    "plt.axvline(token_mean, color='red', alpha=0.6, label=f'mean {token_mean:0.2f}')\n",
    "plt.axvspan(token_mean-token_std, token_mean+token_std, color='grey', alpha=0.2, label=f'std {token_std:0.2f}')\n",
    "plt.suptitle('Verteilung der Anzahl Tokens je Caption')\n",
    "plt.title(f'min bei: {token_series.min()}, max bei: {token_series.max()}', fontsize=8)\n",
    "plt.xlabel('Anzahl Tokens')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling\n",
    "\n",
    "- Im Paper werden Tokens die weniger als fünf mal vorkommen entfernt\n",
    "- Im Paper werden `start` und `end` Token eingeführt, diese sollen dem Modell helfen zu erkennen, wann der Beginn der Generierung einer Beschreibung ist und wann sie beendet werden sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_token = 5\n",
    "count_token_filtered = count_token[count_token['count'] >= min_num_token]\n",
    "print(f'Total Tokens {len(count_token)}')\n",
    "print(f'Anzahl Tokens die mehr als {min_num_token} vorkommen: {len(count_token_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "\n",
    "pd_caption_mod = pd_captions.copy()\n",
    "pd_caption_mod['caption'] = pd_caption_mod['caption'].apply(lambda x: f\"{START_TOKEN} {x} {END_TOKEN}\")\n",
    "pd_captions.to_csv('./data/pd_captions_mod.csv', index=False)\n",
    "\n",
    "print(pd_captions['caption'][0])\n",
    "print(pd_captions['caption'][1])\n",
    "print(pd_captions['caption'][2])\n",
    "print()\n",
    "print(pd_caption_mod['caption'][0])\n",
    "print(pd_caption_mod['caption'][1])\n",
    "print(pd_caption_mod['caption'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Captions: {len(pd_caption_mod)}')\n",
    "unique_images = pd_caption_mod.image_name.unique()\n",
    "print(f'Anzahl Bilder: {len(unique_images)}')\n",
    "\n",
    "unique_images = list(pd_caption_mod.image_name.unique())\n",
    "train_images = random.sample(unique_images, k=int(len(unique_images) * 0.7))\n",
    "test_images = list(set(unique_images) - set(train_images))\n",
    "\n",
    "print(f'Länge Trainingsset: {len(train_images)}')\n",
    "print(f'Länge Testset: {len(test_images)}')\n",
    "\n",
    "pd_train_set = pd_caption_mod[pd_caption_mod.image_name.isin(train_images)]\n",
    "pd_test_set = pd_caption_mod[pd_caption_mod.image_name.isin(test_images)]\n",
    "pd_train_set.to_csv('./data/train_captions.csv', index=False)\n",
    "pd_test_set.to_csv('./data/test_captions.csv', index=False)\n",
    "\n",
    "print(f'Länge Trainingsset: {len(pd_train_set)}')\n",
    "print(f'Länge Testset: {len(pd_test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train_captions.csv')\n",
    "test_set = pd.read_csv('./data/test_captions.csv')\n",
    "\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{train_set.image_name[image_id]}'\n",
    "example_caption1 = train_set.caption[image_id+0]\n",
    "example_caption2 = train_set.caption[image_id+1]\n",
    "example_caption3 = train_set.caption[image_id+2]\n",
    "example_caption4 = train_set.caption[image_id+3]\n",
    "example_caption5 = train_set.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling (Trainingsset)\n",
    "- caption in matrix ablegen, gleiche länge der captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = train_set['caption'].apply(tokenize_en_len)\n",
    "print(f'maximale caption länge: {token_series.max()} Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_captions, min_freq):\n",
    "    # Zähle die Häufigkeit der Tokens in allen Captions\n",
    "    token_counts = Counter(token for caption in tokenized_captions for token in caption)\n",
    "\n",
    "    # Erstelle das Vokabular nur mit Tokens, die min_freq oder mehr Mal vorkommen\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<start>\": 1,\n",
    "        \"<end>\": 2\n",
    "    }\n",
    "    token_id = 3\n",
    "    for token, count in token_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[token] = token_id\n",
    "            token_id += 1\n",
    "    return vocab\n",
    "\n",
    "def create_matrices_from_captions(train_set):\n",
    "    captions = train_set['caption']\n",
    "    tokenized_captions = [tokenize_en(caption, lower_text=True) for caption in captions]\n",
    "\n",
    "    # Erstellen des Vokabular mit einer Mindesthäufigkeit von x\n",
    "    vocab = build_vocab(tokenized_captions, min_freq=5)\n",
    "\n",
    "    # Vokabular verwenden, um Ihre Captions in Indizes umzuwandeln\n",
    "    indexed_captions = [[vocab.get(token, vocab[\"<pad>\"]) for token in caption] for caption in tokenized_captions]\n",
    "\n",
    "    caption_tensors = [torch.tensor(caption) for caption in indexed_captions]\n",
    "\n",
    "    # Bestimmen der maximale Länge für das Padding\n",
    "    max_length = max(len(caption) for caption in caption_tensors)\n",
    "\n",
    "    # Padding hinzufügen, damit alle Captions die gleiche Länge haben\n",
    "    padded_captions = pad_sequence(caption_tensors, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "    return padded_captions, vocab\n",
    "\n",
    "def indices_to_words(tensor_indices, vocab, rm_pad=True):\n",
    "    index_to_word = {index: word for word, index in vocab.items()}    \n",
    "    words = [index_to_word.get(index.item(), '<unk>') for index in tensor_indices]\n",
    "    if rm_pad:\n",
    "        words = [word for word in words if word != '<pad>']    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)\n",
    "print(f'Matrix dim: {padded_captions.shape}')\n",
    "print(f'Länge Wörterbuch: {len(vocab)}')\n",
    "padded_captions[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Dataloaders\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import GloVe\n",
    "# glove = GloVe(name='6B', dim=300)\n",
    "# glove_embeddings = glove.vectors\n",
    "# print(glove_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path='./data/glove.6B.300d.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        vocab = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            vocab[word] = vector\n",
    "    return vocab\n",
    "\n",
    "glove_embeddings = load_glove_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim = 300):\n",
    "    embedding_matrix = torch.zeros((len(vocab)+2, embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Zufälliger Vektor für Wörter, die nicht in GloVe sind\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, csv_file_name, root_dir, vocab, embedding_matrix, transform=None):\n",
    "        self.captions_frame = pd.read_csv(csv_file_name)\n",
    "        self.root_dir = root_dir\n",
    "        self.vocab = vocab\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.captions_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        caption = self.captions_frame.iloc[idx,1]\n",
    "        image_name = self.captions_frame.iloc[idx,0]\n",
    "        tokenized_caption = tokenize_en(caption, lower_text=True)\n",
    "\n",
    "        caption_indices = [self.vocab.get(token, self.vocab['<pad>']) for token in tokenized_caption]\n",
    "        \n",
    "        # Umwandeln der Liste von Indizes in einen Tensor\n",
    "        caption_indices_tensor = torch.tensor(caption_indices, dtype=torch.long)\n",
    "\n",
    "        # Extrahieren der Embeddings für die Indizes\n",
    "        caption_embeddings = torch.stack([self.embedding_matrix[idx] for idx in caption_indices])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption, caption_indices_tensor, caption_embeddings, image_name\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, caption, caption_indices, caption_embeddings, image_name = zip(*batch)\n",
    "\n",
    "        # Pad die caption_indices und caption_embeddings\n",
    "        caption_indices_padded = pad_sequence(caption_indices, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "        caption_embeddings_padded = pad_sequence(caption_embeddings, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "\n",
    "        images = torch.stack(images)  # Stapeln der Bilder zu einem Tensor\n",
    "\n",
    "        return images, caption, caption_indices_padded, caption_embeddings_padded, image_name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Testen des Datensets\n",
    "image, caption, caption_indices, caption_embeddings, image_name = train_set[0]\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(image)\n",
    "plt.suptitle(f'{caption}', fontsize=10)\n",
    "plt.title(f'Länge caption indices: {len(caption_indices)}, Länge caption_embeddings: {len(caption_embeddings)}', fontsize=8)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen und Testen der Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "    std=[1 / s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "test_set = FlickrDataset(\n",
    "    csv_file_name='./data/test_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=train_set.collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=test_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen des Dataloader, prüfen ob die caption_empeddings pro batch gleich lang sind\n",
    "for i_batch, (image, caption, caption_indices, caption_embeddings, image_name) in enumerate(train_dataloader):\n",
    "    print(f'Länge caption_indices 1: {len(caption_indices[0])}, Länge caption_embeddings 1: {len(caption_embeddings[0])}')\n",
    "    print(f'Länge caption_indices 2: {len(caption_indices[1])}, Länge caption_embeddings 2: {len(caption_embeddings[1])}')\n",
    "    print(f'Länge caption_indices 3: {len(caption_indices[2])}, Länge caption_embeddings 3: {len(caption_embeddings[2])}')\n",
    "    print(f'Länge caption_indices 4: {len(caption_indices[3])}, Länge caption_embeddings 4: {len(caption_embeddings[3])}')\n",
    "    print()\n",
    "    print('Example Image 1/4')\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    img_tensor_denorm  = denormalize(image[0])\n",
    "    img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "    plt.imshow(img_pil)\n",
    "    plt.suptitle(caption[0])\n",
    "    plt.title(f'wörter from caption_indices: {indices_to_words(caption_indices[0], vocab)}', fontsize=7)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellierung Modell nach Paper `Show and Tell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob=0.5, glove_em=None):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Laden des vortrainierten ResNet-50 ohne den letzten Layer\n",
    "        resnet = models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
    "        \n",
    "        # Einbettungs-Layer für die Captions\n",
    "        if glove_em is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(glove_em, freeze=True)\n",
    "            print('using glove')\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM für die Caption-Generierung\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Layer, um die Wort-Indizes vorherzusagen\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        # CNN-Teil\n",
    "        with torch.no_grad():  # Gradienten für ResNet nicht berechnen\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "\n",
    "        features = self.batch_norm(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Embedding und LSTM-Teil\n",
    "        captions_cut = captions.clone()\n",
    "        captions_cut[captions_cut>=self.vocab_size-1] = 0\n",
    "        embeddings = self.embedding(captions_cut)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        outputs_cut = outputs[:,:,0:self.vocab_size-1]\n",
    "        \n",
    "        return outputs_cut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu Metrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " es gibt mehrere Varianten des BLEU-Scores (Bilingual Evaluation Understudy Score), die sich hauptsächlich in der Anzahl der betrachteten N-Gramme unterscheiden. Ein N-Gramm ist eine zusammenhängende Sequenz von N Items aus einer gegebenen Text- oder Sprachprobe. Der BLEU-Score kann basierend auf unigrammen (einzelnen Wörtern), bigrammen (Paaren von aufeinanderfolgenden Wörtern), trigrammen und so weiter berechnet werden. Häufig werden BLEU-Scores für verschiedene N-Gramme kombiniert, um eine Gesamtbewertung der Übersetzung zu geben. Dies wird oft als kumulativer BLEU-Score bezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_end_words(predicted_captions):\n",
    "    predicted_captions_trimmed = [caption.replace('<start> ', '') for caption in predicted_captions]\n",
    "    predicted_captions_trimmed = [caption.replace(' <end>', '') for caption in predicted_captions_trimmed]\n",
    "    return predicted_captions_trimmed\n",
    "\n",
    "\n",
    "def calculate_bleu(true_caption, predicted_caption, n_gram=1):\n",
    "    if isinstance(true_caption, str):\n",
    "        true_caption = tokenize_en(true_caption)\n",
    "    if isinstance(predicted_caption, str):\n",
    "        predicted_caption = tokenize_en(predicted_caption)\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "\n",
    "    if n_gram == 1:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(1,), smoothing_function=smoothing)\n",
    "    elif n_gram == 2:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.5, 0.5), smoothing_function=smoothing)\n",
    "    elif n_gram == 3:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.33, 0.33, 0.33), smoothing_function=smoothing)\n",
    "    elif n_gram == 4:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    else:\n",
    "        raise ValueError(\"N-Gram-Wert außerhalb des gültigen Bereichs\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "def calculate_blue_batch(true_captions, predicted_caption, n_gram=1, get_mean=True):\n",
    "    blue_scores_n = [calculate_bleu(true_cap, cap_pred, n_gram) for (true_cap, cap_pred) in zip(true_captions, predicted_caption)]\n",
    "    if get_mean:\n",
    "        return np.mean(blue_scores_n)\n",
    "    else:\n",
    "        return blue_scores_n\n",
    "\n",
    "true_caption = \"Das ist ein Test.\"\n",
    "predicted_caption = \"Dies ist ein Test.\"\n",
    "bleu_score = calculate_bleu(true_caption, predicted_caption, n_gram=4)\n",
    "print('Blue Score Test:')\n",
    "print(f\"Bleu Score: {bleu_score}\")\n",
    "print()\n",
    "\n",
    "predicted_caption = [\n",
    "    \"<start> a brown and white dog is running on the grass . <end>\",\n",
    "    \"<start> a brown and white dog is running on the grass . <end>\"\n",
    "]\n",
    "predicted_captions_trimmed = remove_start_end_words(predicted_caption)\n",
    "print('Remove <start> und <end> von caption Test:')\n",
    "print(f'caption: {predicted_caption[0]}')\n",
    "print(f'caption trimmed: {predicted_captions_trimmed[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modell(config, model, dataloader, optimizer, criterion, epochs, device, test_batch=False):\n",
    "    set_seed(config['set_seed'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "    batch_losses = []\n",
    "\n",
    "    if config['write_wandb']: \n",
    "        model_name = f\"{config['name']}-{config['epochs']}-epochs-{config['start_time']}\"\n",
    "        wandb.init(\n",
    "            project=\"del-mc2\",\n",
    "            entity='manuel-schwarz',\n",
    "            group=config['group'],\n",
    "            name= model_name,\n",
    "            tags=str(config['tags']) + (' is_test_batch' if config['is_test_batch'] else ''),\n",
    "            config=config\n",
    "        )\n",
    "        wandb.watch(model)\n",
    "\n",
    "    if config['use_gpu_memory_snapshot']:\n",
    "        MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT = 100_000\n",
    "        torch.cuda.memory._record_memory_history(max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss = []\n",
    "        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "        for i, (images, captions, caption_indices, glove_embeddings, image_name) in loop:\n",
    "            if test_batch and i > 1:\n",
    "                break\n",
    "\n",
    "            # if i < 13500:  #error: 12613\n",
    "            #      continue\n",
    "\n",
    "            # if config['use_glove_emb']:\n",
    "            #     images, captions_emb2 = images.to(device), glove_embeddings.to(device)\n",
    "            #     # captions_emb2 = captions_emb2.long()\n",
    "            # else:\n",
    "            #     images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            # outputs = model(images, captions_emb2[:, :-1])  # Exclude the <end> token\n",
    "            outputs = model(images, captions_emb[:, :-1])  # Exclude the <end> token\n",
    "            # targets = caption_indices[:, 1:].contiguous().view(-1)  # Exclude the <start> token\n",
    "            targets = captions_emb[:, :].contiguous().view(-1)\n",
    "            targets_cut = targets.clone()\n",
    "            targets_cut[targets_cut >= config['vocab_size']-1] = 0\n",
    "\n",
    "            output_shaped = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # output_shaped_cpu = output_shaped.cpu()\n",
    "\n",
    "            # loss = criterion(outputs.view(-1, outputs.size(-1)), targets)\n",
    "            loss = criterion(output_shaped, targets_cut)\n",
    "            # batch_losses.append(loss.item())\n",
    "            ep_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if config['use_gpu_memory_snapshot'] and epoch == 5:\n",
    "                file_name = f'{config[\"start_time\"]}_epoch_{epoch}_gpu_snapshot'\n",
    "                save_path = f'./gpu_snapshot/'\n",
    "                try:\n",
    "                    torch.cuda.memory._dump_snapshot(f\"{save_path}{file_name}.pickle\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to capture memory snapshot {e}\")\n",
    "        \n",
    "        epoch_losses.append(np.mean(ep_loss))\n",
    "        # print(f'Epoch {epoch}    Loss: {np.mean(ep_loss)}')\n",
    "\n",
    "        if config['write_wandb']:\n",
    "            wandb.log({\n",
    "                \"train loss epoch\": np.mean(ep_loss)\n",
    "                })\n",
    "            \n",
    "    \n",
    "    if config['write_wandb']: \n",
    "        wandb.finish()\n",
    "        time.sleep(5)  # wait for wandb.finish\n",
    "\n",
    "    if config['use_gpu_memory_snapshot']:\n",
    "        torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "    print('Modell finished!')\n",
    "    play_sound(1)\n",
    "    return epoch_losses, batch_losses\n",
    "\n",
    "def plot_loss_model(epoch_losses, name='-'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epoch_losses, marker='o', color='skyblue', label='Training loss per epoch')\n",
    "    plt.title(f'Training Loss per Epoch (model: {name})')\n",
    "    plt.xlabel('Epoche')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, config):\n",
    "    path = f'./models/{config[\"start_time\"]}_{config[\"name\"]}_epochs_{config[\"epochs\"]}.pth'\n",
    "    torch.save(model, path)\n",
    "\n",
    "def load_model(model_name, device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    path = f'./models/{model_name}.pth'\n",
    "    model = torch.load(path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"name\": \"CNN_LSTM_name\",  # CNN_LSTM_glove\n",
    "    \"epochs\": 100,   \n",
    "    \"train_batch_size\": 128, \n",
    "    \"test_batch_size\": 64,\n",
    "    \"dataset\": \"flickr8k\",\n",
    "    \"lr\": 0.1, \n",
    "    \"optimizer\": 'SGD',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"image_size\": 256, \n",
    "    \"is_test_batch\": False,\n",
    "    \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "    \"num_workers\": 0,\n",
    "    \"dropout\": 0.5,\n",
    "    \"set_seed\": 42,\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 1,\n",
    "    'write_wandb':True,\n",
    "    'group': 'first model',\n",
    "    'tags': 'training',\n",
    "    'use_glove_emb': False,\n",
    "    'save_model': True,\n",
    "    'device': str(torch.device('cuda' if torch.cuda.is_available() else 'cpu')),\n",
    "    'use_gpu_memory_snapshot': False  # Achtung Beispiel code: snapshot nur für Linux verfügbar!\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=config['train_batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=train_set.collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=config['test_batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model1 = ImageCaptioningModel(\n",
    "    vocab_size = config['vocab_size'], \n",
    "    embedding_dim = config['embedding_dim'], \n",
    "    hidden_dim = config['hidden_dim'], \n",
    "    num_layers = config['num_layers'],\n",
    "    dropout_prob= config['dropout'],\n",
    "    glove_em = embedding_matrix if config['use_glove_emb'] else None\n",
    ")\n",
    "\n",
    "# model_name = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "# model1 = load_model(model_name)\n",
    "\n",
    "# optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=config['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if False:\n",
    "    epoch_losses, batch_losses = train_modell(\n",
    "        config,\n",
    "        model1,\n",
    "        train_dataloader,\n",
    "        optimizer, \n",
    "        criterion, \n",
    "        epochs=config['epochs'], \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        # device='cpu',\n",
    "        test_batch=config['is_test_batch']\n",
    "    )\n",
    "\n",
    "    if config['save_model']:\n",
    "        save_model(model1, config)\n",
    "        print('Modell saved!')\n",
    "\n",
    "    plot_loss_model(epoch_losses, 'model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell Vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_indices_to_sentences(batch_indices, vocab, rm_pad=True):\n",
    "    sentences = []\n",
    "    for tensor_indices in batch_indices:\n",
    "        words = indices_to_words(tensor_indices, vocab, rm_pad)\n",
    "        sentence = ' '.join(words)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def model_output_to_caption(model_prediction, vocab):\n",
    "    _, pred_indices = torch.max(model_prediction, dim=-1)\n",
    "    pred_captions = batch_indices_to_sentences(pred_indices, vocab)  \n",
    "    return pred_captions\n",
    "\n",
    "def plot_test_images(images, caption_pred, image_name, plot_num_img=4):\n",
    "    if plot_num_img > len(image_name):\n",
    "        plot_num_img = len(image_name)\n",
    "\n",
    "    pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "    for i in range(0, 5 * plot_num_img, 5):\n",
    "        pd_captions = pd_data[pd_data.image_name == image_name[i]].caption.reset_index(drop=True)\n",
    "        cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "        plt.figure(figsize = (10, 4))\n",
    "        img_tensor_denorm  = denormalize(images[i])\n",
    "        img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "        plt.imshow(img_pil)\n",
    "        plt.suptitle(f'pred: {caption_pred[i]}', color='red', y=1.15)\n",
    "        plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Testbilder, Model mit eigenen Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_name = '31.12.2023_1632_CNN_LSTM_epochs_30'\n",
    "# model_name = '31.12.2023_1849_CNN_LSTM_b_epochs_30'\n",
    "# model_name = '31.12.2023_2107_CNN_LSTM_c_epochs_90'\n",
    "# model_name = '01.01.2024_0842_CNN_LSTM_d_epochs_30'\n",
    "# model_name = '01.01.2024_1104_CNN_LSTM_e_epochs_30'\n",
    "model_name = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model1_loaded = load_model(model_name)\n",
    "\n",
    "model1_loaded.eval()\n",
    "\n",
    "for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "        pred = model1_loaded(images, captions_emb[:, :-1])\n",
    "        caption_pred = model_output_to_caption(pred, vocab)\n",
    "\n",
    "        plot_test_images(images, caption_pred, image_name, plot_num_img=4)\n",
    "\n",
    "        blue_scores_n1 = calculate_blue_batch(captions, caption_pred, 1)\n",
    "        blue_scores_n2 = calculate_blue_batch(captions, caption_pred, 2)\n",
    "        blue_scores_n3 = calculate_blue_batch(captions, caption_pred, 3)\n",
    "        blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "    break\n",
    "\n",
    "# caption_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Testbilder, Modell mit Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_glove_name = '01.01.2024_2227_CNN_LSTM_glove_a_epochs_100'\n",
    "# model_glove_name = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "model_glove_name = '04.01.2024_1846_CNN_LSTM_glove_c_epochs_100'\n",
    "model1_glove_loaded = load_model(model_glove_name)\n",
    "\n",
    "model1_glove_loaded.eval()\n",
    "\n",
    "for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "        pred = model1_glove_loaded(images, captions_emb[:, :-1])\n",
    "        caption_pred = model_output_to_caption(pred, vocab)\n",
    "\n",
    "        plot_test_images(images, caption_pred, image_name, plot_num_img=4)\n",
    "\n",
    "        blue_scores_n1 = calculate_blue_batch(captions, caption_pred, 1)\n",
    "        blue_scores_n2 = calculate_blue_batch(captions, caption_pred, 2)\n",
    "        blue_scores_n3 = calculate_blue_batch(captions, caption_pred, 3)\n",
    "        blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Scores, Model mit eigenen Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions_blue_scores(model, testloader, trim_captions=True):\n",
    "    '''\n",
    "    loads the modell, makes predictions and returns the mean blue score for 1-4 gram\n",
    "    param:\n",
    "        trim_captions: removes the added <start> and <end> word in captions\n",
    "    '''\n",
    "    blue_scores_n1_full = []\n",
    "    blue_scores_n2_full = []\n",
    "    blue_scores_n3_full = []\n",
    "    blue_scores_n4_full = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            pred = model(images, captions_emb[:, :-1])\n",
    "            caption_pred = model_output_to_caption(pred, vocab)\n",
    "            if trim_captions:\n",
    "                caption_pred = remove_start_end_words(caption_pred)\n",
    "                captions = remove_start_end_words(captions)\n",
    "\n",
    "            blue_scores_n1_full.append(calculate_blue_batch(captions, caption_pred, 1))\n",
    "            blue_scores_n2_full.append(calculate_blue_batch(captions, caption_pred, 2))\n",
    "            blue_scores_n3_full.append(calculate_blue_batch(captions, caption_pred, 3))\n",
    "            blue_scores_n4_full.append(calculate_blue_batch(captions, caption_pred, 4))\n",
    "\n",
    "    mean_scores = [\n",
    "        np.mean(blue_scores_n1_full), \n",
    "        np.mean(blue_scores_n2_full),\n",
    "        np.mean(blue_scores_n3_full),\n",
    "        np.mean(blue_scores_n4_full)\n",
    "    ]\n",
    "    return mean_scores\n",
    "\n",
    "\n",
    "def plot_model_blue_score(mean_scores, title='title', figsize=(8, 4)):\n",
    "    labels = ['Blue 1-gram', 'Blue 2-gram', 'Blue 3-gram', 'Blue 4-gram']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.bar(labels, mean_scores, color='skyblue', zorder=3)\n",
    "    for i in range(len(labels)):\n",
    "        ax.text(i, mean_scores[i], f'{mean_scores[i]:.2f}', \n",
    "            ha='center', va='top', color='white')\n",
    "    plt.suptitle('Übersicht Model Blue Score zu verschieden n-grams')\n",
    "    plt.title(f'{title}', fontsize=9)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Blue score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0,0.6,0.1))\n",
    "    plt.grid(axis='y', color='silver', zorder=3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_blue_score_diff(mean_scores:list, mean_scores_glove:list, epochs=0, figsize=(10, 6)):\n",
    "    labels = ['Blue 1-gram', 'Blue 2-gram', 'Blue 3-gram', 'Blue 4-gram']\n",
    "\n",
    "    bar_width = 0.3\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bars1 = ax.bar(np.arange(len(labels)), mean_scores, bar_width, label='Eigene Embeddings', color='dodgerblue', zorder=3)\n",
    "    bars2 = ax.bar(np.arange(len(labels)) + bar_width, mean_scores_glove, bar_width, label='Glove Embeddings', color='orange', zorder=3)\n",
    "    for i in range(len(labels)):\n",
    "        ax.text(i, mean_scores[i], f'{mean_scores[i]:.2f}', \n",
    "            ha='center', va='top', color='white')\n",
    "        ax.text(i + bar_width, mean_scores_glove[i], f'{mean_scores_glove[i]:.2f}', \n",
    "            ha='center', va='top', color='white')        \n",
    "    ax.set_xticks(np.arange(len(labels)) + bar_width / 2)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks(np.arange(0,0.6,0.1))\n",
    "    ax.set_ylabel('Blue Score')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(f'Vergleich der Modelle, ohne und mit Glove Embeddings (Epochs {epochs})')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_epoch_240 = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model_loaded_epoch_240 = load_model(model_name_epoch_240)\n",
    "\n",
    "mean_scores = model_predictions_blue_scores(model_loaded_epoch_240, test_dataloader)\n",
    "plot_model_blue_score(mean_scores, title='Eigene Wortembedings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Scores, Model mit Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_name_epoch_240 = '04.01.2024_1846_CNN_LSTM_glove_c_epochs_100'\n",
    "model_glove_loaded_epoch_240 = load_model(model_glove_name_epoch_240)\n",
    "\n",
    "mean_scores_glove = model_predictions_blue_scores(model_glove_loaded_epoch_240, test_dataloader)\n",
    "plot_model_blue_score(mean_scores_glove, title='Glove Wortembedings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Score Vergleich zwischen eigenen und Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_blue_score_diff(mean_scores, mean_scores_glove, epochs=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modellvergleiche mit tieferen Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_epoch_150 = '31.12.2023_2107_CNN_LSTM_c_epochs_90'\n",
    "model_loaded_epoch_150 = load_model(model_name_epoch_150)\n",
    "mean_scores_150 = model_predictions_blue_scores(model_loaded_epoch_150, test_dataloader)\n",
    "\n",
    "# Glove Model\n",
    "model_glove_name_epoch_140 = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "model_glove_loaded_epoch_140 = load_model(model_glove_name_epoch_140)\n",
    "mean_scores_glove_140 = model_predictions_blue_scores(model_glove_loaded_epoch_140, test_dataloader)\n",
    "\n",
    "# plot Modell Differenz\n",
    "plot_model_blue_score_diff(mean_scores_150, mean_scores_glove_140, epochs=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche nach der besten Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_predictions(model, testloader, trim_captions=True):\n",
    "    '''\n",
    "    loads the modell, makes predictions and returns the mean blue score for 1-4 gram\n",
    "    param:\n",
    "        trim_captions: removes the added <start> and <end> word in captions\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    best_blue_4_gram_score = 0\n",
    "\n",
    "    for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            pred = model(images, captions_emb[:, :-1])\n",
    "            caption_pred = model_output_to_caption(pred, vocab)\n",
    "            if trim_captions:\n",
    "                caption_pred = remove_start_end_words(caption_pred)\n",
    "                captions = remove_start_end_words(captions)\n",
    "\n",
    "            blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "            if blue_scores_n4 > best_blue_4_gram_score:\n",
    "                print(f'best blue score 4 gram: {best_blue_4_gram_score:0.4f} on {image_name}')\n",
    "                best_blue_4_gram_score = blue_scores_n4\n",
    "                best_image = images\n",
    "                best_caption_pred = caption_pred\n",
    "                best_true_caption = captions\n",
    "                best_image_name = image_name\n",
    "\n",
    "    return best_image, best_image_name, best_true_caption, best_caption_pred, best_blue_4_gram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_bt_1 = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model_name_epoch_240 = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model_loaded_epoch_240 = load_model(model_name_epoch_240)\n",
    "\n",
    "image, image_name, true_caption, caption_pred, score = get_best_model_predictions(\n",
    "    model_loaded_epoch_240,\n",
    "    test_dataloader_bt_1\n",
    ")\n",
    "\n",
    "pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "pd_captions = pd_data[pd_data.image_name == image_name[0]].caption.reset_index(drop=True)\n",
    "cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "img_tensor_denorm  = denormalize(image[0])\n",
    "img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "plt.imshow(img_pil)\n",
    "plt.suptitle(f'best pred: {caption_pred[0]}', color='red', y=1.15)\n",
    "plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "pd_captions = pd_data[pd_data.image_name == image_name[0]].caption.reset_index(drop=True)\n",
    "cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "img_tensor_denorm  = denormalize(image[0])\n",
    "img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "plt.imshow(img_pil)\n",
    "plt.suptitle(f'pred: {caption_pred[0]}', color='red', y=1.15)\n",
    "plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ende - Mini Challenge 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda117_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
