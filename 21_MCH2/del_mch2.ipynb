{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH2\n",
    "Fachdozent: Martin Melchior     \n",
    "Student: Manuel Schwarz   \n",
    "HS23\n",
    "\n",
    "Dieses Notebook bearbeitet die Mini-Challenge 2 des Moduls Deep Learning (del).   \n",
    "Die Performance der Modelle wurde mit **wandb.ai** aufgezeichnet und kann [hier](https://wandb.ai/manuel-schwarz/del-mc2/workspace?workspace=user-manuel-schwarz) eingesehen werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Ziel:</b> Eine Blaue Box beschreibt das Ziel eines Schrittes' \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Antworte:</b> Eine Grüne Box beschreibt die Bearbeitung / Reflektion des Schrittes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import spacy  # conda install -c conda-forge spacy + python -m spacy download en_core_web_sm\n",
    "import torchvision\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "# from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchvision import datasets, models, transforms\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau Modellierung und Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Aufgabenstelung:</b> Überlege Dir, welche Modell-Architektur Sinn machen könnte. Mindestens zwei Modell-Varianten sollen aufgebaut werden, die miteinander verglichen werden sollen. \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Für die del-MC2 Challenge wird das Modell vom Paper Vinyals et al `Show and Tell: A Neural Image Caption Generator` nachgebaut. Das Paper entwickelte ein Modell welches für Bilder eine Bildbeschreibung erstellt. Für die verwendeten Daten wird das `Flickr 8k` Datenset verwendet. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Daten Flickr 8k lesen & EDA\n",
    "Der Flickr 8k Datenatz umfasst 8000 Bilder, die von Flickr Nutzer erstellt worden sind. Jedes Bild wurde mit fünf Bildbeschreibungen versehen (Captions). Das Datenset wird oft verwendet um Modelle für automatische Bildbeschriftungen zu trainieren und testen. Folgend eine Explorative Datenanalyse:\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = './data/Images'\n",
    "captions_file = './data/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_captions = pd.read_csv('./data/captions.txt', sep='\\t', header=None)\n",
    "pd_captions.columns = ['full_caption']\n",
    "pd_captions[['image_name', 'caption']] = pd_captions['full_caption'].str.split(',', n=1, expand=True)\n",
    "pd_captions.to_csv('./data/pd_captions.csv', index=False)\n",
    "pd_captions.drop('full_caption', axis=1, inplace=True)\n",
    "pd_captions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `caption.txt` File ist der Bildnamen und die Bildbeschreibung (caption) hinterlegt. Pro Bild stehen fünf Captions zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{pd_captions.image_name[image_id]}'\n",
    "example_caption1 = pd_captions.caption[image_id+0]\n",
    "example_caption2 = pd_captions.caption[image_id+1]\n",
    "example_caption3 = pd_captions.caption[image_id+2]\n",
    "example_caption4 = pd_captions.caption[image_id+3]\n",
    "example_caption5 = pd_captions.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bilddaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder_path = './data/Images/'\n",
    "\n",
    "resolutions = []\n",
    "for image_filename in os.listdir(images_folder_path):\n",
    "    if image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        image_path = os.path.join(images_folder_path, image_filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            resolutions.append(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_labels = [f\"{w}x{h}\" for w, h in resolutions]\n",
    "resolution_counts = Counter(dimension_labels)\n",
    "sorted_resolution_counts = dict(resolution_counts.most_common(50))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(sorted_resolution_counts.keys(), sorted_resolution_counts.values(), color='skyblue')\n",
    "\n",
    "plt.title('Verteilung der Bildauflösungen von Flickr 8k (Top 50)')\n",
    "plt.xlabel('Dimension (b x w)')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths, heights = zip(*resolutions)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(widths, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildbreiten')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['b'])\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.boxplot(heights, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildhöhen')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['h'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bildbeschreibungen\n",
    "\n",
    "Textdaten werden folgend in Tokens konvergiert:  \n",
    "[Torchtext](http://man.hubwiz.com/docset/torchtext.docset/Contents/Resources/Documents/index.html) funktionierte mit der aktuellen CUDA Version nicht. Spacy wurde als Tokenizer eingesetzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show torchtext\n",
    "# Version: 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "special_cases = [(\"<start>\", [{ORTH: \"<start>\"}]), (\"<end>\", [{ORTH: \"<end>\"}]), (\"<pad>\", [{ORTH: \"<pad>\"}])]\n",
    "for case in special_cases:\n",
    "    spacy_en.tokenizer.add_special_case(*case)\n",
    "\n",
    "def tokenize_en(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(caption)]\n",
    "    else:\n",
    "        return [tok.text for tok in spacy_en.tokenizer(caption)]    \n",
    "\n",
    "print(f'Test tokenize: {example_caption1}')\n",
    "tokens = tokenize_en(example_caption1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste mit allen vorhandenen Tokens \n",
    "token_series = pd_captions['caption'].apply(tokenize_en).explode()\n",
    "count_token = token_series.value_counts()\n",
    "count_token.index.name = 'token'\n",
    "count_token = count_token.reset_index()\n",
    "# count_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = 50\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.bar(count_token.head(num_token).token, count_token.head(num_token).iloc[:,1], color='skyblue')\n",
    "plt.title(f'Total Tokens: {len(count_token)} Tokens, dargestellt {num_token} Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en_len(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return len([tok.text.lower() for tok in spacy_en.tokenizer(caption)])\n",
    "    else:\n",
    "        return len([tok.text for tok in spacy_en.tokenizer(caption)])\n",
    "\n",
    "token_series = pd_captions['caption'].apply(tokenize_en_len)\n",
    "# token_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3))\n",
    "token_mean = token_series.mean()\n",
    "token_std = token_series.std() / 2\n",
    "plt.hist(token_series, color='skyblue', bins=40)\n",
    "plt.axvline(token_mean, color='red', alpha=0.6, label=f'mean {token_mean:0.2f}')\n",
    "plt.axvspan(token_mean-token_std, token_mean+token_std, color='grey', alpha=0.2, label=f'std {token_std:0.2f}')\n",
    "plt.suptitle('Verteilung der Anzahl Tokens je Caption')\n",
    "plt.title(f'min bei: {token_series.min()}, max bei: {token_series.max()}', fontsize=8)\n",
    "plt.xlabel('Anzahl Tokens')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Bilder des Datensatz haben unterschiedliche Auflösungen, wobei der grösste Teil zwischen `500x333` bis `500x375` und `332x500` bis `400x500` Pixel liegen. Der Mittelwert der Bildauflösung ist `500x375` Pixel. Die Bildbeschreibungen umfassen 9209 einzelne Tokens, wobei die Tokens `a`, `.`, `A`, `in` und `the` am häufigsten Vorkommen. Im Mittel kommen  12 Tokens pro Caption vor, der minimal Wert liegt bei 1 und die Anzahl maximale Tokens bei 42.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Bearbeitung der Captions für Modelling\n",
    "Folgende Bearbeitungen wurden im Paper vorgenommen:\n",
    "- Tokens die weniger als fünf mal vorkommen wurden entfernt\n",
    "- es werden `start` und `end` Token eingeführt, diese sollen dem Modell helfen zu erkennen, wann der Beginn der Generierung einer Beschreibung ist und wann sie beendet werden sollte.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_token = 5\n",
    "count_token_filtered = count_token[count_token['count'] >= min_num_token]\n",
    "print(f'Total Tokens {len(count_token)}')\n",
    "print(f'Anzahl Tokens die mehr als {min_num_token} vorkommen: {len(count_token_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "\n",
    "pd_caption_mod = pd_captions.copy()\n",
    "pd_caption_mod['caption'] = pd_caption_mod['caption'].apply(lambda x: f\"{START_TOKEN} {x} {END_TOKEN}\")\n",
    "pd_captions.to_csv('./data/pd_captions_mod.csv', index=False)\n",
    "\n",
    "print(pd_captions['caption'][0])\n",
    "print(pd_captions['caption'][1])\n",
    "print(pd_captions['caption'][2])\n",
    "print()\n",
    "print(pd_caption_mod['caption'][0])\n",
    "print(pd_caption_mod['caption'][1])\n",
    "print(pd_caption_mod['caption'][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Aufteilung in Trainings- und Testdaten  \n",
    "Die Daten werden in Trainings- und Testdaten aufgeteilt und als separate .csv Dateien gespeichert. Die Start und End Captions sind in beiden Sets den Captions hinzugefügt worden.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Captions: {len(pd_caption_mod)}')\n",
    "unique_images = pd_caption_mod.image_name.unique()\n",
    "print(f'Anzahl Bilder: {len(unique_images)}')\n",
    "\n",
    "unique_images = list(pd_caption_mod.image_name.unique())\n",
    "train_images = random.sample(unique_images, k=int(len(unique_images) * 0.7))\n",
    "test_images = list(set(unique_images) - set(train_images))\n",
    "\n",
    "print(f'Länge Trainingsset: {len(train_images)}')\n",
    "print(f'Länge Testset: {len(test_images)}')\n",
    "\n",
    "pd_train_set = pd_caption_mod[pd_caption_mod.image_name.isin(train_images)]\n",
    "pd_test_set = pd_caption_mod[pd_caption_mod.image_name.isin(test_images)]\n",
    "pd_train_set.to_csv('./data/train_captions.csv', index=False)\n",
    "pd_test_set.to_csv('./data/test_captions.csv', index=False)\n",
    "\n",
    "print(f'Länge Trainingsset: {len(pd_train_set)}')\n",
    "print(f'Länge Testset: {len(pd_test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train_captions.csv')\n",
    "test_set = pd.read_csv('./data/test_captions.csv')\n",
    "\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{train_set.image_name[image_id]}'\n",
    "example_caption1 = train_set.caption[image_id+0]\n",
    "example_caption2 = train_set.caption[image_id+1]\n",
    "example_caption3 = train_set.caption[image_id+2]\n",
    "example_caption4 = train_set.caption[image_id+3]\n",
    "example_caption5 = train_set.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Bearbeitung der Captions für Modelling (Trainingsset)\n",
    "Folgend wird getestet wie die Wort Captions im Trainset in nummerische Werte umgewandelt werden können. Dazu wird ein Wörterburch angelegt (build_vocab()). Das Wörterbuch umfasst jedes Wort der Trainingscaptions und führt ihnen einen Index zu. Das die Caption für das Modell pro Batch gleich lang sein müssen, werden Captions die kürzer als die maximale Caption (max=44) sind mit einem `padding:0` Token verlängert.  \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = train_set['caption'].apply(tokenize_en_len)\n",
    "print(f'maximale caption länge: {token_series.max()} Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_captions, min_freq):\n",
    "    # Zähle die Häufigkeit der Tokens in allen Captions\n",
    "    token_counts = Counter(token for caption in tokenized_captions for token in caption)\n",
    "\n",
    "    # Erstelle das Vokabular nur mit Tokens, die min_freq oder mehr Mal vorkommen\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<start>\": 1,\n",
    "        \"<end>\": 2\n",
    "    }\n",
    "    token_id = 3\n",
    "    for token, count in token_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[token] = token_id\n",
    "            token_id += 1\n",
    "    return vocab\n",
    "\n",
    "def create_matrices_from_captions(train_set):\n",
    "    captions = train_set['caption']\n",
    "    tokenized_captions = [tokenize_en(caption, lower_text=True) for caption in captions]\n",
    "\n",
    "    # Erstellen des Vokabular mit einer Mindesthäufigkeit von x\n",
    "    vocab = build_vocab(tokenized_captions, min_freq=5)\n",
    "\n",
    "    # Vokabular verwenden, um Ihre Captions in Indizes umzuwandeln\n",
    "    indexed_captions = [[vocab.get(token, vocab[\"<pad>\"]) for token in caption] for caption in tokenized_captions]\n",
    "\n",
    "    caption_tensors = [torch.tensor(caption) for caption in indexed_captions]\n",
    "\n",
    "    # Bestimmen der maximale Länge für das Padding\n",
    "    max_length = max(len(caption) for caption in caption_tensors)\n",
    "\n",
    "    # Padding hinzufügen, damit alle Captions die gleiche Länge haben\n",
    "    padded_captions = pad_sequence(caption_tensors, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "    return padded_captions, vocab\n",
    "\n",
    "def indices_to_words(tensor_indices, vocab, rm_pad=True):\n",
    "    index_to_word = {index: word for word, index in vocab.items()}    \n",
    "    words = [index_to_word.get(index.item(), '<unk>') for index in tensor_indices]\n",
    "    if rm_pad:\n",
    "        words = [word for word in words if word != '<pad>']    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)\n",
    "print(f'Matrix dim: {padded_captions.shape}')\n",
    "print(f'Länge Wörterbuch: {len(vocab)}')\n",
    "padded_captions[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Das Wörterbuch aus dem Trainingsset umfasst insgesamt 2484 Wörter. Ein Beispiel Tensor bei dem die Wort Captions durch Nummerische Captions, anhand des Wörterbuches, ersetzt wurden ist oben ersichtlich. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Vortrainiertes Modell\n",
    "Neben dem Modell aus dem Paper soll auch ein vortrainiertes Modell verwendet werden. Dazu werden bereits trainierte Wordempeddings eingesetzt. Verschiedene Möglichkeiten stehen zur Verfügung `Word2Vec`, `FastText`, `GloVe` und viele mehr. Hier wurde GloVe verwendet da eine kurze Suche gleich zum [Download](https://nlp.stanford.edu/projects/glove/) der Wordempeddings führte, auch steht GloVe via Pytorch zur Verfügung. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import GloVe\n",
    "# glove = GloVe(name='6B', dim=300)\n",
    "# glove_embeddings = glove.vectors\n",
    "# print(glove_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path='./data/glove.6B.300d.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        vocab = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            vocab[word] = vector\n",
    "    return vocab\n",
    "\n",
    "glove_embeddings = load_glove_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim = 300):\n",
    "    embedding_matrix = torch.zeros((len(vocab)+2, embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Zufälliger Vektor für Wörter, die nicht in GloVe sind\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Word Embeddings sind eine Methode zur Darstellung von Wörtern in einem Vektorraum, bei der ähnliche Wörter ähnliche Vektoren haben. Diese Vektoren fassen viele Aspekte der Wortbedeutung ein und ermöglichen es Modellen, die semantischen Beziehungen zwischen Wörtern zu erkennen.\n",
    "\n",
    "Die hier verwendete CUDA Version unterstütze die Verwendung von torchtext nicht und GLoVe konnte nicht direkt benutzt werden. Daher wurden die GloVe-Daten heruntergeladen und manuell mit dem Trainings Wörterbuch verknüpft. Für jedes Wort in unserem Wörterbuch werden die Wordempeddings von GLoVe extrahiert. Das führt zu einer Matrix mit 2486 Wörter und einen Vektorraum mit 300 Dimension die die Abstände zwischen den Wörter darstellen. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Erstellen des Dataloaders\n",
    "Folgend wird der Dataloader für das Modell Training definiert. Die Ausgabe umfasst das Bild (torch.tensor), die Wort Caption, die Nummerische Caption, die Caption Embeddings und der Bild Name. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, csv_file_name, root_dir, vocab, embedding_matrix, transform=None):\n",
    "        self.captions_frame = pd.read_csv(csv_file_name)\n",
    "        self.root_dir = root_dir\n",
    "        self.vocab = vocab\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.captions_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        caption = self.captions_frame.iloc[idx,1]\n",
    "        image_name = self.captions_frame.iloc[idx,0]\n",
    "        tokenized_caption = tokenize_en(caption, lower_text=True)\n",
    "\n",
    "        caption_indices = [self.vocab.get(token, self.vocab['<pad>']) for token in tokenized_caption]\n",
    "        \n",
    "        # Umwandeln der Liste von Indizes in einen Tensor\n",
    "        caption_indices_tensor = torch.tensor(caption_indices, dtype=torch.long)\n",
    "\n",
    "        # Extrahieren der Embeddings für die Indizes\n",
    "        caption_embeddings = torch.stack([self.embedding_matrix[idx] for idx in caption_indices])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption, caption_indices_tensor, caption_embeddings, image_name\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, caption, caption_indices, caption_embeddings, image_name = zip(*batch)\n",
    "\n",
    "        # Pad die caption_indices und caption_embeddings\n",
    "        caption_indices_padded = pad_sequence(caption_indices, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "        caption_embeddings_padded = pad_sequence(caption_embeddings, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "\n",
    "        images = torch.stack(images)  # Stapeln der Bilder zu einem Tensor\n",
    "\n",
    "        return images, caption, caption_indices_padded, caption_embeddings_padded, image_name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Testen des Datensets\n",
    "image, caption, caption_indices, caption_embeddings, image_name = train_set[0]\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(image)\n",
    "plt.suptitle(f'{caption}', fontsize=10)\n",
    "plt.title(f'Länge caption indices: {len(caption_indices)}, Länge caption_embeddings: {len(caption_embeddings)}', fontsize=8)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen und Testen der Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "    std=[1 / s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "test_set = FlickrDataset(\n",
    "    csv_file_name='./data/test_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=train_set.collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=test_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen des Dataloader, prüfen ob die caption_empeddings pro batch gleich lang sind\n",
    "for i_batch, (image, caption, caption_indices, caption_embeddings, image_name) in enumerate(train_dataloader):\n",
    "    print(f'Länge caption_indices 1: {len(caption_indices[0])}, Länge caption_embeddings 1: {len(caption_embeddings[0])}')\n",
    "    print(f'Länge caption_indices 2: {len(caption_indices[1])}, Länge caption_embeddings 2: {len(caption_embeddings[1])}')\n",
    "    print(f'Länge caption_indices 3: {len(caption_indices[2])}, Länge caption_embeddings 3: {len(caption_embeddings[2])}')\n",
    "    print(f'Länge caption_indices 4: {len(caption_indices[3])}, Länge caption_embeddings 4: {len(caption_embeddings[3])}')\n",
    "    print()\n",
    "    print('Example Image 1/4')\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    img_tensor_denorm  = denormalize(image[0])\n",
    "    img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "    plt.imshow(img_pil)\n",
    "    plt.suptitle(caption[0])\n",
    "    plt.title(f'wörter from caption_indices: {indices_to_words(caption_indices[0], vocab)}', fontsize=7)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Wie weiter oben erwähnt, sollten die Captions für jeden Batch die gleiche Länge haben. Anstatt einfach alle Captions auf die maximale Länge der längsten Caption in Trainingsset zu verlänger wurde eine `collate_fn()` erstellt. Dies kann in der Dataloader Methode aufgerufen werden und prüft eine ganzen Batch. Jede Caption in ein Batch wird verlängert so dass diese gleich lang sind wie die längste Caption im Batch. \n",
    "\n",
    "Die Idee ist das nicht alle Captions auf die maximale Länge von 44 erweitert werden müssen, was das Training effizienter machen könnte. Das ist soweit eher ein intuitive Entscheidung und auch eine Gelegenheit die `collate_fn()` zu testen. \n",
    "\n",
    "Da ein Resnet50 verwendet wird, mit den votrainierten Gewichten des ImageNet, werden die Bilder mit den gleichen Mittelwerten und Standardabweichungen wie die ImageNet Bilder normalisiert. Weiter wird ein Resizing der Bilder sowie ein random_crop ausgeführt.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Modellierung Modell nach Paper `Show and Tell`\n",
    "Folgen wird das Modell nachgebaut:\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob=0.5, glove_em=None):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Laden des vortrainierten ResNet-50 ohne den letzten Layer\n",
    "        resnet = models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
    "        \n",
    "        # Einbettungs-Layer für die Captions\n",
    "        if glove_em is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(glove_em, freeze=True)\n",
    "            print('using glove')\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM für die Caption-Generierung\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Layer, um die Wort-Indizes vorherzusagen\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        # CNN-Teil\n",
    "        with torch.no_grad():  # Gradienten für ResNet nicht berechnen\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "\n",
    "        features = self.batch_norm(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Embedding und LSTM-Teil\n",
    "        captions_cut = captions.clone()\n",
    "        captions_cut[captions_cut>=self.vocab_size-1] = 0\n",
    "        embeddings = self.embedding(captions_cut)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        outputs_cut = outputs[:,:,0:self.vocab_size-1]\n",
    "        \n",
    "        return outputs_cut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Ob Batchnorm und Dropout verwendet wurde geht aus dem Paper nicht genau hervor. Es sind aber übliche Praktiken und zeigten auch in der Mini-Challeng 1 Vorteile. Daher wurden diese im Modell auch eingebunden. Mit dem Glove Parameter wird eingestellt ob die vortrainierten Wordempeddings verwendet werden sollen oder nicht. GloVe wird derzeit nicht trainiert (freeze=True) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Bleu Metrik\n",
    "\n",
    "Die Bleu-Metrik (Bilingual Evaluation Understudy) ist eine Metrik zur Bewertung der Qualität von Modellen erzeugten Texten und wird häufig als Standardmetrik angewendet.\n",
    "\n",
    "Es gibt mehrere Varianten des Blue-Scores, die sich hauptsächlich in der Anzahl der betrachteten N-Gramme unterscheiden. Ein N-Gramm ist eine zusammenhängende Sequenz von N Items aus einer gegebenen Textprobe. Der Blue-Score kann basierend auf unigrammen (einzelnen Wörtern), bigrammen (Paaren von aufeinanderfolgenden Wörtern), trigrammen und so weiter berechnet werden. Häufig werden Blue-Scores für verschiedene N-Gramme kombiniert, um eine Gesamtbewertung der Übersetzung zu geben.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_end_words(predicted_captions):\n",
    "    predicted_captions_trimmed = [caption.replace('<start> ', '') for caption in predicted_captions]\n",
    "    predicted_captions_trimmed = [caption.replace(' <end>', '') for caption in predicted_captions_trimmed]\n",
    "    return predicted_captions_trimmed\n",
    "\n",
    "\n",
    "def calculate_bleu(true_caption, predicted_caption, n_gram=1):\n",
    "    if isinstance(true_caption, str):\n",
    "        true_caption = tokenize_en(true_caption)\n",
    "    if isinstance(predicted_caption, str):\n",
    "        predicted_caption = tokenize_en(predicted_caption)\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "\n",
    "    if n_gram == 1:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(1,), smoothing_function=smoothing)\n",
    "    elif n_gram == 2:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.5, 0.5), smoothing_function=smoothing)\n",
    "    elif n_gram == 3:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.33, 0.33, 0.33), smoothing_function=smoothing)\n",
    "    elif n_gram == 4:\n",
    "        score = sentence_bleu([true_caption], predicted_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    else:\n",
    "        raise ValueError(\"N-Gram-Wert nicht implementiert\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "def calculate_blue_batch(true_captions, predicted_caption, n_gram=1, get_mean=True):\n",
    "    blue_scores_n = [calculate_bleu(true_cap, cap_pred, n_gram) for (true_cap, cap_pred) in zip(true_captions, predicted_caption)]\n",
    "    if get_mean:\n",
    "        return np.mean(blue_scores_n)\n",
    "    else:\n",
    "        return blue_scores_n\n",
    "\n",
    "true_caption = \"Das ist ein Test.\"\n",
    "predicted_caption = \"Dies ist ein Test.\"\n",
    "bleu_score = calculate_bleu(true_caption, predicted_caption, n_gram=4)\n",
    "print('Test Blue Score:')\n",
    "print(f\"Bleu Score: {bleu_score}\")\n",
    "print()\n",
    "\n",
    "predicted_caption = [\n",
    "    \"<start> a brown and white dog is running on the grass . <end>\",\n",
    "    \"<start> a brown and white dog is running on the grass . <end>\"\n",
    "]\n",
    "predicted_captions_trimmed = remove_start_end_words(predicted_caption)\n",
    "print('Test removing <start> und <end> von caption:')\n",
    "print(f'caption: {predicted_caption[0]}')\n",
    "print(f'caption trimmed: {predicted_captions_trimmed[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Training Modell\n",
    "Folgender Abschnitt widmet sich dem Modell Training und richtet sich nachdem Paper. Cross-Entropy (CE) Loss und Stochastisch Gradient Descent (SGD) wird verwendet. Die Modellparameter werden über ein Dictionary gesteuert. \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modell(config, model, dataloader, optimizer, criterion, epochs, device, test_batch=False):\n",
    "    set_seed(config['set_seed'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "    batch_losses = []\n",
    "\n",
    "    if config['write_wandb']: \n",
    "        model_name = f\"{config['name']}-{config['epochs']}-epochs-{config['start_time']}\"\n",
    "        wandb.init(\n",
    "            project=\"del-mc2\",\n",
    "            entity='manuel-schwarz',\n",
    "            group=config['group'],\n",
    "            name= model_name,\n",
    "            tags=str(config['tags']) + (' is_test_batch' if config['is_test_batch'] else ''),\n",
    "            config=config\n",
    "        )\n",
    "        wandb.watch(model)\n",
    "\n",
    "    if config['use_gpu_memory_snapshot']:\n",
    "        MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT = 100_000\n",
    "        torch.cuda.memory._record_memory_history(max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss = []\n",
    "        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "        for i, (images, captions, caption_indices, glove_embeddings, image_name) in loop:\n",
    "            if test_batch and i > 1:\n",
    "                break\n",
    "\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            outputs = model(images, captions_emb[:, :-1])  # Exclude the <end> token\n",
    "            output_shaped = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            targets = captions_emb[:, :].contiguous().view(-1)\n",
    "            targets_cut = targets.clone()\n",
    "            targets_cut[targets_cut >= config['vocab_size']-1] = 0\n",
    "\n",
    "            loss = criterion(output_shaped, targets_cut)\n",
    "            ep_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if config['use_gpu_memory_snapshot'] and epoch == 5:\n",
    "                file_name = f'{config[\"start_time\"]}_epoch_{epoch}_gpu_snapshot'\n",
    "                save_path = f'./gpu_snapshot/'\n",
    "                try:\n",
    "                    torch.cuda.memory._dump_snapshot(f\"{save_path}{file_name}.pickle\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to capture memory snapshot {e}\")\n",
    "        \n",
    "        epoch_losses.append(np.mean(ep_loss))\n",
    "\n",
    "        if config['write_wandb']:\n",
    "            wandb.log({\n",
    "                \"train loss epoch\": np.mean(ep_loss)\n",
    "                })\n",
    "                \n",
    "    if config['write_wandb']: \n",
    "        wandb.finish()\n",
    "        time.sleep(5)  # wait for wandb.finish\n",
    "\n",
    "    if config['use_gpu_memory_snapshot']:\n",
    "        torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "    print('Modell finished!')\n",
    "    play_sound(1)\n",
    "    return epoch_losses, batch_losses\n",
    "\n",
    "def plot_loss_model(epoch_losses, name='-'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epoch_losses, marker='o', color='skyblue', label='Training loss per epoch')\n",
    "    plt.title(f'Training Loss per Epoch (model: {name})')\n",
    "    plt.xlabel('Epoche')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, config):\n",
    "    path = f'./models/{config[\"start_time\"]}_{config[\"name\"]}_epochs_{config[\"epochs\"]}.pth'\n",
    "    torch.save(model, path)\n",
    "\n",
    "def load_model(model_name, device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    path = f'./models/{model_name}.pth'\n",
    "    model = torch.load(path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"name\": \"CNN_LSTM_name\",  # CNN_LSTM_glove\n",
    "    \"epochs\": 100,   \n",
    "    \"train_batch_size\": 128, \n",
    "    \"test_batch_size\": 64,\n",
    "    \"dataset\": \"flickr8k\",\n",
    "    \"lr\": 0.1, \n",
    "    \"optimizer\": 'SGD',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"image_size\": 256, \n",
    "    \"is_test_batch\": False,\n",
    "    \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "    \"num_workers\": 0,\n",
    "    \"dropout\": 0.5,\n",
    "    \"set_seed\": 42,\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 1,\n",
    "    'write_wandb':True,\n",
    "    'group': 'first model',\n",
    "    'tags': 'training',\n",
    "    'use_glove_emb': False,\n",
    "    'save_model': True,\n",
    "    'device': str(torch.device('cuda' if torch.cuda.is_available() else 'cpu')),\n",
    "    'use_gpu_memory_snapshot': False  # Achtung Beispiel code: snapshot nur für Linux verfügbar!\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=config['train_batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=train_set.collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=config['test_batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model1 = ImageCaptioningModel(\n",
    "    vocab_size = config['vocab_size'], \n",
    "    embedding_dim = config['embedding_dim'], \n",
    "    hidden_dim = config['hidden_dim'], \n",
    "    num_layers = config['num_layers'],\n",
    "    dropout_prob= config['dropout'],\n",
    "    glove_em = embedding_matrix if config['use_glove_emb'] else None\n",
    ")\n",
    "\n",
    "# model_name = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "# model1 = load_model(model_name)\n",
    "\n",
    "# optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=config['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if False:\n",
    "    epoch_losses, batch_losses = train_modell(\n",
    "        config,\n",
    "        model1,\n",
    "        train_dataloader,\n",
    "        optimizer, \n",
    "        criterion, \n",
    "        epochs=config['epochs'], \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        # device='cpu',\n",
    "        test_batch=config['is_test_batch']\n",
    "    )\n",
    "\n",
    "    if config['save_model']:\n",
    "        save_model(model1, config)\n",
    "        print('Modell saved!')\n",
    "\n",
    "    plot_loss_model(epoch_losses, 'model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Es wurden zwei Modelle trainiert, das erste verwendet eigene Wordembeddings die trainiert werden, das zweite verwendet die GloVe Wordembeddings. Die Dimension der Embeddings sind gleich gehalten. Ein Hyperparameter Tunning wurde nicht durchgeführt. \n",
    "\n",
    "Da nicht genau abgeschätzt werden konnte wie lange ein Modelltraining dauern sollte, werden die Modell gespeichert und so neu geladen und weiter trainiert werden.   \n",
    "\n",
    "Folgende Epochenstufen zur Verfügung:\n",
    "- Für Modell 1 (eigene Embeddings)\n",
    "    -  31.12.2023_1632_CNN_LSTM_epochs_30.pth, **total epochen: 30**\n",
    "    -  31.12.2023_1849_CNN_LSTM_b_epochs_30.pth, **total epochen: 60**\n",
    "    -  31.12.2023_2107_CNN_LSTM_c_epochs_90.pth, **total epochen: 150**\n",
    "    -  01.01.2024_0842_CNN_LSTM_d_epochs_30.pth, **total epochen: 180**\n",
    "    -  01.01.2024_1104_CNN_LSTM_e_epochs_30.pth, **total epochen: 210**\n",
    "    -  01.01.2024_1341_CNN_LSTM_f_epochs_30.pth, **total epochen: 240**\n",
    "- Für Modell 2 (Gloce Embeddings)\n",
    "    -  03.01.2024_1811_CNN_LSTM_glove_b_epochs_140.pth, **total epochen: 140**\n",
    "    -  04.01.2024_1846_CNN_LSTM_glove_c_epochs_100.pth, **total epochen: 240**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Modell Vorhersagen\n",
    "Folgend werden visuelle Beispiel der Modell gezeigt. Die vorhergesagte Caption der Modell sind in Rot dargestellt, die fünf True Captions in Grün.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_indices_to_sentences(batch_indices, vocab, rm_pad=True):\n",
    "    sentences = []\n",
    "    for tensor_indices in batch_indices:\n",
    "        words = indices_to_words(tensor_indices, vocab, rm_pad)\n",
    "        sentence = ' '.join(words)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def model_output_to_caption(model_prediction, vocab):\n",
    "    _, pred_indices = torch.max(model_prediction, dim=-1)\n",
    "    pred_captions = batch_indices_to_sentences(pred_indices, vocab)  \n",
    "    return pred_captions\n",
    "\n",
    "def plot_test_images(images, caption_pred, image_name, plot_num_img=4):\n",
    "    if plot_num_img > len(image_name):\n",
    "        plot_num_img = len(image_name)\n",
    "\n",
    "    pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "    for i in range(0, 5 * plot_num_img, 5):\n",
    "        pd_captions = pd_data[pd_data.image_name == image_name[i]].caption.reset_index(drop=True)\n",
    "        cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "        plt.figure(figsize = (10, 4))\n",
    "        img_tensor_denorm  = denormalize(images[i])\n",
    "        img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "        plt.imshow(img_pil)\n",
    "        plt.suptitle(f'pred: {caption_pred[i]}', color='red', y=1.15)\n",
    "        plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Testbilder, Model mit eigenen Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_name = '31.12.2023_1632_CNN_LSTM_epochs_30'\n",
    "# model_name = '31.12.2023_1849_CNN_LSTM_b_epochs_30'\n",
    "# model_name = '31.12.2023_2107_CNN_LSTM_c_epochs_90'\n",
    "# model_name = '01.01.2024_0842_CNN_LSTM_d_epochs_30'\n",
    "# model_name = '01.01.2024_1104_CNN_LSTM_e_epochs_30'\n",
    "model_name = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model1_loaded = load_model(model_name)\n",
    "\n",
    "model1_loaded.eval()\n",
    "\n",
    "for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "        pred = model1_loaded(images, captions_emb[:, :-1])\n",
    "        caption_pred = model_output_to_caption(pred, vocab)\n",
    "\n",
    "        plot_test_images(images, caption_pred, image_name, plot_num_img=4)\n",
    "\n",
    "        blue_scores_n1 = calculate_blue_batch(captions, caption_pred, 1)\n",
    "        blue_scores_n2 = calculate_blue_batch(captions, caption_pred, 2)\n",
    "        blue_scores_n3 = calculate_blue_batch(captions, caption_pred, 3)\n",
    "        blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "    break\n",
    "\n",
    "# caption_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Testbilder, Modell mit Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_glove_name = '01.01.2024_2227_CNN_LSTM_glove_a_epochs_100'\n",
    "# model_glove_name = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "model_glove_name = '04.01.2024_1846_CNN_LSTM_glove_c_epochs_100'\n",
    "model1_glove_loaded = load_model(model_glove_name)\n",
    "\n",
    "model1_glove_loaded.eval()\n",
    "\n",
    "for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "        pred = model1_glove_loaded(images, captions_emb[:, :-1])\n",
    "        caption_pred = model_output_to_caption(pred, vocab)\n",
    "\n",
    "        plot_test_images(images, caption_pred, image_name, plot_num_img=4)\n",
    "\n",
    "        blue_scores_n1 = calculate_blue_batch(captions, caption_pred, 1)\n",
    "        blue_scores_n2 = calculate_blue_batch(captions, caption_pred, 2)\n",
    "        blue_scores_n3 = calculate_blue_batch(captions, caption_pred, 3)\n",
    "        blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Beide Modelle machen Vorhersagen die nicht perfekt sind, aber sie zeigen dass die Modell grundsätzlich in der Lage sind die Bilder richtig einzuordnen. Mit den Satzenden haben beide Modelle Mühe. Auch kommt es vor das teilweise zwei Sätze vorhergesagt werden obwohl dass in den True Captions nicht der Fall ist, Wort Wiederholungen scheinen auch ein Problem zu sein. \n",
    "\n",
    "Für diesen kleinen Datensatz und der Fall dass die Modelle kein Hyperparameter Tunning durchlaufen, sind die Vorhersagen in den Beispielen nicht so schlecht.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Quantitative Modellperformance\n",
    "\n",
    "Um die Modelle quantitaiv zu untersuchen, wird folgend der Blue-Score für die Modelle berechnet. Es wird ein Bleu-Scores mit Ngram von 1 bis 4 angewendet. Die Start und End Tokens wurden dazu in den Captions entfernt.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Scores, Model mit eigenen Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions_blue_scores(model, testloader, trim_captions=True):\n",
    "    '''\n",
    "    loads the modell, makes predictions and returns the mean blue score for 1-4 gram\n",
    "    param:\n",
    "        trim_captions: removes the added <start> and <end> word in captions\n",
    "    '''\n",
    "    blue_scores_n1_full = []\n",
    "    blue_scores_n2_full = []\n",
    "    blue_scores_n3_full = []\n",
    "    blue_scores_n4_full = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            pred = model(images, captions_emb[:, :-1])\n",
    "            caption_pred = model_output_to_caption(pred, vocab)\n",
    "            if trim_captions:\n",
    "                caption_pred = remove_start_end_words(caption_pred)\n",
    "                captions = remove_start_end_words(captions)\n",
    "\n",
    "            blue_scores_n1_full.append(calculate_blue_batch(captions, caption_pred, 1))\n",
    "            blue_scores_n2_full.append(calculate_blue_batch(captions, caption_pred, 2))\n",
    "            blue_scores_n3_full.append(calculate_blue_batch(captions, caption_pred, 3))\n",
    "            blue_scores_n4_full.append(calculate_blue_batch(captions, caption_pred, 4))\n",
    "\n",
    "    mean_scores = [\n",
    "        np.mean(blue_scores_n1_full), \n",
    "        np.mean(blue_scores_n2_full),\n",
    "        np.mean(blue_scores_n3_full),\n",
    "        np.mean(blue_scores_n4_full)\n",
    "    ]\n",
    "    return mean_scores\n",
    "\n",
    "\n",
    "def plot_model_blue_score(mean_scores, title='title', figsize=(8, 4)):\n",
    "    labels = ['Blue 1-gram', 'Blue 2-gram', 'Blue 3-gram', 'Blue 4-gram']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.bar(labels, mean_scores, color='skyblue', zorder=3)\n",
    "    for i in range(len(labels)):\n",
    "        ax.text(i, mean_scores[i], f'{mean_scores[i]:.2f}', \n",
    "            ha='center', va='top', color='white')\n",
    "    plt.suptitle('Übersicht Model Blue Score zu verschieden n-grams')\n",
    "    plt.title(f'{title}', fontsize=9)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Blue score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0,0.6,0.1))\n",
    "    plt.grid(axis='y', color='silver', zorder=3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_blue_score_diff(mean_scores:list, mean_scores_glove:list, epochs=0, figsize=(10, 6)):\n",
    "    labels = ['Blue 1-gram', 'Blue 2-gram', 'Blue 3-gram', 'Blue 4-gram']\n",
    "\n",
    "    bar_width = 0.3\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bars1 = ax.bar(np.arange(len(labels)), mean_scores, bar_width, label='Eigene Embeddings', color='dodgerblue', zorder=3)\n",
    "    bars2 = ax.bar(np.arange(len(labels)) + bar_width, mean_scores_glove, bar_width, label='Glove Embeddings', color='orange', zorder=3)\n",
    "    for i in range(len(labels)):\n",
    "        ax.text(i, mean_scores[i], f'{mean_scores[i]:.3f}', \n",
    "            ha='center', va='top', color='white')\n",
    "        ax.text(i + bar_width, mean_scores_glove[i], f'{mean_scores_glove[i]:.3f}', \n",
    "            ha='center', va='top', color='white')        \n",
    "    ax.set_xticks(np.arange(len(labels)) + bar_width / 2)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks(np.arange(0,0.6,0.1))\n",
    "    ax.set_ylabel('Blue Score')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(f'Vergleich der Modelle, ohne und mit Glove Embeddings (Epochs {epochs})')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_epoch_240 = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model_loaded_epoch_240 = load_model(model_name_epoch_240)\n",
    "\n",
    "mean_scores = model_predictions_blue_scores(model_loaded_epoch_240, test_dataloader)\n",
    "# plot_model_blue_score(mean_scores, title='Eigene Wortembedings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Scores, Model mit Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove_name_epoch_240 = '04.01.2024_1846_CNN_LSTM_glove_c_epochs_100'\n",
    "model_glove_loaded_epoch_240 = load_model(model_glove_name_epoch_240)\n",
    "\n",
    "mean_scores_glove = model_predictions_blue_scores(model_glove_loaded_epoch_240, test_dataloader)\n",
    "# plot_model_blue_score(mean_scores_glove, title='Glove Wortembedings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blue Score Vergleich zwischen eigenen und Glove Wortembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_blue_score_diff(mean_scores, mean_scores_glove, epochs=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Der Vergleich mit Bleu zeigt, dass beide Modelle sehr eng beieinander liegen. Das Modell mit den GloVe Wordembeddings erziehlt jeweils einen etwas tieferen Score. Die Scores zeigen auch, dass die Modell eher schlecht abschneiden, wenn drei oder mehr Wörter als Sequenz korrekt vorherzusagen werden sollen (Blue 3-gram, Blue 4-gram). Da das Satzende der Modelle noch Schächen hat zieht es den Score dafür wohl stark herunter.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Modellvergleiche mit tieferen Epochen\n",
    "Da die 240 Epochen eher experimentell gewählt wurden, soll folgend den Enfluss von weniger Eochen untersucht werden: \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_epoch_150 = '31.12.2023_2107_CNN_LSTM_c_epochs_90'\n",
    "model_loaded_epoch_150 = load_model(model_name_epoch_150)\n",
    "mean_scores_150 = model_predictions_blue_scores(model_loaded_epoch_150, test_dataloader)\n",
    "\n",
    "# Glove Model\n",
    "model_glove_name_epoch_140 = '03.01.2024_1811_CNN_LSTM_glove_b_epochs_140'\n",
    "model_glove_loaded_epoch_140 = load_model(model_glove_name_epoch_140)\n",
    "mean_scores_glove_140 = model_predictions_blue_scores(model_glove_loaded_epoch_140, test_dataloader)\n",
    "\n",
    "# plot Modell Differenz\n",
    "plot_model_blue_score_diff(mean_scores_150, mean_scores_glove_140, epochs='150,140')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Scores, für die Modelle die bis 140 Epochen trainiert wurden, sind nur wenig tiefer als für die Modelle mit 240 Epochen. Vor allem die Scores des GLoVe Modells haben sich kaum verändert. Somit müssten diese Modell wohl nicht für 240 Epochen trainiert werden. Das eigenen Wordembedding Modell profitiert ein wenig von mehr Trainingszeit.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Suche nach der besten Caption\n",
    "Hier soll eine der besten Vorhersagen einer Caption gezeigt werden\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_predictions(model, testloader, trim_captions=True):\n",
    "    '''\n",
    "    loads the modell, makes predictions and returns the mean blue score for 1-4 gram\n",
    "    param:\n",
    "        trim_captions: removes the added <start> and <end> word in captions\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    best_blue_4_gram_score = 0\n",
    "\n",
    "    for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            pred = model(images, captions_emb[:, :-1])\n",
    "            caption_pred = model_output_to_caption(pred, vocab)\n",
    "            if trim_captions:\n",
    "                caption_pred = remove_start_end_words(caption_pred)\n",
    "                captions = remove_start_end_words(captions)\n",
    "\n",
    "            blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "            if blue_scores_n4 > best_blue_4_gram_score:\n",
    "                best_blue_4_gram_score = blue_scores_n4\n",
    "                best_image = images\n",
    "                best_caption_pred = caption_pred\n",
    "                best_true_caption = captions\n",
    "                best_image_name = image_name\n",
    "                print(f'best blue score 4 gram: {best_blue_4_gram_score:0.4f} on {best_image_name}')\n",
    "\n",
    "            if best_blue_4_gram_score == 1:\n",
    "                break\n",
    "\n",
    "    return best_image, best_image_name, best_true_caption, best_caption_pred, best_blue_4_gram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_bt_1 = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model_name_epoch_240 = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model_loaded_epoch_240 = load_model(model_name_epoch_240)\n",
    "\n",
    "image, image_name, true_caption, caption_pred, score = get_best_model_predictions(\n",
    "    model_loaded_epoch_240,\n",
    "    test_dataloader_bt_1\n",
    ")\n",
    "\n",
    "pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "pd_captions = pd_data[pd_data.image_name == image_name[0]].caption.reset_index(drop=True)\n",
    "cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "img_tensor_denorm  = denormalize(image[0])\n",
    "img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "plt.imshow(img_pil)\n",
    "plt.suptitle(f'best pred: {caption_pred[0]}', color='red', y=1.15)\n",
    "plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Suche nach der schlechtesten Caption\n",
    "Hier soll eine der schlechten Vorhersagen der Captions gezeigt werden\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_model_predictions(model, testloader, trim_captions=True):\n",
    "    '''\n",
    "    loads the modell, makes predictions and returns the mean blue score for 1-4 gram\n",
    "    param:\n",
    "        trim_captions: removes the added <start> and <end> word in captions\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    worst_blue_4_gram_score = 0.3\n",
    "\n",
    "    for i, (images, captions, caption_indices, glove_embeddings, image_name) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            pred = model(images, captions_emb[:, :-1])\n",
    "            caption_pred = model_output_to_caption(pred, vocab)\n",
    "            if trim_captions:\n",
    "                caption_pred = remove_start_end_words(caption_pred)\n",
    "                captions = remove_start_end_words(captions)\n",
    "\n",
    "            blue_scores_n4 = calculate_blue_batch(captions, caption_pred, 4)\n",
    "            if blue_scores_n4 < worst_blue_4_gram_score:\n",
    "                worst_blue_4_gram_score = blue_scores_n4\n",
    "                worst_image = images\n",
    "                worst_caption_pred = caption_pred\n",
    "                worst_true_caption = captions\n",
    "                worst_image_name = image_name\n",
    "                print(f'best blue score 4 gram: {worst_blue_4_gram_score:0.4f} on {worst_image_name}')\n",
    "            \n",
    "            if worst_blue_4_gram_score == 0:\n",
    "                break\n",
    "\n",
    "    return worst_image, worst_image_name, worst_true_caption, worst_caption_pred, worst_blue_4_gram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_bt_1 = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model_name_epoch_240 = '01.01.2024_1341_CNN_LSTM_f_epochs_30'\n",
    "model_loaded_epoch_240 = load_model(model_name_epoch_240)\n",
    "\n",
    "image, image_name, true_caption, caption_pred, score = get_worst_model_predictions(\n",
    "    model_loaded_epoch_240,\n",
    "    test_dataloader_bt_1\n",
    ")\n",
    "\n",
    "pd_data = pd.read_csv('./data/pd_captions.csv')\n",
    "pd_captions = pd_data[pd_data.image_name == image_name[0]].caption.reset_index(drop=True)\n",
    "cap1, cap2, cap3, cap4, cap5 = pd_captions[0], pd_captions[1], pd_captions[2], pd_captions[3], pd_captions[4]\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "img_tensor_denorm  = denormalize(image[0])\n",
    "img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "plt.imshow(img_pil)\n",
    "plt.suptitle(f'worst pred: {caption_pred[0]}', color='red', y=1.15)\n",
    "plt.title(f'\\n true:{cap1} \\n true:{cap2} \\n true:{cap3} \\n true:{cap4} \\n true:{cap5}', color='green', fontsize=9)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Gute Captions**:  \n",
    "Hunde kommen im Datenset sehr oft vor, das zeigt auch die Verteilung der Tokens. Es überrascht daher wenig dass Bilder mit Hunden gute Ergebnisse liefern. Es wurde sogar einen perfekte Vorhersage mit einem Blue-Score 4-gram von 1 auf einem Bild generiert.\n",
    "\n",
    "**Schlechte Captions**:   \n",
    "dd \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reflexion:</b> \n",
    "\n",
    "text\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ende - Mini Challenge 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda117_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
