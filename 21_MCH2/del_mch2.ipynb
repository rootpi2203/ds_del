{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH2\n",
    "Fachdozent: Martin Melchior     \n",
    "Student: Manuel Schwarz   \n",
    "HS23\n",
    "\n",
    "Dieses Notebook bearbeitet die Mini-Challenge 2 des Moduls Deep Learning (del).   \n",
    "Die Performance der Modelle wurde mit **wandb.ai** aufgezeichnet und kann [hier](https://wandb.ai/manuel-schwarz/del-mc2/workspace?workspace=user-manuel-schwarz) eingesehen werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Aufgabenstellung:</b> Eine Blaue Box beschreibt die Aufgabe aus der Aufgabenstellung 'SGDS_DEL_MC1.pdf' \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Antworte:</b> Eine Grüne Box beschreibt die Bearbeitung / Reflektion der Aufgabenstellung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import spacy  # conda install -c conda-forge spacy + python -m spacy download en_core_web_sm\n",
    "import torchvision\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau Modellierung und Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Überlege Dir, welche Modell-Architektur Sinn machen könnte. Mindestens zwei Modell-Varianten sollen aufgebaut werden, die miteinander verglichen werden sollen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Für die del-MC2 Challenge wird das Modell vom Paper Vinyals et al `Show and Tell: A Neural Image Caption Generator` nachgebaut. Das Paper entwickelte ein Modell welches für Bilder eine Bildbeschreibung erstellt. Für die verwendeten Daten wird das `Flickr 8k` Datenset verwendet. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Flickr 8k lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = './data/Images'\n",
    "captions_file = './data/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_captions = pd.read_csv('./data/captions.txt', sep='\\t', header=None)\n",
    "pd_captions.columns = ['full_caption']\n",
    "pd_captions[['image_name', 'caption']] = pd_captions['full_caption'].str.split(',', n=1, expand=True)\n",
    "pd_captions.to_csv('./data/pd_captions.csv', index=False)\n",
    "pd_captions.drop('full_caption', axis=1, inplace=True)\n",
    "pd_captions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `caption.txt` File ist der Bildnamen und die Bildbeschreibung (caption) hinterlegt. Pro Bild stehen fünf Captions zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{pd_captions.image_name[image_id]}'\n",
    "example_caption1 = pd_captions.caption[image_id+0]\n",
    "example_caption2 = pd_captions.caption[image_id+1]\n",
    "example_caption3 = pd_captions.caption[image_id+2]\n",
    "example_caption4 = pd_captions.caption[image_id+3]\n",
    "example_caption5 = pd_captions.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bilddaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder_path = './data/Images/'\n",
    "\n",
    "resolutions = []\n",
    "for image_filename in os.listdir(images_folder_path):\n",
    "    if image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        image_path = os.path.join(images_folder_path, image_filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            resolutions.append(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_labels = [f\"{w}x{h}\" for w, h in resolutions]\n",
    "resolution_counts = Counter(dimension_labels)\n",
    "sorted_resolution_counts = dict(resolution_counts.most_common(50))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(sorted_resolution_counts.keys(), sorted_resolution_counts.values(), color='skyblue')\n",
    "\n",
    "plt.title('Verteilung der Bildauflösungen von Flickr 8k (Top 50)')\n",
    "plt.xlabel('Dimension (b x w)')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths, heights = zip(*resolutions)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(widths, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildbreiten')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['b'])\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.boxplot(heights, patch_artist=True, boxprops=dict(facecolor='grey', color='black'),\n",
    "            whiskerprops=dict(color='black'), capprops=dict(color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Verteilung der Bildhöhen')\n",
    "plt.ylabel('Pixel')\n",
    "plt.xticks([1], ['h'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untersuchen der Bildbeschreibungen\n",
    "\n",
    "Textdaten werden folgend in Tokens konvergiert:\n",
    "[doku torchtext](http://man.hubwiz.com/docset/torchtext.docset/Contents/Resources/Documents/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show torchtext\n",
    "# Version: 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "special_cases = [(\"<start>\", [{ORTH: \"<start>\"}]), (\"<end>\", [{ORTH: \"<end>\"}]), (\"<pad>\", [{ORTH: \"<pad>\"}])]\n",
    "for case in special_cases:\n",
    "    spacy_en.tokenizer.add_special_case(*case)\n",
    "\n",
    "def tokenize_en(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(caption)]\n",
    "    else:\n",
    "        return [tok.text for tok in spacy_en.tokenizer(caption)]    \n",
    "\n",
    "print(f'Test tokenize: {example_caption1}')\n",
    "tokens = tokenize_en(example_caption1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste mit allen vorhandenen Tokens \n",
    "token_series = pd_captions['caption'].apply(tokenize_en).explode()\n",
    "count_token = token_series.value_counts()\n",
    "count_token.index.name = 'token'\n",
    "count_token = count_token.reset_index()\n",
    "# count_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token = 50\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.bar(count_token.head(num_token).token, count_token.head(num_token).caption, color='skyblue')\n",
    "plt.title(f'Total Tokens: {len(count_token)} Tokens, dargestellt {num_token} Tokens')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en_len(caption, lower_text=False):\n",
    "    if lower_text:\n",
    "        return len([tok.text.lower() for tok in spacy_en.tokenizer(caption)])\n",
    "    else:\n",
    "        return len([tok.text for tok in spacy_en.tokenizer(caption)])\n",
    "\n",
    "token_series = pd_captions['caption'].apply(tokenize_en_len)\n",
    "# token_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3))\n",
    "token_mean = token_series.mean()\n",
    "token_std = token_series.std() / 2\n",
    "plt.hist(token_series, color='skyblue', bins=40)\n",
    "plt.axvline(token_mean, color='red', alpha=0.6, label=f'mean {token_mean:0.2f}')\n",
    "plt.axvspan(token_mean-token_std, token_mean+token_std, color='grey', alpha=0.2, label=f'std {token_std:0.2f}')\n",
    "plt.suptitle('Verteilung der Anzahl Tokens je Caption')\n",
    "plt.title(f'min bei: {token_series.min()}, max bei: {token_series.max()}', fontsize=8)\n",
    "plt.xlabel('Anzahl Tokens')\n",
    "plt.ylabel('Vorkommen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling\n",
    "\n",
    "- Im Paper werden Tokens die weniger als fünf mal vorkommen entfernt\n",
    "- Im Paper werden `start` und `end` Token eingeführt, diese sollen dem Modell helfen zu erkennen, wann der Beginn der Generierung einer Beschreibung ist und wann sie beendet werden sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_token = 5\n",
    "count_token_filtered = count_token[count_token.caption >= min_num_token]\n",
    "print(f'Total Tokens {len(count_token)}')\n",
    "print(f'Anzahl Tokens die mehr als {min_num_token} vorkommen: {len(count_token_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "\n",
    "pd_caption_mod = pd_captions.copy()\n",
    "pd_caption_mod['caption'] = pd_caption_mod['caption'].apply(lambda x: f\"{START_TOKEN} {x} {END_TOKEN}\")\n",
    "pd_captions.to_csv('./data/pd_captions_mod.csv', index=False)\n",
    "\n",
    "print(pd_captions['caption'][0])\n",
    "print(pd_captions['caption'][1])\n",
    "print(pd_captions['caption'][2])\n",
    "print()\n",
    "print(pd_caption_mod['caption'][0])\n",
    "print(pd_caption_mod['caption'][1])\n",
    "print(pd_caption_mod['caption'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Captions: {len(pd_caption_mod)}')\n",
    "unique_images = pd_caption_mod.image_name.unique()\n",
    "print(f'Anzahl Bilder: {len(unique_images)}')\n",
    "\n",
    "unique_images = list(pd_caption_mod.image_name.unique())\n",
    "train_images = random.sample(unique_images, k=int(len(unique_images) * 0.8))\n",
    "test_images = list(set(unique_images) - set(train_images))\n",
    "\n",
    "print(f'Länge Trainingsset: {len(train_images)}')\n",
    "print(f'Länge Testset: {len(test_images)}')\n",
    "\n",
    "pd_train_set = pd_caption_mod[pd_caption_mod.image_name.isin(train_images)]\n",
    "pd_test_set = pd_caption_mod[pd_caption_mod.image_name.isin(test_images)]\n",
    "pd_train_set.to_csv('./data/train_captions.csv', index=False)\n",
    "pd_test_set.to_csv('./data/test_captions.csv', index=False)\n",
    "\n",
    "print(f'Länge Trainingsset: {len(pd_train_set)}')\n",
    "print(f'Länge Testset: {len(pd_test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train_captions.csv')\n",
    "test_set = pd.read_csv('./data/test_captions.csv')\n",
    "\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 5\n",
    "\n",
    "example_image_path = f'{images_folder}/{train_set.image_name[image_id]}'\n",
    "example_caption1 = train_set.caption[image_id+0]\n",
    "example_caption2 = train_set.caption[image_id+1]\n",
    "example_caption3 = train_set.caption[image_id+2]\n",
    "example_caption4 = train_set.caption[image_id+3]\n",
    "example_caption5 = train_set.caption[image_id+4]\n",
    "image = Image.open(example_image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f'{example_caption1} \\n {example_caption2} \\n {example_caption3} \\n {example_caption4} \\n {example_caption5}')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearbeitung der Captions für Modelling (Trainingsset)\n",
    "- caption in matrix ablegen, gleiche länge der captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = train_set['caption'].apply(tokenize_en_len)\n",
    "print(f'maximale caption länge: {token_series.max()} Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_captions, min_freq):\n",
    "    # Zähle die Häufigkeit der Tokens in allen Captions\n",
    "    token_counts = Counter(token for caption in tokenized_captions for token in caption)\n",
    "\n",
    "    # Erstelle das Vokabular nur mit Tokens, die min_freq oder mehr Mal vorkommen\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<start>\": 1,\n",
    "        \"<end>\": 2\n",
    "    }\n",
    "    token_id = 3\n",
    "    for token, count in token_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[token] = token_id\n",
    "            token_id += 1\n",
    "    return vocab\n",
    "\n",
    "def create_matrices_from_captions(train_set):\n",
    "    captions = train_set['caption']\n",
    "    tokenized_captions = [tokenize_en(caption, lower_text=True) for caption in captions]\n",
    "\n",
    "    # Erstellen des Vokabular mit einer Mindesthäufigkeit von x\n",
    "    vocab = build_vocab(tokenized_captions, min_freq=5)\n",
    "\n",
    "    # Vokabular verwenden, um Ihre Captions in Indizes umzuwandeln\n",
    "    indexed_captions = [[vocab.get(token, vocab[\"<pad>\"]) for token in caption] for caption in tokenized_captions]\n",
    "\n",
    "    caption_tensors = [torch.tensor(caption) for caption in indexed_captions]\n",
    "\n",
    "    # Bestimmen der maximale Länge für das Padding\n",
    "    max_length = max(len(caption) for caption in caption_tensors)\n",
    "\n",
    "    # Padding hinzufügen, damit alle Captions die gleiche Länge haben\n",
    "    padded_captions = pad_sequence(caption_tensors, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "    return padded_captions, vocab\n",
    "\n",
    "def indices_to_words(tensor_indices, vocab, rm_pad=True):\n",
    "    index_to_word = {index: word for word, index in vocab.items()}    \n",
    "    words = [index_to_word.get(index.item(), '<unk>') for index in tensor_indices]\n",
    "    if rm_pad:\n",
    "        words = [word for word in words if word != '<pad>']    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)\n",
    "print(f'Matrix dim: {padded_captions.shape}')\n",
    "print(f'Länge Wörterbuch: {len(vocab)}')\n",
    "padded_captions[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Dataloaders\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import GloVe\n",
    "# glove = GloVe(name='6B', dim=300)\n",
    "# glove_embeddings = glove.vectors\n",
    "# print(glove_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_captions, vocab = create_matrices_from_captions(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path='./data/glove.6B.300d.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        vocab = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            vocab[word] = vector\n",
    "    return vocab\n",
    "\n",
    "glove_embeddings = load_glove_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, glove_embeddings, embedding_dim = 300):\n",
    "    embedding_matrix = torch.zeros((len(vocab)+2, embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Zufälliger Vektor für Wörter, die nicht in GloVe sind\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove_embeddings)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, csv_file_name, root_dir, vocab, embedding_matrix, transform=None):\n",
    "        self.captions_frame = pd.read_csv(csv_file_name)\n",
    "        self.root_dir = root_dir\n",
    "        self.vocab = vocab\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.captions_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        caption = self.captions_frame.iloc[idx,1]\n",
    "        tokenized_caption = tokenize_en(caption, lower_text=True)\n",
    "\n",
    "        caption_indices = [self.vocab.get(token, self.vocab['<pad>']) for token in tokenized_caption]\n",
    "        \n",
    "        # Umwandeln der Liste von Indizes in einen Tensor\n",
    "        caption_indices_tensor = torch.tensor(caption_indices, dtype=torch.long)\n",
    "\n",
    "        # Extrahieren der Embeddings für die Indizes\n",
    "        caption_embeddings = torch.stack([self.embedding_matrix[idx] for idx in caption_indices])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption, caption_indices_tensor, caption_embeddings\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, caption, caption_indices, caption_embeddings = zip(*batch)\n",
    "\n",
    "        # Pad die caption_indices und caption_embeddings\n",
    "        caption_indices_padded = pad_sequence(caption_indices, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "        caption_embeddings_padded = pad_sequence(caption_embeddings, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "\n",
    "        images = torch.stack(images)  # Stapeln der Bilder zu einem Tensor\n",
    "\n",
    "        return images, caption, caption_indices_padded, caption_embeddings_padded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Testen des Datensets\n",
    "image, caption, caption_indices, caption_embeddings = train_set[0]\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.imshow(image)\n",
    "plt.suptitle(f'{caption}', fontsize=10)\n",
    "plt.title(f'Länge caption indices: {len(caption_indices)}, Länge caption_embeddings: {len(caption_embeddings)}', fontsize=8)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen und Testen des Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "    std=[1 / s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = FlickrDataset(\n",
    "    csv_file_name='./data/train_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "test_set = FlickrDataset(\n",
    "    csv_file_name='./data/test_captions.csv',\n",
    "    root_dir='./data/Images',\n",
    "    vocab=vocab,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    transform=transformations\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=train_set.collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=4, shuffle=False, collate_fn=test_set.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen des Dataloader, prüfen ob die caption_empeddings pro batch gleich lang sind\n",
    "for i_batch, (image, caption, caption_indices, caption_embeddings) in enumerate(train_dataloader):\n",
    "    print(f'Länge caption_indices 1: {len(caption_indices[0])}, Länge caption_embeddings 1: {len(caption_embeddings[0])}')\n",
    "    print(f'Länge caption_indices 2: {len(caption_indices[1])}, Länge caption_embeddings 2: {len(caption_embeddings[1])}')\n",
    "    print(f'Länge caption_indices 3: {len(caption_indices[2])}, Länge caption_embeddings 3: {len(caption_embeddings[2])}')\n",
    "    print(f'Länge caption_indices 4: {len(caption_indices[3])}, Länge caption_embeddings 4: {len(caption_embeddings[3])}')\n",
    "    print()\n",
    "    print('Example Image 1/4')\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    img_tensor_denorm  = denormalize(image[0])\n",
    "    img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "    plt.imshow(img_pil)\n",
    "    plt.suptitle(caption[0])\n",
    "    plt.title(f'wörter from caption_indices: {indices_to_words(caption_indices[0], vocab)}', fontsize=7)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellierung Modell nach Paper `Show and Tell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob=0.5, glove_em=None):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Laden des vortrainierten ResNet-50 ohne den letzten Layer\n",
    "        resnet = models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
    "        \n",
    "        # Einbettungs-Layer für die Captions\n",
    "        if glove_em is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(glove_em, freeze=True)\n",
    "            print('using glove')\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM für die Caption-Generierung\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Layer, um die Wort-Indizes vorherzusagen\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        # CNN-Teil\n",
    "        with torch.no_grad():  # Gradienten für ResNet nicht berechnen\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "\n",
    "        features = self.batch_norm(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Embedding und LSTM-Teil\n",
    "        embeddings = self.embedding(captions)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_modell(config, model, dataloader, optimizer, criterion, epochs, device, test_batch=False):\n",
    "    set_seed(config['set_seed'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "    batch_losses = []\n",
    "\n",
    "    # Initialize wandb\n",
    "    if config['write_wandb']: \n",
    "        model_name = f\"{config['name']}-{config['epochs']}-epochs-{config['start_time']}\"\n",
    "        wandb.init(\n",
    "            project=\"del-mc2\",\n",
    "            entity='manuel-schwarz',\n",
    "            group=config['group'],\n",
    "            name= model_name,\n",
    "            tags=str(config['tags']) + (' is_test_batch' if config['is_test_batch'] else ''),\n",
    "            config=config\n",
    "        )\n",
    "        wandb.watch(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss = []\n",
    "        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "        for i, (images, captions, caption_indices, glove_embeddings) in loop:\n",
    "            if test_batch and i > 1:\n",
    "                break\n",
    "\n",
    "            # if config['use_glove_emb']:\n",
    "            #     images, captions_emb2 = images.to(device), glove_embeddings.to(device)\n",
    "            #     # captions_emb2 = captions_emb2.long()\n",
    "            # else:\n",
    "            #     images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            images, captions_emb = images.to(device), caption_indices.to(device)\n",
    "\n",
    "            # outputs = model(images, captions_emb2[:, :-1])  # Exclude the <end> token\n",
    "            outputs = model(images, captions_emb[:, :-1])  # Exclude the <end> token\n",
    "            # targets = caption_indices[:, 1:].contiguous().view(-1)  # Exclude the <start> token\n",
    "            targets = captions_emb[:, :].contiguous().view(-1)\n",
    "\n",
    "            output_shaped = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # loss = criterion(outputs.view(-1, outputs.size(-1)), targets)\n",
    "            loss = criterion(output_shaped, targets)\n",
    "            batch_losses.append(loss.item())\n",
    "            ep_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(np.mean(ep_loss))\n",
    "        # print(f'Epoch {epoch}    Loss: {np.mean(ep_loss)}')\n",
    "\n",
    "        if config['write_wandb']:\n",
    "            wandb.log({\n",
    "                \"train loss epoch\": np.mean(ep_loss)\n",
    "                })\n",
    "            \n",
    "    if config['write_wandb']: \n",
    "        wandb.finish()\n",
    "        time.sleep(5)  # wait for wandb.finish\n",
    "\n",
    "    print('Modell finished!')\n",
    "    play_sound(1)\n",
    "    return epoch_losses, batch_losses\n",
    "\n",
    "def plot_loss_model(epoch_losses, name='-'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epoch_losses, marker='o', color='skyblue', label='Training loss per epoch')\n",
    "    plt.title(f'Training Loss per Epoch (model: {name})')\n",
    "    plt.xlabel('Epoche')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, config):\n",
    "\ttorch.save({\n",
    "\t\t'state_dict': model.state_dict(),\n",
    "\t}, f'./models/{config[\"start_time\"]}_{config[\"name\"]}_epochs_{config[\"epochs\"]}.tar')\n",
    "    \n",
    "def load_model(model, model_name, config):\n",
    "\tdata = torch.load(f'./models/{model_name}.tar', map_location=device)\n",
    "\tmodel.load_state_dict(data['state_dict'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Hyperparameters example\n",
    "config = {\n",
    "    \"name\": \"CNN_LSTM\", \n",
    "    \"epochs\": 2,   \n",
    "    \"train_batch_size\": 4, \n",
    "    \"test_batch_size\": 2,\n",
    "    \"dataset\": \"flickr8k\",\n",
    "    \"lr\": 0.1, \n",
    "    \"optimizer\": 'SGD',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"image_size\": 256,\n",
    "    \"is_test_batch\": False,\n",
    "    \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "    \"num_workers\": 0,\n",
    "    \"dropout\": 0.5,\n",
    "    \"set_seed\": 42,\n",
    "    'vocab_size': len(vocab)+1,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 1,\n",
    "    'write_wandb':True,\n",
    "    'group': 'cpu test',\n",
    "    'tags': 'tests',\n",
    "    'use_glove_emb': True,\n",
    "    'save_model': True\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=config['train_batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=train_set.collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=config['test_batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=test_set.collate_fn\n",
    ")\n",
    "\n",
    "model1 = ImageCaptioningModel(\n",
    "    vocab_size = config['vocab_size'], \n",
    "    embedding_dim = config['embedding_dim'], \n",
    "    hidden_dim = config['hidden_dim'], \n",
    "    num_layers = config['num_layers'],\n",
    "    dropout_prob= config['dropout'],\n",
    "    glove_em = embedding_matrix if config['use_glove_emb'] else None\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=config['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_losses, batch_losses = train_modell(\n",
    "    config,\n",
    "    model1,\n",
    "    train_dataloader,\n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs=5, \n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    test_batch=config['is_test_batch']\n",
    ")\n",
    "\n",
    "if config['save_model']:\n",
    "    save_model(model1, config)\n",
    "    print('Modell saved!')\n",
    "\n",
    "plot_loss_model(epoch_losses, 'model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell Vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '30.12.2023_1630_CNN_LSTM_epochs_2'\n",
    "loaded_model = load_model(model1, model_name, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda117_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
