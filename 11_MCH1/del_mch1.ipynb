{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH1\n",
    "FS23, Manuel Schwarz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import wandb\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# sound\n",
    "import winsound\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 1: Auswahl Task / Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dir Gedanken, mit welchen Daten Du arbeiten möchtest und welcher Task gelernt werden soll.\n",
    "    \n",
    "2. Diskutiere die Idee mit dem Fachcoach.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten und Task\n",
    "Pytorch stellt einige Datensets zur Verfügung [datasets torch](https://pytorch.org/vision/main/datasets.html).\n",
    "Verschiedene Kategorien stehen zur Auswahl:\n",
    "- Image classification\n",
    "- Image detection or segmentation\n",
    "- Optical Flow\n",
    "- Stereo Matching\n",
    "- Image pairs\n",
    "- Image captioning\n",
    "- video classification\n",
    "- Base classes for custom datasets\n",
    "\n",
    "\n",
    "**Datenset**  \n",
    "Eine beliebtes Dateset ist CIFAR10. Es beinhaltet Bilder von 10 Klassen (Flugzeuge, Katzen, Vögel, etc.), die Bilder kommen mit einer Auflösung von 32x32x3 pixel (rgb). Viele Tutorials starten mit diesem Datenset, das lässt darauf schliessen, dass der Rechenaufwand für die Hardware in einem vernümpftigen Rahmen liegt. Daher wird CIFAR10 als Datenset für die Challenge gewählt.\n",
    "\n",
    "**Task**  \n",
    "Anhand von CIFAR10 soll ein Modell erstellt werden, welches die Klasse eines Bildes korrekt klassifiziert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 2: Daten Kennenlernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dich mit dem Datensatz vertraut, indem Du eine explorative Analyse der Features durchführst: z.B. Vergleich der Klassen pro Feature, Balanciertheit der Klassen. \n",
    "2. Führe ein geeignetes Preprocessing durch, z.B. Normalisierung der Daten.  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explorative Datenanalyse\n",
    "[Tutorial Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten von CIFAR10\n",
    "data_path = './data/'\n",
    "train_data = torchvision.datasets.CIFAR10(data_path, train=True, download=True)\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datengrösse der Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Trainingsdaten: {len(train_data)}\\n'\n",
    "      f'Anzahl Testdaten: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie sind die Bilder im Datensatz gespeichert?**  \n",
    "Die Bilder sind direkt auf dem Datenset via dem Index abrufbar. Ein Tupel mit dem Bild (RGB, 32x32 pixel) und dem Label (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_0, label_0 = train_data[0]\n",
    "img_0, label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deffinieren der Labels CIFAR10\n",
    "labels_cifar10_dict = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "labels_cifar10_list = list(labels_cifar10_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisierung der Bilder**  \n",
    "Die Bilder können mit matplotlib und imshow() direkt vom Dateset über den index dargestellt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6,6))\n",
    "cols, rows = 5, 5\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_cifar10_list[label], fontsize=8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Untersuchen der Verteilungen der Klassen**  \n",
    "Folgend werden die Verteilungen der Klassen der Cifar10 Datensets auf den Trainings- und Testdaten geprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_data.targets\n",
    "test_target = test_data.targets\n",
    "print(f'Labels in Trainingsdaten: {len(train_target)}')\n",
    "print(f'Labels in Testdaten: {len(test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "ax[0].hist(train_target)\n",
    "ax[0].set_xticks(np.arange(10))\n",
    "ax[0].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[0].set_title('Trainingset', fontsize=8)\n",
    "\n",
    "ax[1].hist(test_target)\n",
    "ax[1].set_xticks(np.arange(10))\n",
    "ax[1].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[1].set_title('Testset', fontsize=8)\n",
    "\n",
    "plt.suptitle('Verteilung der Klassen von Cifar-10', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Das Trainingsset umfasst total 50'000 Bilder, davon sind jeweils 5000 Bilder jeder Klasse enthalten. Somit kann zum Beispiel eine Metrik wie'Accuracy' verwendet werden, um die Klassifikation der Modelle zu beurteilen und zu vergleichen. Die 10'000 Bilder in den Testdaten sind ebenfalls gleich verteilt.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing der Daten  \n",
    "Folgend werden die Daten in einem Preprocessing Schritt für die Modelle vorbereitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function preprocessing\n",
    "def preprocessing_cifar10(path='./data/', batch_size=32, \n",
    "                          norm_mean=(0.5, 0.5, 0.5), norm_std=(0.5, 0.5, 0.5), \n",
    "                          download=True, print_info=False):\n",
    "    '''\n",
    "    '''\n",
    "    if print_info: print(f'------------'), print(f'Preprocessing start')\n",
    "    # transform tensor to normalized range [-1, 1]\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])\n",
    "    if print_info: print('Normalized Tensor'), print(f'mean: {norm_mean} | std: {norm_std}')\n",
    "\n",
    "    \n",
    "    # CIFAR10: 50000 32x32 color images in 10 classes, with 5000 images per class\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                            download=download, transform=transform)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                        download=download, transform=transform)\n",
    "    if print_info: print('Data transformed')\n",
    "\n",
    "    # dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    if print_info: print(f'Dataloader created with {batch_size=}')\n",
    "\n",
    "    if print_info: print(f'Preprocessing done'), print()   \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "data_path = './data/'\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(path=data_path, \n",
    "                                                                               batch_size=batch_size,\n",
    "                                                                               norm_mean=(0.5, 0.5, 0.5), \n",
    "                                                                               norm_std=(0.5, 0.5, 0.5),\n",
    "                                                                               download=False,  \n",
    "                                                                               print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 3: Aufbau Modellierung  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Lege fest, wie (mit welchen Metriken) Du die Modelle evaluieren möchtest. Berücksichtige auch den Fehler in der Schätzung dieser Metriken.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Klassengrössen sind ausbalanciert, daher ist für die Metrik **Accuracy** für die Modell Beurteilung geieignet und soll hier zum Einsatz kommen:\n",
    "\n",
    "$$ Accuracy = \\frac{Anzahl\\ korrekte\\ Klassifizierungen}{Total\\ Klassifizierungen}  $$\n",
    "\n",
    "Für eine Klassifizierung von mehr als zwei Klassen eignet sich **CrossEntropyLoss**. Mehr dazu [hier](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "$$ \\text{CrossEntropyLoss} = -\\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right] $$\n",
    "\n",
    "\n",
    "Die Accuracy die berechnet wird entspricht einer Schätzung. Bei der Initialisierung der Modellgewichte werden Zufallswerte verwendet. Auch die Batchsize wird durch `shuffle=True` mit unterschiedlicher Aufteilungen erstellt *(siehe `Preprocessing`)*. Somit varriert die Accuracy nach jedem Modeltraining ein wenig. Cross-Validation würde sich hier anbieten, um auf k-folds unterschiedliche Modelle zu erstellen. Mit berechneten Mittelwert $\\mu$ und Standardabweichung $\\sigma$ kann ein Fehlerabschätzung gemacht werde.  \n",
    "$$ Fehler_{range} = [\\mu - \\sigma; \\mu + \\sigma]$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "2. Implementiere Basisfunktionalität, um Modelle zu trainieren und gegeneinander zu evaluieren. Wie sollen die Gewichte initialisiert werden?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Methode wie die Intitialisierung der Gewichte stattfindet hat Einfluss wie schnell die das Modell konvergiert und hilft die Probleme von `vanishing` oder `exploding` Gradienten abzuschwächen. Kleine zufällige Werte führen zu einem effizienteren Trainig. Grosse Werte führen zu Problemenn bei dem das Modell nicht oder nur sehr langsam konvergiert. Je nach Problemstellung und Modelarchitekture können unterschiedliche Initialisierungsmethoden verwendet werden\n",
    "\n",
    "Mit der Verwendung von `nn.init` (Pytorch) stehen zum Beispiel folgende Optionen zur Verfügung:\n",
    "1. Uniform initialization\n",
    "1. Xavier initialization\n",
    "1. Kaiming initialization\n",
    "1. Zeros initialization\n",
    "1. One’s initialization\n",
    "1. Normal initialization\n",
    "\n",
    "\n",
    "Die genaue Beschreibung der Optionen kann [hier](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/) gefunden werden. Eine eigene Option stellt Pytorch auch zur Verfügung.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions  \n",
    "Um die Erkentnisse dieses Notebook reproduzierbar zu machen, ist es wichtig einen Seed zu definieren. Folgend eine Seed Funktion mit einem üblichen Standart [Quelle](https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def plot_loss_epoch(num_epochs, loss, figsize=(8, 4)):\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "    plt.plot(np.arange(num_epochs), loss)\n",
    "    plt.xticks(np.arange(num_epochs))\n",
    "    plt.title('Loss Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def calc_true_predictions(output_model, true_labels):\n",
    "    _, predicted = torch.max(output_model.data, 1)\n",
    "    return (predicted == true_labels).sum().item()\n",
    "\n",
    "def measure_model_time(start_time, calc='min'):\n",
    "    model_time = np.round(((time.time() - start_time) / 60), 2)  # from seconds to minute\n",
    "    return model_time\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n",
    "\n",
    "def plot_init_weights(model, figsize=(6, 3)):\n",
    "    l1_weights = model.state_dict()['linear1.weight'].numpy()\n",
    "    l2_weights = model.state_dict()['linear2.weight'].numpy()\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize)\n",
    "\n",
    "    ax[0].hist(l1_weights.flatten(), bins=50)\n",
    "    ax[1].hist(l2_weights.flatten(), bins=50)\n",
    "\n",
    "    ax[0].set_title(\"Layer 1\", fontsize=8)\n",
    "    ax[0].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[0].set_ylabel(\"Anzahl\", fontsize=8)\n",
    "    ax[0].tick_params(axis='y', labelsize=6)\n",
    "    ax[0].tick_params(axis='x', labelsize=6)\n",
    "    ax[1].set_title(\"Layer 2\", fontsize=8)\n",
    "    ax[1].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[1].tick_params(axis='y', labelsize=6)\n",
    "    ax[1].tick_params(axis='x', labelsize=6)\n",
    "    plt.suptitle(f'Verteilung Initialisierungs Methode {model.init_methode}')\n",
    "    plt.show()\n",
    "\n",
    "def test_eval_plot():\n",
    "    num_epochs = 10\n",
    "    n_loss_epochs = (0.05 * np.sqrt(np.arange(10))) * -1\n",
    "    n_correct_train = 0.1 * n_loss_epochs**4\n",
    "    n_correct_test = 0.05 * n_loss_epochs**4\n",
    "    # create dataloader\n",
    "    train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(batch_size=4,\n",
    "                                                                                norm_mean=(0.5, 0.5, 0.5), \n",
    "                                                                                norm_std=(0.5, 0.5, 0.5),\n",
    "                                                                                download=False,  \n",
    "                                                                                print_info=False)\n",
    "    eval_model(num_epochs, n_loss_epochs, n_correct_train, n_correct_test,\n",
    "                train_loader, test_loader)\n",
    "# test_eval_plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(num_epochs, n_loss_epochs, n_correct_train, n_correct_test,\n",
    "               train_loader, test_loader, n_loss_batches=None, id_name='None', \n",
    "               figsize=(8,5), print_info=True):\n",
    "    \n",
    "    epoch = np.arange(num_epochs)\n",
    "    acc_train_iter = np.array(n_correct_train) / len(train_loader.dataset)\n",
    "    acc_test_iter = np.array(n_correct_test) / len(test_loader.dataset)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(epoch, n_loss_epochs, color= 'grey', label='Loss')\n",
    "    ax2.plot(epoch, acc_train_iter, color='steelblue', label='Accuracy Train')\n",
    "    ax2.plot(epoch, acc_test_iter, color='coral', label='Accuracy Test')\n",
    "    \n",
    "    ax1.set_xticks(epoch)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss') \n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.tick_params(axis='x', labelsize=8)\n",
    "    ax1.tick_params(axis='y', labelsize=8)\n",
    "    ax2.tick_params(axis='y', labelsize=8)\n",
    "    ax1.legend(loc='upper center', bbox_to_anchor=(0.25, -0.12), ncol=3)    \n",
    "    ax2.legend(loc='upper center', bbox_to_anchor=(0.6, -0.12), ncol=3)\n",
    "    \n",
    "    plt.title(f'Training: {id_name}')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    if print_info:\n",
    "        print(f'Accuracy Train {acc_train_iter[-1]:2f}')\n",
    "        print(f'Accuracy Test {acc_test_iter[-1]:4f}')\n",
    "        print(f'Loss Train {n_loss_epochs[-1]:2f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model function\n",
    "def train_model(model, train_loader, test_loader,\n",
    "                loss_func='CrossEntropyLoss', opt='SGD', \n",
    "                lr=1e-4, num_epochs=10, id_name='test',\n",
    "                print_info=False, plot_eval=True, sound=False,\n",
    "                write_wandb=False):  \n",
    "    set_seed()\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "    print(f'------------'), print(f'Starten des Trainings auf Device {device}')\n",
    "\n",
    "    # Model to device\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # define Loss and Optimizer\n",
    "    if loss_func == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if print_info: print(f'Loss Funktion: {loss_func}')\n",
    "    if opt == 'SGD':        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        if print_info: print(f'Optimizer: {opt} mit lr: {lr}')\n",
    "\n",
    "    # train loop\n",
    "    #n_total_steps = len(train_loader)\n",
    "    n_loss_epochs = []\n",
    "    n_loss_all_batches = []\n",
    "    n_correct_train = []\n",
    "    n_correct_test = []\n",
    "    acc_train_epoch = []\n",
    "    acc_test_epoch = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        true_train = 0\n",
    "        n_loss_batch = []\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "            # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            n_loss_batch.append(loss.item())\n",
    "            n_loss_all_batches.append(loss.item())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            true_train += calc_true_predictions(outputs, labels)\n",
    "            acc_train = true_train / len(train_loader.dataset)\n",
    "\n",
    "        # save results\n",
    "        acc_train = true_train / len(train_loader.dataset)\n",
    "        epoch_loss = np.mean(n_loss_batch)\n",
    "\n",
    "        n_correct_train.append(true_train)        \n",
    "        n_loss_epochs.append(epoch_loss)\n",
    "        acc_train_epoch.append(acc_train)  \n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            true_val = 0 \n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                true_val += calc_true_predictions(outputs, labels)                \n",
    "\n",
    "            n_correct_test.append(true_val)\n",
    "        model.train()\n",
    "\n",
    "        acc_test = true_val / len(test_loader.dataset)\n",
    "        acc_test_epoch.append(acc_test)            \n",
    "        \n",
    "        if print_info:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # log metrics to wandb\n",
    "        if write_wandb:\n",
    "            wandb.log({\"loss epoch\": epoch_loss, \"acc_train\": acc_train, \"acc_test\": acc_test,})\n",
    "\n",
    "    acc_train = n_correct_train[-1] / len(train_loader.dataset)\n",
    "    acc_test = n_correct_test[-1] / len(test_loader.dataset)\n",
    "\n",
    "    if plot_eval:\n",
    "        eval_model(num_epochs, n_loss_epochs, n_correct_train, n_correct_test,\n",
    "                   train_loader, test_loader, id_name=id_name, print_info=print_info)\n",
    "    if sound: play_sound(1)\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "    return epoch_loss, acc_train, acc_test\n",
    "\n",
    " #if (i+1) % 2000 == 0:\n",
    "            #    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfacher Modell Test  \n",
    "Folgend soll ein einfaches Modell mit einem hidden Layer erstellt werden um die Basisfunktionen von `train_model()` zu testen. \n",
    "\n",
    "Initilisierung Gewichte: \n",
    "Da unterschiedliche Problemstellung verschiedene Initialisierungen erfordern, sollen mehrere Methoden ausprobiert werden um die Gewichte zu initialisiern `linear_layer = torch.nn.Linear(2, 3)`:\n",
    "1. `uniform`: torch.nn.init.uniform_(linear_layer.weight)\n",
    "1. `xavier`: torch.nn.init.xavier_uniform_(linear_layer.weight)\n",
    "1. `normal`: torch.nn.init.normal_(linear_layer.weight, mean=0, std=1)\n",
    "1. `kaiming_norm`: torch.nn.init.kaiming_normal_(linear_layer.weight, nonlinearity=\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet_hl1(nn.Module):\n",
    "    def __init__(self, hidden_l_1:int, act_fn=F.relu, dropout=0, init_methode:str='kaiming_norm'):\n",
    "        super(MLPNet_hl1, self).__init__()\n",
    "        self.init_methode = init_methode\n",
    "        self.linear1 = nn.Linear(3*32*32, hidden_l_1)  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(hidden_l_1, 10)\n",
    "        self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "       \n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) # x.shape = (n, 10)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def init_dits_weight(self, linear_weight):\n",
    "        if self.init_methode == 'uniform':\n",
    "            return torch.nn.init.uniform_(linear_weight)\n",
    "        if self.init_methode == 'xavier':\n",
    "            return torch.nn.init.xavier_uniform_(linear_weight)\n",
    "        if self.init_methode == 'normal':\n",
    "            return torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "        if self.init_methode == 'kaiming_norm':\n",
    "            return torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_lst = ['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "# create model\n",
    "for method in method_lst:\n",
    "    model = MLPNet_hl1(hidden_l_1=64, init_methode=method)\n",
    "    plot_init_weights(model, figsize=(5, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilungen der Gewichte mit den Methoden `uniform` und  `xavier` sind sehr ähnlich. Mit Xavier ist die X-Skala unterschiedlich, da die Gewichte so skalliert werden dass die Varianz des Output der Varianz des Inputs entspricht.  \n",
    "Bei `normal` und `kaiming_norm` besteht das gleiche Prinzip, die skallierung der Gewichte, zudem wird die Aktivierungsfunktion berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_data_to_wand = True\n",
    "exp_name = \"simple mlp (1hl)\"\n",
    "project_name = \"del-mc1\"\n",
    "\n",
    "#--------- Hyperparameters ---------- \n",
    "config = {\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"name\": exp_name,\n",
    "    \"architecture\": \"MLP\", \n",
    "    \"num_epochs\": 20,   \n",
    "    \"batchsize\": 64,\n",
    "    \"lr\": 1e-3, \n",
    "    \"opt\": 'SGD',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"hidden_layer_sizes\": [128], \n",
    "    \"kernel_sizes\": [],\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"pool_sizes\": [],\n",
    "    \"dropout\": 0,\n",
    "    \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "    \"norm_std\": (0.5, 0.5, 0.5),\n",
    "    \"num_classes\": 10,\n",
    "    \"init_w_method\":\"kaiming_norm\"  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "}\n",
    "\n",
    "if send_data_to_wand:\n",
    "    wandb.init(project=project_name, name=exp_name, notes=\"\", config=config)\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(batch_size=config[\"batchsize\"],\n",
    "                                                                               norm_mean=config[\"norm_mean\"], \n",
    "                                                                               norm_std=config[\"norm_std\"],\n",
    "                                                                               download=False,  \n",
    "                                                                               print_info=False)\n",
    "# create model\n",
    "MLPNet_hl1(hidden_l_1=config[\"hidden_layer_sizes\"][0], \n",
    "           init_methode=config[\"init_w_method\"])\n",
    "\n",
    "# train model\n",
    "train_model(model, train_loader, test_loader,\n",
    "            loss_func=config[\"loss_func\"], \n",
    "            opt=config[\"opt\"], \n",
    "            lr=config[\"lr\"], \n",
    "            num_epochs=config[\"num_epochs\"], \n",
    "            id_name=config[\"name\"],\n",
    "            print_info=True, plot_eval=True, sound=True, write_wandb=True)\n",
    "\n",
    "\n",
    "# Weights & Biases save Parameter\n",
    "if send_data_to_wand:\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelltraining Parametereinstellungen mit wandb sweep [Anleitung](https://docs.wandb.ai/ref/python/sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 4: Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Bei der Evaluation ist darauf zu achten, dass das Vorgehen stets möglichst reflektiert erfolgt und versucht wird, die Ergebnisse zu interpretieren. Am Schluss soll auch ein Fazit gezogen werden, darüber welche Variante am besten funktioniert.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**1. Training mit SGD, ohne REG, ohne BN:**  \n",
    "Untersuche verschiedene Modelle unterschiedlicher Komplexität, welche geeignet sein könnten, um das Klassifikationsproblem zu lösen. Verwende Stochastic Gradient Descent - ohne Beschleunigung, ohne Regularisierung (REG) und ohne Batchnorm (BN).  \n",
    "\n",
    "a. Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein sorgfältiges Hyper-Parameter-Tuning durch (Lernrate, Batch-Grösse). Achte stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl Epochen, Lernrate und Batch-Grösse.\n",
    "\n",
    "b. Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning) verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding.\n",
    "\n",
    "c. Fasse die Ergebnisse zusammen in einem geeigneten Plot, bilde eine Synthese und folgere, welche Modell-Komplexität Dir am sinnvollsten erscheint.  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition von Modellen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell Class\n",
    "class MLPNet_hl1(nn.Module):\n",
    "    def __init__(self, hidden_l_1:int, act_fn=F.relu, dropout=0, init_methode:str='kaiming_norm'):\n",
    "        super(MLPNet_hl1, self).__init__()\n",
    "        self.init_methode = init_methode\n",
    "        self.linear1 = nn.Linear(3*32*32, hidden_l_1)  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(hidden_l_1, 10)\n",
    "        self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "       \n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) # x.shape = (n, 10)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Modell Class\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition wandb Trainingloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_1hl():\n",
    "        wandb.init()\n",
    "        config = wandb.config\n",
    "        run_name = f'{config[\"name\"]}_lr_{str(config[\"lr\"])}_layer_{str(config[\"hidden_layer_sizes\"])}_bs_{str(config[\"batchsize\"])}'\n",
    "        print(run_name)\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        # create dataloader\n",
    "        _, _, train_loader, test_loader = preprocessing_cifar10(batch_size=config[\"batchsize\"],\n",
    "                                                                                        norm_mean=config[\"norm_mean\"], \n",
    "                                                                                        norm_std=config[\"norm_std\"],\n",
    "                                                                                        download=False,  \n",
    "                                                                                        print_info=False)\n",
    "        # create model\n",
    "        MLPNet_hl1(hidden_l_1=config[\"hidden_layer_sizes\"][0], \n",
    "                init_methode=config[\"init_w_method\"])\n",
    "\n",
    "        # train model\n",
    "        train_model(model, train_loader, test_loader,\n",
    "                        loss_func=config[\"loss_func\"], \n",
    "                        opt=config[\"opt\"], \n",
    "                        lr=config[\"lr\"], \n",
    "                        num_epochs=config[\"num_epochs\"], \n",
    "                        id_name=config[\"name\"],\n",
    "                        print_info=False, plot_eval=False, sound=False, write_wandb=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MLP** Parameter Suche mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sweep = True\n",
    "exp_name = \"mlp\"\n",
    "num_epochs = 20\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": exp_name,\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\"name\": {\"values\": [exp_name]},                    \n",
    "                    \"loss_func\": {\"values\": ['CrossEntropyLoss']},\n",
    "                    \"opt\": {\"values\": [\"SGD\"]},\n",
    "                    \"init_w_method\": {\"values\": [\"kaiming_norm\"]},  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "                    \"norm_mean\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"norm_std\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"num_epochs\": {\"values\": [num_epochs]},\n",
    "                    \"batchsize\": {\"values\": [64]},\n",
    "                    \"hidden_layer_sizes\": {\"values\": [(32,), (64,)]},\n",
    "                    \"lr\": {\"values\": [1e-3, 1e-4]}\n",
    "                    },\n",
    "}\n",
    "\n",
    "if write_sweep:\n",
    "    sweep_id = wandb.sweep(sweep_configuration, project=\"del_mc1_sweeptest\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_1hl)\n",
    "play_sound(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN** Parameter Suche mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**2. Nutzen der Regularisierung**  \n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:  \n",
    "a. L1/L2 Weight Penalty  \n",
    "b. Dropout\n",
    "\n",
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung verschiedener Regularisierungsstärken. Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist (Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden). Inwiefern wird dieses Ziel im gegebenen Fall erreicht?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**3. Nutzen von Batchnorm BN (ohne REG, mit SGD)**  \n",
    "Evaluiere, ob Batchnorm etwas bringt. Beschreibe kurz, was die Idee von BN ist, wozu es helfen soll.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**4. Nutzen von Adam (ohne BN, ohne / mit REG)**   \n",
    "Evaluiere, ob Du mit Adam bessere Resultate erzielen kannst.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tipp:</b> Blaue Boxen .. \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Erfolg:</b> Grüne Boxen (alert-success)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warnung:</b> Rote Boxen (alert-danger)\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
