{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH1\n",
    "HS23, Manuel Schwarz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreibung Notebook\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Aufgabenstellung:</b> Eine Blaue Box beschreibt die Aufgab aus der Aufgabenstellung 'SGDS_DEL_MC1.pdf' \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Antworte:</b> Eine Grüne Box beschreibt die Bearbeitung / Reflektion der Aufgabenstellung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 1: Auswahl Task / Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dir Gedanken, mit welchen Daten Du arbeiten möchtest und welcher Task gelernt werden soll.\n",
    "    \n",
    "2. Diskutiere die Idee mit dem Fachcoach.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten und Task\n",
    "Pytorch stellt einige Datensets zur Verfügung [datasets torch](https://pytorch.org/vision/main/datasets.html).\n",
    "Verschiedene Kategorien stehen zur Auswahl:\n",
    "- Image classification\n",
    "- Image detection or segmentation\n",
    "- Optical Flow\n",
    "- Stereo Matching\n",
    "- Image pairs\n",
    "- Image captioning\n",
    "- video classification\n",
    "- Base classes for custom datasets\n",
    "\n",
    "\n",
    "**Datenset**  \n",
    "Eine beliebtes Dateset ist CIFAR10. Es beinhaltet Bilder von 10 Klassen (Flugzeuge, Katzen, Vögel, etc.), die Bilder kommen mit einer Auflösung von 32x32x3 pixel (rgb). Viele Tutorials starten mit diesem Datenset, das lässt darauf schliessen, dass der Rechenaufwand für die Hardware in einem vernümpftigen Rahmen liegt. Daher wird CIFAR10 als Datenset für die Challenge gewählt.\n",
    "\n",
    "**Task**  \n",
    "Anhand von CIFAR10 soll ein Modell erstellt werden, welches die Klasse eines Bildes korrekt klassifiziert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 2: Daten Kennenlernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dich mit dem Datensatz vertraut, indem Du eine explorative Analyse der Features durchführst: z.B. Vergleich der Klassen pro Feature, Balanciertheit der Klassen. \n",
    "2. Führe ein geeignetes Preprocessing durch, z.B. Normalisierung der Daten.  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explorative Datenanalyse\n",
    "[Tutorial Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten von CIFAR10\n",
    "data_path = './data/'\n",
    "train_data = torchvision.datasets.CIFAR10(data_path, train=True, download=True)\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datengrösse der Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Trainingsdaten: {len(train_data)}\\n'\n",
    "      f'Anzahl Testdaten: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie sind die Bilder im Datensatz gespeichert?**  \n",
    "Die Bilder sind direkt auf dem Datenset via dem Index abrufbar. Ein Tupel mit dem Bild (RGB, 32x32 pixel) und dem Label (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_0, label_0 = train_data[0]\n",
    "img_0, label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deffinieren der Labels CIFAR10\n",
    "labels_cifar10_dict = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "labels_cifar10_list = list(labels_cifar10_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisierung der Bilder**  \n",
    "Die Bilder können mit matplotlib und imshow() direkt vom Dateset über den index dargestellt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6,6))\n",
    "cols, rows = 5, 5\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_cifar10_list[label], fontsize=8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Untersuchen der Verteilungen der Klassen**  \n",
    "Folgend werden die Verteilungen der Klassen der Cifar10 Datensets auf den Trainings- und Testdaten geprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_data.targets\n",
    "test_target = test_data.targets\n",
    "print(f'Labels in Trainingsdaten: {len(train_target)}')\n",
    "print(f'Labels in Testdaten: {len(test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "ax[0].hist(train_target)\n",
    "ax[0].set_xticks(np.arange(10))\n",
    "ax[0].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[0].set_title('Trainingset', fontsize=8)\n",
    "\n",
    "ax[1].hist(test_target)\n",
    "ax[1].set_xticks(np.arange(10))\n",
    "ax[1].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[1].set_title('Testset', fontsize=8)\n",
    "\n",
    "plt.suptitle('Verteilung der Klassen von Cifar-10', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Das Trainingsset umfasst total 50'000 Bilder, davon sind jeweils 5000 Bilder jeder Klasse enthalten (Balanced Datenset). Somit kann zum Beispiel eine Metrik wie'Accuracy' verwendet werden, um die Klassifikation der Modelle zu beurteilen und zu vergleichen. Die 10'000 Bilder in den Testdaten sind ebenfalls gleich verteilt.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittelwerte und Standardabweichungen des Trainingdatensets (RGB):\n",
    "Benötigt um die Daten zu normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Datenset mit Tensoren für Berechnunen\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = torchvision.datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(data_path, train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std_dataset(train_data, print_info=True):\n",
    "    '''\n",
    "    Berechnet den Mittelwert und Standardabweichung für alle Bilder\n",
    "    '''\n",
    "    num_images = len(train_data)\n",
    "\n",
    "    # erstelle Array mit Dimension der input Bilder\n",
    "    pixel_values = np.zeros((num_images, 3, 32, 32), dtype=np.float32)\n",
    "\n",
    "    # füllen des Arrays mit Pixel Werten\n",
    "    for i in range(num_images):\n",
    "        image, _ = train_data[i]\n",
    "        pixel_values[i] = image.numpy()\n",
    "\n",
    "    mean = np.mean(pixel_values, axis=(0, 2, 3))\n",
    "    std = np.std(pixel_values, axis=(0, 2, 3))\n",
    "\n",
    "    if print_info:\n",
    "        print(\"RGB-Mittelwerte:\", mean)\n",
    "        print(\"RGB-Standardabweichungen:\", std)\n",
    "\n",
    "calc_mean_std_dataset(train_data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing der Daten  \n",
    "Folgend werden die Daten in einem Preprocessing Schritt für die Modelle vorbereitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cv\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "results = {}\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False)\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_validation_loaders(dataset, kfold, train_batch_size, test_batch_size):\n",
    "    cv_train_loader = []\n",
    "    cv_test_loader = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "        # train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_subsampler = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subsampler = torch.utils.data.Subset(dataset, test_idx)\n",
    "        \n",
    "        # train_loader = torch.utils.data.DataLoader(dataset, \n",
    "        #                 batch_size=train_batch_size, \n",
    "        #                 sampler=train_subsampler,\n",
    "        #                 shuffle=True)\n",
    "        # val_loader = torch.utils.data.DataLoader(dataset,\n",
    "        #                 batch_size=test_batch_size,\n",
    "        #                 sampler=val_subsampler,\n",
    "        #                 shuffle=False)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_subsampler, \n",
    "                        batch_size=train_batch_size, \n",
    "                        shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subsampler,\n",
    "                        batch_size=test_batch_size,\n",
    "                        shuffle=False)\n",
    "        \n",
    "        cv_train_loader.append(train_loader)\n",
    "        cv_test_loader.append(val_loader)\n",
    "\n",
    "    return cv_train_loader, cv_test_loader\n",
    "\n",
    "# function preprocessing\n",
    "def preprocessing_cifar10(path='./data/', train_batch_size=32, test_batch_size=32, normalize='zero_one',\n",
    "                          norm_mean=(0.4914009  , 0.548215896, 0.4465308), \n",
    "                          norm_std=(0.24703279 , 0.24348423 , 0.26158753), \n",
    "                          download=True, print_info=False, cv=False, k_folds=5, \n",
    "                          set_seed=42):\n",
    "    '''\n",
    "    '''\n",
    "    if print_info: print(f'------------'), print(f'Preprocessing start')\n",
    "\n",
    "    if normalize == 'zero_one':\n",
    "        # transform tensor to normalized range [0, 1], 0=schwarz\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "    elif normalize == 'minusone_one':\n",
    "        # transform tensor to normalized range [-1, 1], 0=schwarz\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])\n",
    "    elif normalize == 'None':\n",
    "        transform = transforms.Compose([])\n",
    "    else:\n",
    "        raise ValueError('Nomalize must be either \"zero_one\", \"minusone_one\" or \"None\"') \n",
    "    \n",
    "    if cv:\n",
    "        dataset_train = torchvision.datasets.CIFAR10(root=path, train=True, download=download, transform=transform)        \n",
    "        dataset_test = torchvision.datasets.CIFAR10(root=path, train=False, download=download, transform=transform)\n",
    "        dataset = dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "        if print_info: \n",
    "            print(f'Data transformed: {normalize}')\n",
    "            calc_mean_std_dataset(dataset, print_info)\n",
    "        \n",
    "        kfold = KFold(n_splits=k_folds, shuffle=True, random_state=set_seed)        \n",
    "\n",
    "        cv_train_loader, cv_test_loader = get_cross_validation_loaders(\n",
    "            dataset, kfold, train_batch_size, test_batch_size)\n",
    "        \n",
    "        if print_info: print(f'Dataloader created with {train_batch_size=}, {test_batch_size=}')\n",
    "        if print_info: print(f'Preprocessing done'), print('------------')\n",
    "\n",
    "        return cv_train_loader, cv_test_loader\n",
    "\n",
    "    else:\n",
    "        # CIFAR10: 50000 32x32 color images in 10 classes, with 5000 images per class\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                                download=download, transform=transform)\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                            download=download, transform=transform)\n",
    "        if print_info: \n",
    "            print(f'Data transformed: {normalize}')\n",
    "            calc_mean_std_dataset(train_dataset, print_info)\n",
    "\n",
    "        # dataloader\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "        if print_info: print(f'Dataloader created with {train_batch_size=}, {test_batch_size=}')\n",
    "        if print_info: print(f'Preprocessing done'), print('------------')   \n",
    "\n",
    "        return train_dataset, test_dataset, train_loader, test_loader\n",
    "    \n",
    "\n",
    "# def get_cross_validation_loaders(dataset, kfold, train_batch_size, test_batch_size):\n",
    "#     cv_train_loader = []\n",
    "#     cv_test_loader = []\n",
    "\n",
    "#     for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "#         train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "#         val_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        \n",
    "#         train_loader = torch.utils.data.DataLoader(dataset, \n",
    "#                         batch_size=train_batch_size, \n",
    "#                         sampler=train_subsampler)\n",
    "#         val_loader = torch.utils.data.DataLoader(dataset,\n",
    "#                         batch_size=test_batch_size,\n",
    "#                         sampler=val_subsampler)\n",
    "        \n",
    "#         cv_train_loader.append(train_loader)\n",
    "#         cv_test_loader.append(val_loader)\n",
    "\n",
    "#     return cv_train_loader, cv_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "data_path = './data/'\n",
    "batch_size = 32\n",
    "transform = 'zero_one' # minusone_one\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(path=data_path, \n",
    "                                                                               train_batch_size=batch_size,\n",
    "                                                                               test_batch_size=batch_size,\n",
    "                                                                               normalize=transform,  \n",
    "                                                                               download=False,  \n",
    "                                                                               print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "data_path = './data/'\n",
    "batch_size = 32\n",
    "transform = 'zero_one' # minusone_one\n",
    "\n",
    "cv_train_loader, cv_test_loader = preprocessing_cifar10(train_batch_size=batch_size,\n",
    "                                                            test_batch_size=batch_size,\n",
    "                                                            normalize=transform,\n",
    "                                                            download=False,  \n",
    "                                                            print_info=False, cv=True)\n",
    "\n",
    "print(f'K Fold CV: {len(cv_train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Aus den Berechnungen des Mittelwert und Standardabweichung lässt sich schliessen dass die Daten von Cifar-10 bereits normalisiert wurden auf Werte zwischen [0,1]. Mit der Transformation im Preprocessing `transforms.Normalize(mean, std)` können die Werte in den Bereich [-1, 1] gesetzt werden. Welcher Wertebereich die bessere Wahl ist, ist Situationsbedingt (z.B. verwendete Aktivierungsfunktion). Eine Zentrierung um Null kann für Netze Vorteile haben, hingegen sind Werte zwischen [0,1] besser zu interpretieren (0=schwarz, 1=RGB 255). Vorerst soll mit dem Wertebereich [0,1] gearbeitet werden.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 3: Aufbau Modellierung  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Lege fest, wie (mit welchen Metriken) Du die Modelle evaluieren möchtest. Berücksichtige auch den Fehler in der Schätzung dieser Metriken.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Klassengrössen sind ausbalanciert, daher ist für die Metrik **Accuracy** für die Modell Beurteilung geeignet und soll hier für die Modellbewertung zum Einsatz kommen:\n",
    "\n",
    "$$ Accuracy = \\frac{Anzahl\\ korrekte\\ Klassifizierungen}{Total\\ Klassifizierungen}  $$\n",
    "\n",
    "Für eine Klassifizierung von mehr als zwei Klassen eignet sich **CrossEntropyLoss**. Mehr dazu [hier](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "$$ \\text{CrossEntropyLoss} = -\\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right] $$\n",
    "\n",
    "\n",
    "Die Accuracy berechnet eine Schätzung. Bei der Initialisierung der Modellgewichte werden Zufallswerte verwendet. Auch die Wahl der Bilder innerhalb der Batchsize wird durch `shuffle=True` zufällig getroffen *(siehe `Preprocessing`)*. Somit varriert die Accuracy nach jedem Modeltraining ein wenig. Cross-Validation würde sich hier anbieten, um auf k-folds unterschiedliche Modelle zu erstellen. Auch ein Modell mehrmals ausführen wäre denkbar. Mit dem berechneten Mittelwert $\\mu$ und der Standardabweichung $\\sigma$ kann ein Fehlerabschätzung gemacht werde.  \n",
    "$$ Fehler_{range} = [\\mu - \\sigma; \\mu + \\sigma]$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "2. Implementiere Basisfunktionalität, um Modelle zu trainieren und gegeneinander zu evaluieren. Wie sollen die Gewichte initialisiert werden?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Methode wie die Intitialisierung der Gewichte stattfindet hat Einfluss wie schnell das Modell konvergiert und hilft die Probleme wie `vanishing` oder `exploding` Gradienten abzuschwächen. Kleine zufällige Werte führen zu einem effizienteren Training. Grosse Werte führen zu Problemenn bei dem das Modell nicht oder nur sehr langsam konvergiert. Je nach Problemstellung und Modelarchitekture können unterschiedliche Initialisierungsmethoden verwendet werden\n",
    "\n",
    "Mit der Verwendung von `nn.init` (Pytorch) stehen zum Beispiel folgende Optionen zur Verfügung:\n",
    "1. **Uniform initialization** (help prevents: vanishing gradient, can suffer: exploding gradient)\n",
    "1. **Xavier initialization** (help prevent: vanishing gradient)\n",
    "1. **Kaiming initialization** (help prevent: vanishing gradient, account activation function)\n",
    "1. **Normal initialization** (help prevent: exploding gradient)\n",
    "1. **Zeros initialization** (can suffer: slow converge, vanishing gradient)\n",
    "1. **One’s initialization** (can suffer: slow converge, vanishing gradient)\n",
    "\n",
    "Die genaue Beschreibung der Intitialisierungen können [hier](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/) gefunden werden. Eine eigene Custom-Option stellt Pytorch auch zur Verfügung.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition Helper Functions  \n",
    "Um die Erkentnisse dieses Notebook reproduzierbar zu machen, ist es wichtig einen Seed zu definieren. Folgend eine Seed Funktion mit einem üblichen Standart [Quelle](https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()\n",
    "\n",
    "def plot_loss_epoch(num_epochs, loss, figsize=(8, 4)):\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "    plt.plot(np.arange(num_epochs), loss)\n",
    "    plt.xticks(np.arange(num_epochs))\n",
    "    plt.title('Loss Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def calc_true_predictions(output_model, true_labels):\n",
    "    _, predicted = torch.max(output_model.data, 1)\n",
    "    return (predicted == true_labels).sum().item()\n",
    "\n",
    "def measure_model_time(start_time, calc='min'):\n",
    "    model_time = np.round(((time.time() - start_time) / 60), 2)  # from seconds to minute\n",
    "    return model_time\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n",
    "\n",
    "def plot_init_weights(model, method, figsize=(6, 5)):\n",
    "    l1_weights = model.state_dict()['linear1.weight'].numpy()\n",
    "    l2_weights = model.state_dict()['linear2.weight'].numpy()\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize)\n",
    "\n",
    "    ax[0].hist(l1_weights.flatten(), bins=50)\n",
    "    ax[1].hist(l2_weights.flatten(), bins=50)\n",
    "\n",
    "    ax[0].set_title(\"Layer 1\", fontsize=8)\n",
    "    ax[0].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[0].set_ylabel(\"Anzahl\", fontsize=8)\n",
    "    ax[0].tick_params(axis='y', labelsize=6)\n",
    "    ax[0].tick_params(axis='x', labelsize=6)\n",
    "    ax[1].set_title(\"Layer 2\", fontsize=8)\n",
    "    ax[1].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[1].tick_params(axis='y', labelsize=6)\n",
    "    ax[1].tick_params(axis='x', labelsize=6)\n",
    "    plt.suptitle(f'Verteilung Initialisierungs Methode: {method}', fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "def test_eval_plot():\n",
    "    num_epochs = 10\n",
    "    n_loss_epochs = (0.05 * np.sqrt(np.arange(10))) * -1\n",
    "    n_correct_train = 0.1 * n_loss_epochs**4\n",
    "    n_correct_test = 0.05 * n_loss_epochs**4\n",
    "    # create dataloader\n",
    "    train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(batch_size=4,\n",
    "                                                                                norm_mean=(0.5, 0.5, 0.5), \n",
    "                                                                                norm_std=(0.5, 0.5, 0.5),\n",
    "                                                                                download=False,  \n",
    "                                                                                print_info=False)\n",
    "    eval_model(num_epochs, n_loss_epochs, n_correct_train, n_correct_test,\n",
    "                train_loader, test_loader)\n",
    "# test_eval_plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(num_epochs, n_loss_epochs, n_val_loss_epochs, \n",
    "                n_acc_train_epochs, n_acc_test_epochs, train_loader, test_loader, \n",
    "                n_loss_batches=None, group_name='group_name', tag_name='tag_name', figsize=(8,5),\n",
    "                print_info=True, save_image=False):\n",
    "    \n",
    "    epoch = np.arange(num_epochs)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(epoch, n_loss_epochs, color= 'grey', linestyle='--', label='Train Loss')\n",
    "    ax1.plot(epoch, n_val_loss_epochs, color= 'grey', label='Test Loss')\n",
    "    ax2.plot(epoch, n_acc_train_epochs, color='steelblue', label='Accuracy Train')\n",
    "    ax2.plot(epoch, n_acc_test_epochs, color='coral', label='Accuracy Test')\n",
    "    \n",
    "    ax1.set_xticks(epoch)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss') \n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.tick_params(axis='x', labelsize=8)\n",
    "    ax1.tick_params(axis='y', labelsize=8)\n",
    "    ax2.tick_params(axis='y', labelsize=8)\n",
    "    ax1.legend(loc='upper center', bbox_to_anchor=(0.25, -0.12), ncol=3)    \n",
    "    ax2.legend(loc='upper center', bbox_to_anchor=(0.72, -0.12), ncol=3)\n",
    "    \n",
    "    plt.suptitle(f'Training: {group_name} ')\n",
    "    cur_date = datetime.now().strftime(\"%d.%m.%Y_%H%M\")\n",
    "    plt.title(f'{cur_date} - Tags: {tag_name}', size=6)\n",
    "    plt.grid()\n",
    "    if save_image: \n",
    "            plt.savefig(f'./plots/eval_model_plots/{cur_date}_{group_name}_{tag_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    if print_info:\n",
    "        print(f'Accuracy Train {n_acc_train_epochs[-1]:2f}')\n",
    "        print(f'Accuracy Test {n_acc_test_epochs[-1]:4f}')\n",
    "        print(f'Loss Train {n_loss_epochs[-1]:2f}')\n",
    "        print(f'Loss Test {n_val_loss_epochs[-1]:2f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Hyperparameters example\n",
    "config = {\n",
    "    \"name\": \"MLP\", \n",
    "    \"epochs\": 20,   \n",
    "    \"train_batch_size\": 32, \n",
    "    \"test_batch_size\": 32,\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"lr\": 1e-3, \n",
    "    \"optimizer\": 'SGD',\n",
    "    \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "    \"L1_lambda\":0,\n",
    "    \"L2_weight_decay\":0,\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"image_size\": 32,\n",
    "    \"cross_validation\": False,\n",
    "    \"is_test_batch\": True,\n",
    "    \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "    \"num_workers\": 0,\n",
    "    \"normalize\":\"zero_one\",\n",
    "    \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "    \"hidden_layer_sizes\": [128],\n",
    "    \"filter_sizes\": [32, 64],\n",
    "    \"dense_layers\": [512],\n",
    "    \"kernel_sizes\": 3,\n",
    "    \"padding\": 1,\n",
    "    \"stride\": 1,\n",
    "    \"pool_sizes\": [],\n",
    "    \"dropout\": 0,\n",
    "    \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "    \"norm_std\": (0.5, 0.5, 0.5),\n",
    "    \"num_classes\": 10,\n",
    "    \"save_eval_image\": True,\n",
    "    \"set_seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "# Train model function\n",
    "def train_model(\n",
    "        model, train_loader, test_loader, config, tags=['tag_name'], group='group_name',\n",
    "        print_info=False, plot_eval=True, sound=False, write_wandb=True, fold=0):      \n",
    "    \n",
    "    set_seed(config['set_seed'])\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "    if print_info: print(f'------------'), print(f'Starten des Trainings auf Device {device}')\n",
    "\n",
    "    # Model to device\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # define Loss and Optimizer\n",
    "    if config['loss_func'] == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if print_info: print(f\"Loss Funktion: {config['loss_func']}\")\n",
    "\n",
    "    if config['optimizer'] == 'SGD':        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}\")\n",
    "    if config['optimizer'] == 'SGD' and config['Regularisierung'] == 'L2':        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], weight_decay=config['L2_weight_decay'] )\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}, w_decay:{config['L2_weight_decay']}\")\n",
    "        \n",
    "    if config['optimizer']  == 'Adam':        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}\")\n",
    "    if config['optimizer']  == 'Adam' and config['Regularisierung'] == 'L2':        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['L2_weight_decay'])\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}\")\n",
    "\n",
    "    # Initialize wandb\n",
    "    if write_wandb: \n",
    "        cv_model_name = f\"{config['name']}-CV-{fold+1}/{config['n_folds']}-{config['epochs']}-epochs-{config['optimizer']}-{config['start_time']}\"\n",
    "        model_name = f\"{config['name']}-{config['epochs']}-epochs-{config['optimizer']}-{config['start_time']}\"\n",
    "        wandb.init(\n",
    "            project=\"del-mc1\",\n",
    "            entity='manuel-schwarz',\n",
    "            group=group,\n",
    "            name=cv_model_name if config['cross_validation'] else model_name,\n",
    "            tags=tags + (['is_test_batch'] if config['is_test_batch'] else []),\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # --------------------- train loop --------------------------\n",
    "    if write_wandb:\n",
    "        wandb.watch(model)\n",
    "\n",
    "    n_loss_epochs = []\n",
    "    n_val_loss_epochs = []\n",
    "    n_loss_all_batches = []\n",
    "    n_val_loss_all_batches = []\n",
    "    n_correct_train_epochs = []\n",
    "    n_correct_test_epochs = []\n",
    "    n_acc_train_epochs = []\n",
    "    n_acc_test_epochs = []\n",
    "\n",
    "    best_train_loss = 0\n",
    "    best_val_loss = 0\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    loop = range(config[\"epochs\"])\n",
    "    epoch_loop = tqdm(loop, desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "    for epoch in epoch_loop:\n",
    "        true_train = 0\n",
    "        n_loss_batch = []\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            if config['is_test_batch'] and i > 1:\n",
    "                break\n",
    "            # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "            # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # L1-Regularisierung\n",
    "            if config[\"Regularisierung\"] == 'L1':\n",
    "                l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + config[\"L1_lambda\"] * l1_loss\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_loss_batch.append(loss.item())\n",
    "            true_train += calc_true_predictions(outputs, labels)\n",
    "            # acc_train = true_train / len(train_loader.dataset)\n",
    "            n_loss_all_batches.append(loss.item())\n",
    "\n",
    "            inner_progress = f\"{i+1}/{len(train_loader)}\"\n",
    "            epoch_loop.set_description(f\"Epochs (Batch: {inner_progress})\")\n",
    "            epoch_loop.refresh()\n",
    "\n",
    "        # save results\n",
    "        acc_train = true_train / len(train_loader.dataset)\n",
    "        epoch_loss = np.mean(n_loss_batch)\n",
    "\n",
    "        n_correct_train_epochs.append(true_train)        \n",
    "        n_acc_train_epochs.append(acc_train)  \n",
    "        n_loss_epochs.append(epoch_loss)\n",
    "\n",
    "        # ---------- eval\n",
    "        model.eval()\n",
    "        true_test_val = 0 \n",
    "        n_val_loss_batch = []\n",
    "        # n_val_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                if config['is_test_batch'] and i > 1:\n",
    "                    break\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                n_val_loss_batch.append(loss.item())\n",
    "                true_test_val += calc_true_predictions(outputs, labels)\n",
    "                n_val_loss_all_batches.append(loss.item())              \n",
    "                # _, predicted = torch.max(outputs, 1)  # prediction speichern todo\n",
    "\n",
    "        # save results\n",
    "        acc_test = true_test_val / len(test_loader.dataset)\n",
    "        epoch_val_loss = np.mean(n_val_loss_batch)\n",
    "\n",
    "        n_correct_test_epochs.append(true_test_val)\n",
    "        n_acc_test_epochs.append(acc_test) \n",
    "        n_val_loss_epochs.append(epoch_val_loss)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # best scores\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "\n",
    "        if acc_train > best_train_acc:\n",
    "            best_train_acc = acc_train\n",
    "\n",
    "        if acc_test > best_test_acc:\n",
    "            best_test_acc = acc_test\n",
    "            best_epoch = epoch\n",
    "\n",
    "        # log metrics to wandb\n",
    "        if write_wandb:\n",
    "            wandb.log({\n",
    "                \"loss epoch\": epoch_loss, \n",
    "                \"validation loss epoch\": epoch_val_loss, \n",
    "                \"acc_train\": acc_train, \n",
    "                \"acc_test\": acc_test,\n",
    "                })\n",
    "    if write_wandb:\n",
    "            wandb.log({\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_train_loss\": best_train_loss,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"best_acc_train\": best_train_acc,\n",
    "                \"best_acc_test\": best_test_acc,\n",
    "                })\n",
    "\n",
    "    acc_train = n_correct_train_epochs[-1] / len(train_loader.dataset)\n",
    "    acc_test = n_correct_test_epochs[-1] / len(test_loader.dataset)\n",
    "\n",
    "    if plot_eval:\n",
    "        eval_model(config['epochs'], n_loss_epochs, n_val_loss_epochs, \n",
    "                    n_acc_train_epochs, n_acc_test_epochs, train_loader, test_loader,\n",
    "                    group_name=group, tag_name=tags, print_info=print_info, \n",
    "                    save_image=config['save_eval_image']\n",
    "                    )\n",
    "        \n",
    "    print(\n",
    "            f\"Epoch {epoch + 1}/{config['epochs']}, \\\n",
    "            Loss: {round(epoch_loss, 5)}, Validation Loss: {round(epoch_val_loss, 5)} \\\n",
    "            Acc: {round(acc_train, 5)}, Validation Acc: {round(acc_test, 5)}\"\n",
    "            )\n",
    "    \n",
    "    if sound: play_sound(1)\n",
    "\n",
    "    if write_wandb: \n",
    "        wandb.finish()\n",
    "        time.sleep(5)  # wait for wandb.finish\n",
    "\n",
    "    print('Finished Training')\n",
    "    return epoch_loss, epoch_val_loss, acc_train, acc_test\n",
    "\n",
    "\n",
    "def model_trainer(model, config=config, group='group_name', tags=['tag1'],\n",
    "                print_info=False, plot_eval=True, sound=True, \n",
    "                write_wandb=True, download=False):\n",
    "    \n",
    "    if config['cross_validation']:\n",
    "        cv_train_loader, cv_test_loader = preprocessing_cifar10(\n",
    "            train_batch_size=config[\"train_batch_size\"],\n",
    "            test_batch_size=config[\"test_batch_size\"],\n",
    "            normalize=config['normalize'],\n",
    "            download=download,  \n",
    "            print_info=print_info, cv=True,\n",
    "            set_seed=config['set_seed']\n",
    "            )\n",
    "        \n",
    "        cv_train_acc = []\n",
    "        cv_test_acc = []\n",
    "\n",
    "        for fold, (train_loader, test_loader) in enumerate(zip(cv_train_loader, cv_test_loader)):\n",
    "            _, _, train_acc, test_acc = train_model(model, train_loader, test_loader, config=config,\n",
    "                    group=group,\n",
    "                    tags=tags,\n",
    "                    print_info=print_info, plot_eval=plot_eval, sound=sound, \n",
    "                    write_wandb=write_wandb, fold=fold\n",
    "                    )\n",
    "            \n",
    "            cv_train_acc.append(train_acc)\n",
    "            cv_test_acc.append(test_acc)\n",
    "\n",
    "        cv_train_acc_mean = np.mean(cv_train_acc)\n",
    "        cv_test_acc_mean = np.mean(cv_test_acc)\n",
    "\n",
    "        return cv_train_acc_mean, cv_test_acc_mean, cv_train_acc, cv_test_acc\n",
    "\n",
    "    else:\n",
    "        # create dataloader\n",
    "        _, _, train_loader, test_loader = preprocessing_cifar10(\n",
    "            train_batch_size=config[\"train_batch_size\"],\n",
    "            test_batch_size=config[\"test_batch_size\"],\n",
    "            normalize=config['normalize'],\n",
    "            download=download,  \n",
    "            print_info=print_info,\n",
    "            set_seed=config['set_seed']\n",
    "            )\n",
    "\n",
    "            # train model\n",
    "        _, _, train_acc, test_acc = train_model(model, train_loader, test_loader, config=config,\n",
    "                    group=group,\n",
    "                    tags=tags,\n",
    "                    print_info=print_info, plot_eval=plot_eval, sound=sound, \n",
    "                    write_wandb=write_wandb\n",
    "                    )\n",
    "        return train_acc, test_acc, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfacher Modell Test  \n",
    "Folgend soll ein einfaches Modell mit einem hidden Layer erstellt werden um die Basisfunktionen von `model_trainer()` zu testen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model Class\n",
    "class MLPNet_hl1(nn.Module):\n",
    "    def __init__(self, hidden_l_1:list, act_fn=F.relu, dropout=0, init_methode:str='kaiming_norm'):\n",
    "        super(MLPNet_hl1, self).__init__()\n",
    "        self.linear1 = nn.Linear(3*32*32, hidden_l_1[0])  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear2 = nn.Linear(hidden_l_1[0], 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "\n",
    "        for linear_weight in [self.linear1.weight, self.linear2.weight]:          \n",
    "            if init_methode == 'uniform':\n",
    "                torch.nn.init.uniform_(linear_weight)\n",
    "            if init_methode == 'xavier':\n",
    "                torch.nn.init.xavier_uniform_(linear_weight)\n",
    "            if init_methode == 'normal':\n",
    "                torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "            if init_methode == 'kaiming_norm':\n",
    "                torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (3072, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) # x.shape = (128, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilisierung Gewichte: \n",
    "Da unterschiedliche Problemstellung verschiedene Initialisierungen erfordern, sollen mehrere Methoden von Pytorch ausprobiert werden um die Gewichte zu initialisiern: Folgende Grafik zeigt die Gewichte des ersten (initial) und zweiten Layers:\n",
    "1. `uniform`: torch.nn.init.uniform_(linear_layer.weight)\n",
    "1. `xavier`: torch.nn.init.xavier_uniform_(linear_layer.weight)\n",
    "1. `normal`: torch.nn.init.normal_(linear_layer.weight, mean=0, std=1)\n",
    "1. `kaiming_norm`: torch.nn.init.kaiming_normal_(linear_layer.weight, nonlinearity=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_lst = ['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "# create model\n",
    "for method in method_lst:\n",
    "    model = MLPNet_hl1(hidden_l_1=[64], init_methode=method)\n",
    "    plot_init_weights(model, method, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilungen der Gewichte mit den Methoden `uniform` und  `xavier` sind sehr ähnlich. Mit Xavier ist die X-Skala unterschiedlich, da die Gewichte so skalliert werden dass die Varianz des Output der Varianz des Inputs entspricht.  \n",
    "Bei `normal` und `kaiming_norm` besteht das gleiche Prinzip, die skallierung der Gewichte, zudem wird die Aktivierungsfunktion berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "    # Hyperparameters  \n",
    "    config = {\n",
    "        \"name\": \"MLP-test1\", \n",
    "        \"epochs\": 1,   \n",
    "        \"train_batch_size\": 32, \n",
    "        \"test_batch_size\": 32,\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"lr\": 1e-3, \n",
    "        \"optimizer\": 'SGD',\n",
    "        \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "        \"L1_lambda\":0,\n",
    "        \"L2_weight_decay\":0,\n",
    "        \"loss_func\": 'CrossEntropyLoss',\n",
    "        \"loss_func\": 'CrossEntropyLoss',\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"image_size\": 32,\n",
    "        \"cross_validation\": False,\n",
    "        \"n_folds\": 5,\n",
    "        \"is_test_batch\": True,\n",
    "        \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "        \"num_workers\": 0,\n",
    "        \"normalize\":\"zero_one\",\n",
    "        \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "        \"hidden_layer_sizes\": [128],\n",
    "        \"filter_sizes\": [32, 64],\n",
    "        \"dense_layers\": [512],\n",
    "        \"kernel_sizes\": 3,\n",
    "        \"padding\": 1,\n",
    "        \"stride\": 1,\n",
    "        \"pool_sizes\": [],\n",
    "        \"dropout\": 0,\n",
    "        \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "        \"norm_std\": (0.5, 0.5, 0.5),\n",
    "        \"num_classes\": 10,\n",
    "        \"save_eval_image\": True,\n",
    "        \"set_seed\": 42\n",
    "    }\n",
    "\n",
    "    # create model\n",
    "    model2 = MLPNet_hl1(\n",
    "        hidden_l_1=config[\"hidden_layer_sizes\"], \n",
    "        act_fn=F.relu,\n",
    "        dropout=0,\n",
    "        init_methode=config[\"init_w_method\"],\n",
    "        )\n",
    "\n",
    "    cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(model2, config=config,\n",
    "                    group='MLP_CV_test2',\n",
    "                    tags=['MLP', 'cv-test'],\n",
    "                    print_info=False, plot_eval=False, sound=True, \n",
    "                    write_wandb=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 4: Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Bei der Evaluation ist darauf zu achten, dass das Vorgehen stets möglichst reflektiert erfolgt und versucht wird, die Ergebnisse zu interpretieren. Am Schluss soll auch ein Fazit gezogen werden, darüber welche Variante am besten funktioniert.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**1. Training mit SGD, ohne REG, ohne BN:**  \n",
    "Untersuche verschiedene Modelle unterschiedlicher Komplexität, welche geeignet sein könnten, um das Klassifikationsproblem zu lösen. Verwende Stochastic Gradient Descent - ohne Beschleunigung, ohne Regularisierung (REG) und ohne Batchnorm (BN).  \n",
    "\n",
    "a. Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein sorgfältiges Hyper-Parameter-Tuning durch (Lernrate, Batch-Grösse). Achte stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl Epochen, Lernrate und Batch-Grösse.\n",
    "\n",
    "b. Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning) verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding.\n",
    "\n",
    "c. Fasse die Ergebnisse zusammen in einem geeigneten Plot, bilde eine Synthese und folgere, welche Modell-Komplexität Dir am sinnvollsten erscheint.  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition von MLP Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP_dynamic_layer(nn.Module):\n",
    "#     def __init__(self, h_layer_sizes:list, input_size=3*32*32, act_fn=F.relu, dropout=0, \n",
    "#                 init_methode:str='kaiming_norm', output_size=10):\n",
    "#         super(MLP_dynamic_layer, self).__init__()\n",
    "#         self.init_methode = init_methode\n",
    "#         self.activation = act_fn\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Initial layer from input list\n",
    "#         self.layers = nn.ModuleList([nn.Linear(input_size, h_layer_sizes[0])])\n",
    "        \n",
    "#         # Add hidden layers based on layer_sizes\n",
    "#         for i in range(len(h_layer_sizes)-1):\n",
    "#             self.layers.append(nn.Linear(h_layer_sizes[i], h_layer_sizes[i+1]))\n",
    "        \n",
    "#         # Final layer to output\n",
    "#         self.output_layer = nn.Linear(h_layer_sizes[-1], output_size)\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 if self.init_method == 'uniform':\n",
    "#                     nn.init.uniform_(m.weight)\n",
    "#                 if self.init_method == 'xavier':\n",
    "#                     nn.init.xavier_uniform_(m.weight)\n",
    "#                 if self.init_method == 'normal':\n",
    "#                     torch.nn.init.normal_(m.weight, mean=0, std=1)\n",
    "#                 elif self.init_method == 'kaiming':\n",
    "#                     nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "#                 m.bias.data.fill_(0.01)  # better than zero -> set small constant\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = self.activation(layer(x))\n",
    "#             x = self.dropout(x)\n",
    "#         x = self.output_layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLP** Hyperparameter Suche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein gutes Modell müssen optimale Hyperparamter gefunden werden. Folgende werden MLP-Modelle mit unterschiedlichen Anzahl und Grösse von Layers erstellt. Zu jedem Modell soll weiter den Einfluss von Lernrate und Batchsize untersucht werden:\n",
    "\n",
    "**Ablauf pro Modell:**  \n",
    "1. Der Umfang sowie die Anzahl der hidden Layers werden gesetzt\n",
    "    1. Pro Modell werden verschiedene Lernraten untersucht\n",
    "    1. Pro Modell werden verschiedene Grössen von Layern untersucht\n",
    "    1. Pro Modell werden verschiedene batchgrössen untersucht\n",
    "\n",
    "Das Tracking von Loss und Metriken werden mit wandb für jedes Training aufgezeichnet. Ein Bildauschnitt des Trainings wurde diesem Notebook hinzugefügt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP mit einem hidden Layer**  \n",
    "In den folgenden Experimenten soll die Lernrate, die Grösse des hidden Layers und die Batchsize untersucht werden:\n",
    "\n",
    "|Experiment| Lernrate | Grösse hL1 | Batchsize |\n",
    "|---|----------|----------|----------|\n",
    "|1| 1e-1, 1e-2, 1e-3, 1e-4, 1e-5 | 128*  | 32*  |\n",
    "|2| 1e-2** | 64, 128, 256, 512, 1024 |  32* |\n",
    "|3| 1e-2** | 1024**  | 4, 16, 32, 64, 128  |\n",
    "\n",
    "Info:  \n",
    "*: gewählter Startwert  \n",
    "** : optimaler Wert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet_hl1(nn.Module):\n",
    "    def __init__(self, h_layer_sizes:list, input_size=3*32*32, act_fn=F.relu, dropout=0, \n",
    "                init_methode:str='kaiming_norm', output_size=10):\n",
    "        super(MLPNet_hl1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, h_layer_sizes[0])  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear2 = nn.Linear(h_layer_sizes[0], 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "\n",
    "        for linear_weight in [self.linear1.weight, self.linear2.weight]:          \n",
    "            if init_methode == 'uniform':\n",
    "                torch.nn.init.uniform_(linear_weight)\n",
    "            if init_methode == 'xavier':\n",
    "                torch.nn.init.xavier_uniform_(linear_weight)\n",
    "            if init_methode == 'normal':\n",
    "                torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "            if init_methode == 'kaiming_norm':\n",
    "                torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (3072, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) # x.shape = (128, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    #for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    #for hl_size in [64, 128, 256, 512, 1024]:\n",
    "    #for bt_size in [4, 16, 32, 64, 128]:\n",
    "    for seed in [12, 16, 42, 64, 71]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"MLP-hL1\", \n",
    "            \"epochs\": 15,   \n",
    "            \"train_batch_size\": 32, \n",
    "            \"test_batch_size\": 32,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\":0,\n",
    "            \"L2_weight_decay\":0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": 0,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": seed  # [12, 16, 42, 64, 71]\n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        mlp_h1 = MLPNet_hl1(\n",
    "            h_layer_sizes=config[\"hidden_layer_sizes\"], \n",
    "            act_fn=F.relu,\n",
    "            dropout=0,\n",
    "            init_methode=config[\"init_w_method\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(mlp_h1, config=config,\n",
    "                        group=f'MLP_hL1_seed_val',  # _5f_cv, _seed_val\n",
    "                        tags=['MLP', 'seed val'],  # 5fold cv, seed val\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unterschiedliche Lernrate**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl1_lr.PNG\" width=\"800\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Layergrösse**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl1_layer_size.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Batchgrösse**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl1_batch_size.PNG\" width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Einfluss der Learningrate:**  \n",
    "Die Lernrate bestimmt, wie stark die Modellgewichte in jedem Trainingsschritt angepasst werden. $$ W_{neu}= W_{alt} - Lernrate \\cdot Gradient $$\n",
    "Bei einer zu hohen Lernrate kann es passieren, dass das optimale globale Minimum übersprungen wird. Das Training könnte auch divergieren statt konvergieren. Bei einer zu tiefen Lernrate kann das Training an einem lokalen Minimum hängen bleiben und das globale Minimum nicht erreichen. Auch findet der Trainingsprozess weniger schnell statt. Das ist auch der Grund der tieferen Accuracy bei den verschiedenen Lernraten, tiefere Lernraten müssten mit mehr Epochen trainiert werden. Bei $e^{-2}$ erreicht das Training die höchste Accuracy, wobei der Verlauf etwas weniger stabil aussieht. Die Lernraten $e^{-1}$ und $e^{-3}$ sind sehr ähnlich. Ein Optimum, in unserem Suchumfang, bei $e^{-2}$ gefunden \n",
    "\n",
    "\n",
    "**Einfluss Grösse des Hidden Layers:**  \n",
    "Ein Modell mit einem hidden Layer verbindet den Input Layer mit dem Output Layer. In unserem Fall $Input\\ Layer (Bild): 3*32*32 = 3072 -> hidden\\ Layer -> Output\\ Layer (Anzahl\\ Klassen) = 10$. Die Lernrate wurde auf $e^{-2}$ gesetzt. In der  Entwicklung von Trainings Loss und der Training Accuracy zeigt sich ein sehr ähnliches Verhalten. Die Test Daten zeigen, dass eine sehr tiefe Layergrösse weniger gut geeignet ist. Die Unterschiede ab Grösse 256 sind weniger markant. Eine Layergrösse von 1024 zeigte die beste Accuracy. Grössere Layer können besser komplexe Funktionen abbilden und dadurch mehr Muster aus den Daten erkennen. Es bedeutet aber auch mehr Gewichte und eine höherer Trainingszeit. Die Layergrösse hat auch Einfluss auf den Bias-Varianz Tradeoff. \n",
    "\n",
    "**Einfluss der Batchgrösse:**  \n",
    "Mit einer kleineren Batchgrösse werden die Gewichte des Modells häufiger angepasst. Das kann das Training schneller aber auch instabiler machen. Eine grössere Batchsize kann eine genauere Schätzung des Gradienten erstellen. Ähnlich wie bei einer tiefen Lernrate, könnten aber lokale Minimas weniger gut übersprungen werden. Die Wahl, spezifisch bei Bilddaten, wird auch stark durch den vorhandenen GPU-Speicher begrenzt. In unseren Versuchen sehen wir ähnliche Ergebnisse in der Test Accuracy wobei die Batchgrössen 16 und 32 die höchste Accuracy erreichten. Interessant war dass das Modell mit der Batchgrösse 4 für 15 Epochen 24 Minuten benötigte, gegenüber von Batchgrösse 16 = 8 Minuten, Batchgrösse 32 = 5 Minuten, Batchgrösse 64 = 4 Minuten und Batchgrösse 128 = 3 Minuten. Die häufige Anpassungen der Gewichte haben deutlichen Einfluss auf die Trainingszeit. Für das Modell soll mit der Batchgrösse 32 fortgefahren werden.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP_hl1 Modell Evaluation**:  \n",
    "Für die optimierten Parameter soll das Modell mit 5Fold-Cross-Validation und mit unterschiedlichen Seeds geprüft werden:\n",
    "\n",
    "<!-- <img src=\"../01_Dokumentation/wandb_images/MLP_hl1_5f_cv.PNG\" width=\"900\" height=\"400\">\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl1_seeds_val.PNG\" width=\"900\" height=\"400\"> -->\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl1_5f_cv_seeds.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "|Modell| Lernrate | Grösse hL1 | Batchsize | Train Acc | Test Acc | Test Schätzfehler ($\\sigma$) |\n",
    "|----------|----------|----------|----------   |---------- | -------- |-------- |\n",
    "|MLP-hl1-5f-cv (grün)   |1e-2      | 1024     | 32           | 0.6795       | 0.6139       | 0.086      |\n",
    "|MLP-hl1-seeds (gelb)   |1e-2      | 1024     | 32           | 0.5648      | 0.5071       | 0.0088      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP mit 2 hidden Layers**  \n",
    "Ein gängiger Ansatz ist die Grösse der Layer schrittweise zu verringern. Der Pyramiden-Ansatz verringert die Grösse gar exponentiell.  \n",
    "In den folgenden Experimenten soll die Lernrate, die Grössen der hidden Layers und die Batchsize untersucht werden:\n",
    "\n",
    "|Experiment| Lernrate | Grösse hL1 | Grösse hL2 |Batchsize |\n",
    "|----------|----------|----------|----------|----------|\n",
    "|1          | 1e-1, 1e-2, 1e-3, 1e-4, 1e-5 | 1024*                       | 256*  | 32*  |\n",
    "|2          | 1e-2**                        | 1024, 2048  |  256*        | 32*        |\n",
    "|3          | 1e-2**                        | 1024**                    | 64, 128, 256, 512  | 32* |\n",
    "|4          | 1e-2**                        | 1024**                    | 512* | 4, 16, 32, 64, 128 |\n",
    "\n",
    "Info:  \n",
    "*: gewählter Startwert  \n",
    "** : optimaler Wert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet_hl2(nn.Module):\n",
    "    def __init__(self, h_layer_sizes:list, input_size=3*32*32, act_fn=F.relu, dropout=0, \n",
    "                init_methode:str='kaiming_norm', output_size=10):\n",
    "        super(MLPNet_hl2, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, h_layer_sizes[0])  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear2 = nn.Linear(h_layer_sizes[0], h_layer_sizes[1])\n",
    "        self.linear3 = nn.Linear(h_layer_sizes[1], output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "\n",
    "        for linear_weight in [self.linear1.weight, self.linear2.weight]:          \n",
    "            if init_methode == 'uniform':\n",
    "                torch.nn.init.uniform_(linear_weight)\n",
    "            if init_methode == 'xavier':\n",
    "                torch.nn.init.xavier_uniform_(linear_weight)\n",
    "            if init_methode == 'normal':\n",
    "                torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "            if init_methode == 'kaiming_norm':\n",
    "                torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (3072, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.linear2(x)) # Durchlauf durch den neuen Hidden Layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    # for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    # for hl1_size in [1024, 2048]:\n",
    "    # for hl2_size in [64, 128, 256, 512]:\n",
    "    # for bt_size in [4, 16, 32, 64, 128]:\n",
    "    for seed in [12, 16, 42, 64, 71]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"MLP-hL2\", \n",
    "            \"epochs\": 15,   \n",
    "            \"train_batch_size\": 16, \n",
    "            \"test_batch_size\": 16,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\":0,\n",
    "            \"L2_weight_decay\":0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 512],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": 0,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": seed  # [12, 16, 42, 64, 71]\n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        mlp_h2 = MLPNet_hl2(\n",
    "            h_layer_sizes=config[\"hidden_layer_sizes\"], \n",
    "            act_fn=F.relu,\n",
    "            dropout=0,\n",
    "            init_methode=config[\"init_w_method\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(mlp_h2, config=config,\n",
    "                        group=f'MLP_hL2_seed_val', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['MLP', 'seed val'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unterschiedliche Lernrate**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_lr.PNG\" width=\"800\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Layergrössen Layer 1**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_layer1_size.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Layergrössen Layer 2**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_layer2_size.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Batchgrösse**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_batch_size.PNG\" width=\"900\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Einfluss der Learningrate:**  \n",
    " $e^{-2}$ gefunden \n",
    "\n",
    "\n",
    "**Einfluss Grösse des Hidden Layers:**  \n",
    "Ein Modell mit einem hidden Layer verbindet den Input Layer mit dem Output Layer. In unserem Fall $Input\\ Layer (Bild): 3*32*32 = 3072 -> hidden\\ Layer -> Output\\ Layer (Anzahl\\ Klassen) = 10$. Die Lernrate wurde auf $lr=e^{-2}$ gesetzt. \n",
    "\n",
    "**Einfluss der Batchsize:**  \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP_hl2 Modell Evaluation**:  \n",
    "Für die optimierten Parameter soll das Modell mit 5Fold-Cross-Validation und mit unterschiedlichen Seeds geprüft werden\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_5f_cv_seeds.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "|Modell| Lernrate | Grösse hL1 | Grösse hL1 |Batchsize | Train Acc | Test Acc | Test Schätzfehler ($\\sigma$) |\n",
    "|------|----------|----------   |---------- |---------- |---------- | -------- |--------                    |\n",
    "|MLP-hl2-5f-cv (--)   |1e-2      | 1024     | 512      | 16           | 0.8208       | 0.07026       | 0.1646      |\n",
    "|MLP-hl2-seeds (-)   |1e-2      | 1024     | 512      | 16           | 0.6124      | 0.5310       | 0.0070      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN** Hyperparameter Suche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende wird ein einfaches CNN Modell erstellt. Es soll den Einfluss von verschiedenen Lernraten, Batchsize, Anzahl Filtern, Kernel-Grössen, Strides und Paddings untersucht werden:\n",
    "\n",
    "**Ablauf:**  \n",
    "1. verschiedene Lernraten werden untersucht\n",
    "1. verschiedene Anzahl Filter werden untersucht\n",
    "1. verschiedene Batchgrössen werden untersucht\n",
    "1. verschiedene Kernelgrössen, Strides und Padding werden untersucht\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple CNN**  \n",
    "In den folgende Experimenten soll die Lernrate, die Grösse der Convolution Layers und die Batchsize untersucht werden:\n",
    "\n",
    "|Experiment| Lernrate | Grösse Filter1 |  Grösse Filter2 |Batchsize |Kernel-Grösse | Stride | Padding |\n",
    "|---|----------|----------|----------|----------|----------|----------|----------|\n",
    "|1| 1e-1, 1e-2, 1e-3, 1e-4, 1e-5 | 32*  | 64*  | 32*  | 3*  | 1*  | 1*  |\n",
    "|2| 1e-2** | 16, 32, 64 | 64* |  32* | 3*  | 1*  | 1*  |\n",
    "|3| 1e-2** | 32**  | 32, 64, 96  | 32*  | 3*  | 1*  | 1*  |\n",
    "|4| 1e-2** | 32**  | 64**  | 8, 16, 32, 64  | 3*  | 1*  | 1*  |\n",
    "|5| 1e-2** | 32**  | 64**  | 32**  | 3, 5, 7  | 1*  | 1*  |\n",
    "|6| 1e-2** | 32**  | 64**  | 32**  | 3** | 1, 2, 3  | 1*  |\n",
    "|7| 1e-2** | 32**  | 64**  | 32**  | 3** | 1**  | 0, 1, 2  |\n",
    "\n",
    "Info:  \n",
    "*: gewählter Startwert  \n",
    "** : optimaler Wert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionsberechnungen für Covolution Layers:\n",
    "$$Output\\ Grösse = \\frac{Input\\ Grösse + (2 \\cdot padding - Kernel\\ Grösse)}{stride} + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_dim(input_size, kernel_size, padding, stride, pooling=True):\n",
    "    conv_dim = int((input_size + 2 * padding - kernel_size) / stride + 1)\n",
    "\n",
    "    if pooling: conv_dim = conv_dim / 2\n",
    "    return conv_dim\n",
    "\n",
    "input_dim_bild = 32  # 32x32 pixel cifar-10\n",
    "kernel_size = 3  # 3x3\n",
    "padding = 1\n",
    "stride = 1\n",
    "conv2_out_channel = 64\n",
    "\n",
    "print(f'{input_dim_bild=}')  \n",
    "output_dim_conv1 = compute_conv_dim(input_dim_bild, kernel_size, padding, stride, pooling=True)\n",
    "print(f'{output_dim_conv1=}') \n",
    "\n",
    "output_dim_conv2 = compute_conv_dim(output_dim_conv1, kernel_size, padding, stride, pooling=True)\n",
    "print(f'{output_dim_conv2=}') \n",
    "\n",
    "input_fc1 = output_dim_conv2 * output_dim_conv2 * conv2_out_channel\n",
    "print(f'{input_fc1=} = {output_dim_conv2} x {output_dim_conv2} x {conv2_out_channel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell Class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_ch=3, act_fn=F.relu, filter_size:list=[32, 64], dlayers:list=[512], num_class=10,\n",
    "                kernel_size=3, padding=1, stride=1, dropout_ch=0, bn=False, reg='L1', input_image=32):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.bn = bn\n",
    "        self.reg = reg\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_ch, out_channels=filter_size[0], \n",
    "                            kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(in_channels=filter_size[0], out_channels=filter_size[1], \n",
    "                            kernel_size=kernel_size, padding=padding, stride=stride)      \n",
    "                \n",
    "        self.dim1 = self.compute_conv_dim(input_image, kernel_size, padding, stride)\n",
    "        self.dim2 = self.compute_conv_dim(self.dim1, kernel_size, padding, stride)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.dim2 * self.dim2 * filter_size[1], dlayers[0])\n",
    "        self.fc2 = nn.Linear(dlayers[0], num_class)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.activation = act_fn \n",
    "\n",
    "        # Dropout for convolution layers\n",
    "        self.dropout2D = nn.Dropout2d(dropout_ch)\n",
    "        # Dropout for dense layers\n",
    "        self.dropout = nn.Dropout(dropout_ch)\n",
    "\n",
    "        # Batch normalization layers for convolution layers\n",
    "        self.bn1 = nn.BatchNorm2d(filter_size[0])\n",
    "        self.bn2 = nn.BatchNorm2d(filter_size[1])\n",
    "        \n",
    "        # Batch normalization layer for dense layer\n",
    "        self.bn_fc1 = nn.BatchNorm1d(dlayers[0])\n",
    "        self.dim = []\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_conv_dim(input_size, kernel_size, padding, stride, pooling=True):\n",
    "        conv_dim = int((input_size + 2 * padding - kernel_size) / stride + 1)\n",
    "        if pooling: conv_dim = int(conv_dim / 2)\n",
    "        return conv_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.bn: x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        if self.bn: x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        if self.bn: x = self.bn_fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    # for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]: \n",
    "    # for f1_size in [16, 32, 64]:\n",
    "    # for f2_size in [32, 64, 96]:\n",
    "    # for bt_size in [8, 16, 32, 64]:\n",
    "    # for kernel_size in [3, 5, 7]:\n",
    "    # for stride in [1, 2, 3]:\n",
    "    # for padding in [0, 1, 2]:  \n",
    "    for seed in [12, 16, 42, 64, 71]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"CNN-simple\", \n",
    "            \"epochs\": 20,   \n",
    "            \"train_batch_size\": 32, \n",
    "            \"test_batch_size\": 32,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\":0,\n",
    "            \"L2_weight_decay\":0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 256],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 2,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": 0,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": seed  # [12, 16, 42, 64, 71]\n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        cnn_simple = SimpleCNN(\n",
    "            filter_size=config[\"filter_sizes\"],\n",
    "            dlayers=config[\"dense_layers\"],\n",
    "            act_fn=F.relu,\n",
    "            kernel_size=config[\"kernel_sizes\"],\n",
    "            padding=config[\"padding\"],\n",
    "            stride=config[\"stride\"],\n",
    "            dropout_ch=0,\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(cnn_simple, config=config,\n",
    "                        group=f'CNN_simple_seed_val', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['CNN', 'seed val'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unterschiedliche Lernrate**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_lr.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Anzahl Filter**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_num_filters.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Batchgrössen**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_bt_sizes.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Kernel Grössen**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_kernel_sizes.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Strides und Paddings**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_strides_paddings.PNG\" width=\"900\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Einfluss der Learningrate:**  \n",
    "\n",
    "\n",
    "**Einfluss der Filter Grösse:** \n",
    "- Filtergrössen im Bild zusammen genommen da wenig veränderugnen, \n",
    "- overfitting ansprechen, \n",
    "- Trainingszeiten anschauen\n",
    "- Nutzen evtl mit zusätzlichen layers \n",
    "\n",
    "\n",
    "**Einfluss der Batchgrössen:** \n",
    "- GPU Speicher schauen \n",
    "\n",
    "\n",
    "**Einfluss der Kernel Grösse:**  \n",
    "\n",
    "\n",
    "**Einfluss von Stride und Padding:**   \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Modell Evaluation**:  \n",
    "Für die optimierten Parameter soll das Modell mit 5Fold-Cross-Validation und mit unterschiedlichen Seeds geprüft werden\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_5f_cv_seeds.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "|Modell| Lernrate | Grösse Filter1 | Grösse Filter2 |Batchsize | Kernel, Stride, Padding |Train Acc | Test Acc | Test Schätzfehler ($\\sigma$) |\n",
    "|------|----------|----------   |---------- |---------- |---------- | --------          |--------        |--------                    |\n",
    "|CNN-simple-5f-cv   |1e-2      | 32     | 64      | 32      |  3,1,2    | 0.9671       | 0.8978       | 0.139      |\n",
    "|CNN-simple-seeds   |1e-2      | 32     | 64      | 32      |  3,1,2    | 0.8453      | 0.6900       | 0.009      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Fazit Modelkomplexität\n",
    "- MLP\n",
    "- CNN\n",
    "- weitere Modell Architekturen, letzten Jahren komplexer und besser, Datenlage achten\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**2. Nutzen der Regularisierung**  \n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:  \n",
    "a. L1/L2 Weight Penalty  \n",
    "b. Dropout\n",
    "\n",
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung verschiedener Regularisierungsstärken. Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist (Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden). Inwiefern wird dieses Ziel im gegebenen Fall erreicht?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Beschreibung warum\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLP** Regularisierung\n",
    "\n",
    "Folgende wird zum MLP Modell mit 2 Hidden Layers den Einsatz von Regularisierungen getestet. Es soll der Einfluss der `L1`, `L2` Regularisierung sowie von `Dropout` untersucht werden.\n",
    "\n",
    "**Ablauf:**  \n",
    "1. Modell ohne Regularisierung mit längerer Trainingsdauer\n",
    "1. L1-Regularisierung zu verschiedenen L1-Lambda Werten\n",
    "1. L2-Regularisierung zu verschiedenen Weight-Decay Werten\n",
    "1. Dropout mit verschiedenen Wahrscheinlichkeiten\n",
    "\n",
    "\n",
    "|Experiment| L1-Lambda | L2-Weight Decay |  Dropout |Train Acc |Test Acc | \n",
    "|---|----------|----------|----------|----------|----------|\n",
    "|1| 0| 0  | 0  | xx  | xx  | \n",
    "|2| 1e-5, 1e-4  | 0  | 0 | x  | xx  |\n",
    "|3| 0 | 1e-4, 1e-3, 1e-2|  0 |  xx | x  | \n",
    "|4| 0 | 0 | 0.2, 0.4, 0.6, 0.8  | xx | xx  | \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    # for no_reg in [1]:\n",
    "    # for L1_lambda in [1e-5, 1e-4]:  \n",
    "    # for L2_w_decay in [1e-4, 1e-3, 1e-2]:\n",
    "    for dropout_p in [0.2, 0.4, 0.6, 0.8]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"MLP-hL2\", \n",
    "            \"epochs\": 25,   \n",
    "            \"train_batch_size\": 16, \n",
    "            \"test_batch_size\": 16,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\": 0,\n",
    "            \"L2_weight_decay\": 0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 512],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": dropout_p,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": 42 \n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        mlp_h2 = MLPNet_hl2(\n",
    "            h_layer_sizes=config[\"hidden_layer_sizes\"], \n",
    "            act_fn=F.relu,\n",
    "            dropout=config['dropout'],\n",
    "            init_methode=config[\"init_w_method\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(mlp_h2, config=config,\n",
    "                        group=f'MLP_hL2_drop_{dropout_p}', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['MLP', 'dropout'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unterschiedliche L1-Lambda Werte**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_reg_l1.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche L2-Weight Decay Werte**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_reg_L2.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Dropout**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/MLP_hl2_reg_dropout.PNG\" width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Unterschiedliche L1-Lambda Werte**\n",
    "\n",
    "**Unterschiedliche L2-Weight Decay Werte**\n",
    "\n",
    "**Unterschiedliche Dropout**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CNN** Regularisierung\n",
    "\n",
    "Folgende wird zum CNN simple Modell den Einsatz von Regularisierungen getestet. Es soll der Einfluss der `L1`, `L2` Regularisierung sowie von `Dropout` untersucht werden.\n",
    "\n",
    "**Ablauf:**  \n",
    "1. Modell ohne Regularisierung mit längerer Trainingsdauer\n",
    "1. L1-Regularisierung zu verschiedenen L1-Lambda Werten\n",
    "1. L2-Regularisierung zu verschiedenen Weight-Decay Werten\n",
    "1. Dropout mit verschiedenen Wahrscheinlichkeiten\n",
    "\n",
    "\n",
    "|Experiment| L1-Lambda | L2-Weight Decay |  Dropout |Train Acc |Test Acc | \n",
    "|---|----------|----------|----------|----------|----------|\n",
    "|1| 0| 0  | 0  | xx  | xx  | --  | \n",
    "|2| 1e-5, 1e-4 | 0  | 0 | x  | xx  | \n",
    "|3| 0 | 1e-5, 1e-4, 1e-3, 1e-2|  0 |  xx | xx  | \n",
    "|4| 0 | 0  | 0.2, 0.4, 0.6  | xx | xx  | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    # for no_reg in [1]:\n",
    "    # for L1_lambda in [1e-5, 1e-4, 1e-3]:\n",
    "    # for L2_w_decay in [1e-4, 1e-3, 1e-2]:\n",
    "    for dropout_p in [0.2, 0.4, 0.6, 0.8]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"CNN-simple\", \n",
    "            \"epochs\": 30,   \n",
    "            \"train_batch_size\": 32, \n",
    "            \"test_batch_size\": 32,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\":0,\n",
    "            \"L2_weight_decay\":0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 256],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 2,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": dropout_p,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": 42  # [12, 16, 42, 64, 71]\n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        cnn_simple = SimpleCNN(\n",
    "            filter_size=config[\"filter_sizes\"],\n",
    "            dlayers=config[\"dense_layers\"],\n",
    "            act_fn=F.relu,\n",
    "            kernel_size=config[\"kernel_sizes\"],\n",
    "            padding=config[\"padding\"],\n",
    "            stride=config[\"stride\"],\n",
    "            dropout_ch=config[\"dropout\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(cnn_simple, config=config,\n",
    "                        group=f'CNN_simple_drop_{dropout_p}', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['CNN', 'dropout reg'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unterschiedliche L1-Lambda Werte**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_reg_L1.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche L2-Weight Decay Werte**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_reg_L2.PNG\" width=\"900\" height=\"400\">\n",
    "\n",
    "**Unterschiedliche Dropout**\n",
    "\n",
    "<img src=\"../01_Dokumentation/wandb_images/CNN_simple_reg_dropout.PNG\" width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- gridsearch teilweise unnötig ein paar parameter testen und evaluieren\n",
    "\n",
    "**Unterschiedliche L1-Lambda Werte**\n",
    "\n",
    "**Unterschiedliche L2-Weight Decay Werte**\n",
    "\n",
    "**Unterschiedliche Dropout Wahrscheinlichkeit**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**3. Nutzen von Batchnorm BN (ohne REG, mit SGD)**  \n",
    "Evaluiere, ob Batchnorm etwas bringt. Beschreibe kurz, was die Idee von BN ist, wozu es helfen soll.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Beschreibung warum\n",
    "\n",
    "- Kombination mit dropout erwähnen\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLP** und **CNN** Batchnorm\n",
    "\n",
    "Folgend wird zum MLP und CNN Modell den Einsatz von Batchnormalisierung getestet. \n",
    "\n",
    "**Ablauf:**  \n",
    "1. Modelle ohne Batchnormalisierung \n",
    "1. Modelle mit Batchnormalisierung \n",
    "\n",
    "\n",
    "|Modell| Batchnorm | Train Acc |Test Acc | \n",
    "|---|----------|----------|----------|\n",
    "|MLP-h2l| Aus|  xx  | xx  | \n",
    "|MLP-h2l|  Ein | x  | xx  |\n",
    "|CNN-simple| Aus |   xx | x  | \n",
    "|CNN-simple| Ein |  xx | xx  | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "if True:\n",
    "    from datetime import datetime\n",
    "\n",
    "    for bn in [False, True]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"MLP-hL2\", \n",
    "            \"epochs\": 25,   \n",
    "            \"train_batch_size\": 16, \n",
    "            \"test_batch_size\": 16,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\": 0,\n",
    "            \"L2_weight_decay\": 0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 512],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": 0,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": 42 \n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        mlp_h2 = MLPNet_hl2(\n",
    "            h_layer_sizes=config[\"hidden_layer_sizes\"], \n",
    "            act_fn=F.relu,\n",
    "            dropout=config['dropout'],\n",
    "            init_methode=config[\"init_w_method\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(mlp_h2, config=config,\n",
    "                        group=f'MLP_hL2_bn_{bn}', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['MLP', 'dropout'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "if False:\n",
    "    from datetime import datetime\n",
    "\n",
    "    # for no_reg in [1]:\n",
    "    # for L1_lambda in [1e-5, 1e-4, 1e-3]:\n",
    "    # for L2_w_decay in [1e-4, 1e-3, 1e-2]:\n",
    "    for dropout_p in [0.2, 0.4, 0.6, 0.8]:\n",
    "        # Hyperparameters  \n",
    "        config = {\n",
    "            \"name\": \"CNN-simple\", \n",
    "            \"epochs\": 30,   \n",
    "            \"train_batch_size\": 32, \n",
    "            \"test_batch_size\": 32,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"lr\": 1e-2, # default 1e-3\n",
    "            \"optimizer\": 'SGD',\n",
    "            \"Regularisierung\": 'None',  # 'None', 'L1', 'L2',\n",
    "            \"L1_lambda\":0,\n",
    "            \"L2_weight_decay\":0,\n",
    "            \"loss_func\": 'CrossEntropyLoss',\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"image_size\": 32,\n",
    "            \"cross_validation\": False,\n",
    "            \"n_folds\": 5,\n",
    "            \"is_test_batch\": False,\n",
    "            \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "            \"num_workers\": 0,\n",
    "            \"normalize\":\"zero_one\",\n",
    "            \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "            \"hidden_layer_sizes\": [1024, 256],\n",
    "            \"filter_sizes\": [32, 64],\n",
    "            \"dense_layers\": [512],\n",
    "            \"kernel_sizes\": 3,\n",
    "            \"padding\": 2,\n",
    "            \"stride\": 1,\n",
    "            \"pool_sizes\": [],\n",
    "            \"dropout\": dropout_p,\n",
    "            \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "            \"norm_std\": (0.5, 0.5, 0.5),\n",
    "            \"num_classes\": 10,\n",
    "            \"save_eval_image\": True,\n",
    "            \"set_seed\": 42  # [12, 16, 42, 64, 71]\n",
    "        }\n",
    "\n",
    "        # create model\n",
    "        cnn_simple = SimpleCNN(\n",
    "            filter_size=config[\"filter_sizes\"],\n",
    "            dlayers=config[\"dense_layers\"],\n",
    "            act_fn=F.relu,\n",
    "            kernel_size=config[\"kernel_sizes\"],\n",
    "            padding=config[\"padding\"],\n",
    "            stride=config[\"stride\"],\n",
    "            dropout_ch=config[\"dropout\"],\n",
    "            )\n",
    "\n",
    "        cv_train_acc_mean, cv_test_acc_mean, _, _ = model_trainer(cnn_simple, config=config,\n",
    "                        group=f'CNN_simple_drop_{dropout_p}', # _5f_cv, _seed_val, _hl_size{}, _lr{}\n",
    "                        tags=['CNN', 'dropout reg'], # 5fold cv, seed val, lr\n",
    "                        print_info=False, plot_eval=True, sound=True, \n",
    "                        write_wandb=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verwednung Batchnorm MLP und CNN**\n",
    "\n",
    "<!-- <img src=\"../01_Dokumentation/wandb_images/MLP_hl2_lr.PNG\" width=\"800\" height=\"400\"> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Nutzen von Batchnorm für MLP und CNN** (ohne REG, mit SGD)  \n",
    "Beschreibung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**4. Nutzen von Adam (ohne BN, ohne / mit REG)**   \n",
    "Evaluiere, ob Du mit Adam bessere Resultate erzielen kannst.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Beschreibung warum\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLP** und **CNN** Optimizer Adam\n",
    "\n",
    "Folgend wird zum MLP und CNN Modell den Einsatz von Batchnormalisierung getestet. \n",
    "\n",
    "**Ablauf:**  \n",
    "1. Modell mit SGD und mit/ohne Regularisierung\n",
    "1. Modell mit Adam und mit/ohne Regularisierung\n",
    "\n",
    "\n",
    "|Modell| Optimizer | Regularisierung | Train Acc |Test Acc | \n",
    "|---|----------|----------|----------|----------|\n",
    "|MLP-h2l| SGD|  ohne  | xx  | xx  |\n",
    "|MLP-h2l| SGD|  mit xx  | xx  | xx  | \n",
    "|MLP-h2l|  Adam | ohne  | xx  | xx  | \n",
    "|MLP-h2l|  Adam | mit xx  | xx  | xx  | \n",
    "|CNN-simple| SGD |   ohne | x  | xx  |\n",
    "|CNN-simple| SGD |  mit xx | xx  | xx  | \n",
    "|CNN-simple| Adam |  ohne | xx  | xx  | \n",
    "|CNN-simple| Adam |  mit xx | xx  | xx  | \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verwendung von Adam für MLP**\n",
    "\n",
    "<!-- <img src=\"../01_Dokumentation/wandb_images/MLP_hl2_lr.PNG\" width=\"800\" height=\"400\"> -->\n",
    "\n",
    "**Verwendung von Adam für CNN**\n",
    "\n",
    "<!-- <img src=\"../01_Dokumentation/wandb_images/MLP_hl2_lr.PNG\" width=\"800\" height=\"400\"> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Nutzen von Adam MLP und CNN** (ohne REG, mit SGD)\n",
    "\n",
    "Beschreibung\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ende Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "e452ba78c6e1c5f07381d012e3449bb8aff006d5d5ee6ce630663b7f08ee76a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
