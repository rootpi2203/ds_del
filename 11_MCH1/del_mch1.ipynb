{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - MCH1\n",
    "FS23, Manuel Schwarz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "\n",
    "# sound\n",
    "import time\n",
    "import winsound\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 1: Auswahl Task / Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dir Gedanken, mit welchen Daten Du arbeiten möchtest und welcher Task gelernt werden soll.\n",
    "    \n",
    "2. Diskutiere die Idee mit dem Fachcoach.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten und Task\n",
    "Pytorch stellt einige Datensets zur Verfügung [datasets torch](https://pytorch.org/vision/main/datasets.html).\n",
    "Verschiedene Kategorien stehen zur Auswahl:\n",
    "- Image classification\n",
    "- Image detection or segmentation\n",
    "- Optical Flow\n",
    "- Stereo Matching\n",
    "- Image pairs\n",
    "- Image captioning\n",
    "- video classification\n",
    "- Base classes for custom datasets\n",
    "\n",
    "\n",
    "**Datenset**  \n",
    "Eine beliebtes Dateset ist CIFAR10. Es beinhaltet Bilder von 10 Klassen (Flugzeuge, Katzen, Vögel, etc.), die Bilder kommen mit einer Auflösung von 32x32x3 pixel (rgb). Viele Tutorials starten mit diesem Datenset, das lässt darauf schliessen, dass der Rechenaufwand für die Hardware in einem vernümpftigen Rahmen liegt. Daher wird CIFAR10 als Datenset für die Challenge gewählt.\n",
    "\n",
    "**Task**  \n",
    "Anhand von CIFAR10 soll ein Modell erstellt werden, welches die Klasse eines Bildes korrekt klassifiziert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 2: Daten Kennenlernen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "1. Mache Dich mit dem Datensatz vertraut, indem Du eine explorative Analyse der Features durchführst: z.B. Vergleich der Klassen pro Feature, Balanciertheit der Klassen. \n",
    "2. Führe ein geeignetes Preprocessing durch, z.B. Normalisierung der Daten.  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explorative Datenanalyse\n",
    "[Tutorial Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten von CIFAR10\n",
    "data_path = './data/'\n",
    "train_data = torchvision.datasets.CIFAR10(data_path, train=True, download=True)\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datengrösse der Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Anzahl Trainingsdaten: {len(train_data)}\\n'\n",
    "      f'Anzahl Testdaten: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie sind die Bilder im Datensatz gespeichert?**  \n",
    "Die Bilder sind direkt auf dem Datenset via dem Index abrufbar. Ein Tupel mit dem Bild (RGB, 32x32 pixel) und dem Label (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_0, label_0 = train_data[0]\n",
    "img_0, label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deffinieren der Labels CIFAR10\n",
    "labels_cifar10_dict = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "labels_cifar10_list = list(labels_cifar10_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisierung der Bilder**  \n",
    "Die Bilder können mit matplotlib und imshow() direkt vom Dateset über den index dargestellt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(6,6))\n",
    "cols, rows = 5, 5\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_cifar10_list[label], fontsize=8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Untersuchen der Verteilungen der Klassen**  \n",
    "Folgend werden die Verteilungen der Klassen der Cifar10 Datensets auf den Trainings- und Testdaten geprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_data.targets\n",
    "test_target = test_data.targets\n",
    "print(f'Labels in Trainingsdaten: {len(train_target)}')\n",
    "print(f'Labels in Testdaten: {len(test_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "ax[0].hist(train_target)\n",
    "ax[0].set_xticks(np.arange(10))\n",
    "ax[0].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[0].set_title('Trainingset', fontsize=8)\n",
    "\n",
    "ax[1].hist(test_target)\n",
    "ax[1].set_xticks(np.arange(10))\n",
    "ax[1].set_xticklabels(labels_cifar10_list, rotation=45)\n",
    "ax[1].set_title('Testset', fontsize=8)\n",
    "\n",
    "plt.suptitle('Verteilung der Klassen von Cifar-10', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Das Trainingsset umfasst total 50'000 Bilder, davon sind jeweils 5000 Bilder jeder Klasse enthalten (Balanced Datenset). Somit kann zum Beispiel eine Metrik wie'Accuracy' verwendet werden, um die Klassifikation der Modelle zu beurteilen und zu vergleichen. Die 10'000 Bilder in den Testdaten sind ebenfalls gleich verteilt.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mittelwerte und Standardabweichungen des Trainingdatensets (RGB):\n",
    "Benötigt um die Daten zu normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Datenset mit Tensoren für Berechnunen\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = torchvision.datasets.CIFAR10(data_path, train=True, download=False, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(data_path, train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_std_dataset(train_data, print_info=True):\n",
    "    '''\n",
    "    Berechnet den Mittelwert und Standardabweichung für alle Billder\n",
    "    '''\n",
    "    num_images = len(train_data)\n",
    "\n",
    "    # erstelle Array mit Dimension der input Bilder\n",
    "    pixel_values = np.zeros((num_images, 3, 32, 32), dtype=np.float32)\n",
    "\n",
    "    # füllen des Arrays mit Pixel Werten\n",
    "    for i in range(num_images):\n",
    "        image, _ = train_data[i]\n",
    "        pixel_values[i] = image.numpy()\n",
    "\n",
    "    mean = np.mean(pixel_values, axis=(0, 2, 3))\n",
    "    std = np.std(pixel_values, axis=(0, 2, 3))\n",
    "\n",
    "    if print_info:\n",
    "        print(\"RGB-Mittelwerte:\", mean)\n",
    "        print(\"RGB-Standardabweichungen:\", std)\n",
    "\n",
    "calc_mean_std_dataset(train_data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing der Daten  \n",
    "Folgend werden die Daten in einem Preprocessing Schritt für die Modelle vorbereitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function preprocessing\n",
    "def preprocessing_cifar10(path='./data/', train_batch_size=32, test_batch_size=32, normalize='zero_one',\n",
    "                          norm_mean=(0.4914009  , 0.548215896, 0.4465308), \n",
    "                          norm_std=(0.24703279 , 0.24348423 , 0.26158753), \n",
    "                          download=True, print_info=False):\n",
    "    '''\n",
    "    '''\n",
    "    if print_info: print(f'------------'), print(f'Preprocessing start')\n",
    "\n",
    "    if normalize == 'zero_one':\n",
    "        # transform tensor to normalized range [0, 1], 0=schwarz\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "    elif normalize == 'minusone_one':\n",
    "        # transform tensor to normalized range [-1, 1], 0=schwarz\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(norm_mean, norm_std)])\n",
    "    else:\n",
    "        raise ValueError('Nomalize must be either \"zero_one\" or \"minusone_one\"') \n",
    "    # if print_info: print('Normalized Tensor'), print(f'mean: {norm_mean} | std: {norm_std}')\n",
    "    \n",
    "    # CIFAR10: 50000 32x32 color images in 10 classes, with 5000 images per class\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                            download=download, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                        download=download, transform=transform)\n",
    "    if print_info: \n",
    "        print(f'Data transformed: {normalize}')\n",
    "        calc_mean_std_dataset(train_dataset, print_info)\n",
    "\n",
    "    # dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    if print_info: print(f'Dataloader created with {train_batch_size=}, {test_batch_size=}')\n",
    "    if print_info: print(f'Preprocessing done'), print('------------')   \n",
    "\n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "data_path = './data/'\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(path=data_path, \n",
    "                                                                               train_batch_size=batch_size,\n",
    "                                                                               test_batch_size=batch_size,\n",
    "                                                                               normalize='zero_one',  # minusone_one\n",
    "                                                                               download=False,  \n",
    "                                                                               print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Aus den Berechnungen des Mittelwert und Standardabweichung lässt sich schliessen dass die Daten von Cifar-10 bereits normalisiert wurden auf Werte zwischen [0,1]. Mit der Transformation im Preprocessing `transforms.Normalize(mean, std)` können die Werte in den Bereich [-1, 1] gesetzt werden. Welcher Wertebereich die bessere Wahl ist, ist Situationsbedingt (z.B. verwendete Aktivierungsfunktion). Eine Zentrierung um Null kann für Netze Vorteile haben, hingegen sind Werte zwischen [0,1] besser zu interpretieren (0=schwarz, 1=RGB 255). Vorerst soll mit dem Wertebereich [0,1] gearbeitet werden.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 3: Aufbau Modellierung  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "1. Lege fest, wie (mit welchen Metriken) Du die Modelle evaluieren möchtest. Berücksichtige auch den Fehler in der Schätzung dieser Metriken.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Klassengrössen sind ausbalanciert, daher ist für die Metrik **Accuracy** für die Modell Beurteilung geeignet und soll hier für die Modellbewertung zum Einsatz kommen:\n",
    "\n",
    "$$ Accuracy = \\frac{Anzahl\\ korrekte\\ Klassifizierungen}{Total\\ Klassifizierungen}  $$\n",
    "\n",
    "Für eine Klassifizierung von mehr als zwei Klassen eignet sich **CrossEntropyLoss**. Mehr dazu [hier](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):\n",
    "\n",
    "$$ \\text{CrossEntropyLoss} = -\\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right] $$\n",
    "\n",
    "\n",
    "Die Accuracy berechnet eine Schätzung. Bei der Initialisierung der Modellgewichte werden Zufallswerte verwendet. Auch die Wahl der Bilder innerhalb der Batchsize wird durch `shuffle=True` zufällig getroffen *(siehe `Preprocessing`)*. Somit varriert die Accuracy nach jedem Modeltraining ein wenig. Cross-Validation würde sich hier anbieten, um auf k-folds unterschiedliche Modelle zu erstellen. Auch ein Modell mehrmals ausführen wäre denkbar. Mit dem berechneten Mittelwert $\\mu$ und der Standardabweichung $\\sigma$ kann ein Fehlerabschätzung gemacht werde.  \n",
    "$$ Fehler_{range} = [\\mu - \\sigma; \\mu + \\sigma]$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "2. Implementiere Basisfunktionalität, um Modelle zu trainieren und gegeneinander zu evaluieren. Wie sollen die Gewichte initialisiert werden?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Die Methode wie die Intitialisierung der Gewichte stattfindet hat Einfluss wie schnell das Modell konvergiert und hilft die Probleme wie `vanishing` oder `exploding` Gradienten abzuschwächen. Kleine zufällige Werte führen zu einem effizienteren Training. Grosse Werte führen zu Problemenn bei dem das Modell nicht oder nur sehr langsam konvergiert. Je nach Problemstellung und Modelarchitekture können unterschiedliche Initialisierungsmethoden verwendet werden\n",
    "\n",
    "Mit der Verwendung von `nn.init` (Pytorch) stehen zum Beispiel folgende Optionen zur Verfügung:\n",
    "1. **Uniform initialization** (help prevents: vanishing gradient, can suffer: exploding gradient)\n",
    "1. **Xavier initialization** (help prevent: vanishing gradient)\n",
    "1. **Kaiming initialization** (help prevent: vanishing gradient, account activation function)\n",
    "1. **Normal initialization** (help prevent: exploding gradient)\n",
    "1. **Zeros initialization** (can suffer: slow converge, vanishing gradient)\n",
    "1. **One’s initialization** (can suffer: slow converge, vanishing gradient)\n",
    "\n",
    "Die genaue Beschreibung der Intitialisierungen können [hier](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/) gefunden werden. Eine eigene Custom-Option stellt Pytorch auch zur Verfügung.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions  \n",
    "Um die Erkentnisse dieses Notebook reproduzierbar zu machen, ist es wichtig einen Seed zu definieren. Folgend eine Seed Funktion mit einem üblichen Standart [Quelle](https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "set_seed()\n",
    "\n",
    "def plot_loss_epoch(num_epochs, loss, figsize=(8, 4)):\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "    plt.plot(np.arange(num_epochs), loss)\n",
    "    plt.xticks(np.arange(num_epochs))\n",
    "    plt.title('Loss Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "def calc_true_predictions(output_model, true_labels):\n",
    "    _, predicted = torch.max(output_model.data, 1)\n",
    "    return (predicted == true_labels).sum().item()\n",
    "\n",
    "def measure_model_time(start_time, calc='min'):\n",
    "    model_time = np.round(((time.time() - start_time) / 60), 2)  # from seconds to minute\n",
    "    return model_time\n",
    "\n",
    "def play_sound(typ=0):\n",
    "    # play 'finish' sound\n",
    "    if typ==0:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep.wav', winsound.SND_ASYNC)\n",
    "    if typ==1:\n",
    "        winsound.PlaySound('../01_Dokumentation/win_sounds/beep2.wav', winsound.SND_ASYNC)\n",
    "\n",
    "def plot_init_weights(model, figsize=(6, 3)):\n",
    "    l1_weights = model.state_dict()['linear1.weight'].numpy()\n",
    "    l2_weights = model.state_dict()['linear2.weight'].numpy()\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize)\n",
    "\n",
    "    ax[0].hist(l1_weights.flatten(), bins=50)\n",
    "    ax[1].hist(l2_weights.flatten(), bins=50)\n",
    "\n",
    "    ax[0].set_title(\"Layer 1\", fontsize=8)\n",
    "    ax[0].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[0].set_ylabel(\"Anzahl\", fontsize=8)\n",
    "    ax[0].tick_params(axis='y', labelsize=6)\n",
    "    ax[0].tick_params(axis='x', labelsize=6)\n",
    "    ax[1].set_title(\"Layer 2\", fontsize=8)\n",
    "    ax[1].set_xlabel(\"Gewichtswert\", fontsize=6)\n",
    "    ax[1].tick_params(axis='y', labelsize=6)\n",
    "    ax[1].tick_params(axis='x', labelsize=6)\n",
    "    plt.suptitle(f'Verteilung Initialisierungs Methode')\n",
    "    plt.show()\n",
    "\n",
    "def test_eval_plot():\n",
    "    num_epochs = 10\n",
    "    n_loss_epochs = (0.05 * np.sqrt(np.arange(10))) * -1\n",
    "    n_correct_train = 0.1 * n_loss_epochs**4\n",
    "    n_correct_test = 0.05 * n_loss_epochs**4\n",
    "    # create dataloader\n",
    "    train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(batch_size=4,\n",
    "                                                                                norm_mean=(0.5, 0.5, 0.5), \n",
    "                                                                                norm_std=(0.5, 0.5, 0.5),\n",
    "                                                                                download=False,  \n",
    "                                                                                print_info=False)\n",
    "    eval_model(num_epochs, n_loss_epochs, n_correct_train, n_correct_test,\n",
    "                train_loader, test_loader)\n",
    "# test_eval_plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(num_epochs, n_loss_epochs, n_val_loss_epochs, \n",
    "                n_acc_train_epochs, n_acc_test_epochs, train_loader, test_loader, \n",
    "                n_loss_batches=None, group_name='group_name', tag_name='tag_name', figsize=(8,5),\n",
    "                print_info=True, save_image=False):\n",
    "    \n",
    "    epoch = np.arange(num_epochs)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(epoch, n_loss_epochs, color= 'grey', linestyle='--', label='Train Loss')\n",
    "    ax1.plot(epoch, n_val_loss_epochs, color= 'grey', label='Test Loss')\n",
    "    ax2.plot(epoch, n_acc_train_epochs, color='steelblue', label='Accuracy Train')\n",
    "    ax2.plot(epoch, n_acc_test_epochs, color='coral', label='Accuracy Test')\n",
    "    \n",
    "    ax1.set_xticks(epoch)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss') \n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.tick_params(axis='x', labelsize=8)\n",
    "    ax1.tick_params(axis='y', labelsize=8)\n",
    "    ax2.tick_params(axis='y', labelsize=8)\n",
    "    ax1.legend(loc='upper center', bbox_to_anchor=(0.25, -0.12), ncol=3)    \n",
    "    ax2.legend(loc='upper center', bbox_to_anchor=(0.72, -0.12), ncol=3)\n",
    "    \n",
    "    plt.suptitle(f'Training: {group_name} ')\n",
    "    cur_date = datetime.now().strftime(\"%d.%m.%Y_%H%M\")\n",
    "    plt.title(f'{cur_date} - Tags: {tag_name}', size=6)\n",
    "    plt.grid()\n",
    "    if save_image: \n",
    "            plt.savefig(f'./plots/eval_model_plots/{cur_date}_{group_name}_{tag_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    if print_info:\n",
    "        print(f'Accuracy Train {n_acc_train_epochs[-1]:2f}')\n",
    "        print(f'Accuracy Test {n_acc_test_epochs[-1]:4f}')\n",
    "        print(f'Loss Train {n_loss_epochs[-1]:2f}')\n",
    "        print(f'Loss Test {n_val_loss_epochs[-1]:2f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Hyperparameters example\n",
    "config = {\n",
    "    \"name\": \"MLP\", \n",
    "    \"epochs\": 20,   \n",
    "    \"train_batch_size\": 32, \n",
    "    \"test_batch_size\": 32,\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"lr\": 1e-3, \n",
    "    \"optimizer\": 'SGD',\n",
    "    \"loss_func\": 'CrossEntropyLoss',\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"image_size\": 32,\n",
    "    \"cross_validation\": False,\n",
    "    \"is_test_batch\": True,\n",
    "    \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "    \"num_workers\": 0,\n",
    "    \"normalize\":\"zero_one\",\n",
    "    \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "    \"hidden_layer_sizes\": [128], \n",
    "    \"kernel_sizes\": [],\n",
    "    \"pool_sizes\": [],\n",
    "    \"dropout\": 0,\n",
    "    \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "    \"norm_std\": (0.5, 0.5, 0.5),\n",
    "    \"num_classes\": 10,\n",
    "    \"save_eval_image\": True\n",
    "}\n",
    "\n",
    "\n",
    "# Train model function\n",
    "def train_model(\n",
    "        model, train_loader, test_loader, config, tags=['tag_name'], group='group_name',\n",
    "        print_info=False, plot_eval=True, sound=False, write_wandb=True):  \n",
    "    \n",
    "    set_seed()\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "    if print_info: print(f'------------'), print(f'Starten des Trainings auf Device {device}')\n",
    "\n",
    "    # Model to device\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # define Loss and Optimizer\n",
    "    if config['loss_func'] == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if print_info: print(f\"Loss Funktion: {config['loss_func']}\")        \n",
    "    if config['optimizer'] == 'SGD':        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'])\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}\")\n",
    "    elif config['optimizer']  == 'Adam':        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        if print_info: print(f\"Optimizer: {config['optimizer']} mit lr: {config['lr']}\")\n",
    "\n",
    "    # Initialize wandb\n",
    "    if write_wandb: \n",
    "        wandb.init(\n",
    "            project=\"del-mc1\",\n",
    "            entity='manuel-schwarz',\n",
    "            group=group,\n",
    "            name=f\"{config['name']}-{config['epochs']}-epochs-{config['optimizer']}-optimizer-{config['start_time']}\",\n",
    "            tags=tags + (['is_test_batch'] if config['is_test_batch'] else []),\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # --------------------- train loop --------------------------\n",
    "    if write_wandb:\n",
    "        wandb.watch(model)\n",
    "\n",
    "    n_loss_epochs = []\n",
    "    n_val_loss_epochs = []\n",
    "    n_loss_all_batches = []\n",
    "    n_val_loss_all_batches = []\n",
    "    n_correct_train_epochs = []\n",
    "    n_correct_test_epochs = []\n",
    "    n_acc_train_epochs = []\n",
    "    n_acc_test_epochs = []\n",
    "\n",
    "    best_train_loss = 0\n",
    "    best_val_loss = 0\n",
    "    best_train_acc = 0\n",
    "    best_test_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    loop = range(config[\"epochs\"])\n",
    "    epoch_loop = tqdm(loop, desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "    for epoch in epoch_loop:\n",
    "        true_train = 0\n",
    "        n_loss_batch = []\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            if config['is_test_batch'] and i > 1:\n",
    "                break\n",
    "            # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "            # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_loss_batch.append(loss.item())\n",
    "            true_train += calc_true_predictions(outputs, labels)\n",
    "            # acc_train = true_train / len(train_loader.dataset)\n",
    "            n_loss_all_batches.append(loss.item())\n",
    "\n",
    "            inner_progress = f\"{i+1}/{len(train_loader)}\"\n",
    "            epoch_loop.set_description(f\"Epochs (Batch: {inner_progress})\")\n",
    "            epoch_loop.refresh()\n",
    "\n",
    "        # save results\n",
    "        acc_train = true_train / len(train_loader.dataset)\n",
    "        epoch_loss = np.mean(n_loss_batch)\n",
    "\n",
    "        n_correct_train_epochs.append(true_train)        \n",
    "        n_acc_train_epochs.append(acc_train)  \n",
    "        n_loss_epochs.append(epoch_loss)\n",
    "\n",
    "        # ---------- eval\n",
    "        model.eval()\n",
    "        true_test_val = 0 \n",
    "        n_val_loss_batch = []\n",
    "        # n_val_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                if config['is_test_batch'] and i > 1:\n",
    "                    break\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                n_val_loss_batch.append(loss.item())\n",
    "                true_test_val += calc_true_predictions(outputs, labels)\n",
    "                n_val_loss_all_batches.append(loss.item())              \n",
    "                # _, predicted = torch.max(outputs, 1)  # prediction speichern todo\n",
    "\n",
    "        # save results\n",
    "        acc_test = true_test_val / len(test_loader.dataset)\n",
    "        epoch_val_loss = np.mean(n_val_loss_batch)\n",
    "\n",
    "        n_correct_test_epochs.append(true_test_val)\n",
    "        n_acc_test_epochs.append(acc_test) \n",
    "        n_val_loss_epochs.append(epoch_val_loss)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # best scores\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "\n",
    "        if acc_train > best_train_acc:\n",
    "            best_train_acc = acc_train\n",
    "\n",
    "        if acc_test > best_test_acc:\n",
    "            best_test_acc = acc_test\n",
    "            best_epoch = epoch\n",
    "\n",
    "        # log metrics to wandb\n",
    "        if write_wandb:\n",
    "            wandb.log({\n",
    "                \"loss epoch\": epoch_loss, \n",
    "                \"validation loss epoch\": epoch_val_loss, \n",
    "                \"acc_train\": acc_train, \n",
    "                \"acc_test\": acc_test,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_epoch\": best_train_loss,\n",
    "                \"best_epoch\": best_val_loss,\n",
    "                \"best_epoch\": best_train_acc,\n",
    "                \"best_epoch\": best_test_acc,\n",
    "                })\n",
    "\n",
    "    acc_train = n_correct_train_epochs[-1] / len(train_loader.dataset)\n",
    "    acc_test = n_correct_test_epochs[-1] / len(test_loader.dataset)\n",
    "\n",
    "    if plot_eval:\n",
    "        eval_model(config['epochs'], n_loss_epochs, n_val_loss_epochs, \n",
    "                    n_acc_train_epochs, n_acc_test_epochs, train_loader, test_loader,\n",
    "                    group_name=group, tag_name=tags, print_info=print_info, save_image=config['save_eval_image'])\n",
    "        \n",
    "    print(\n",
    "            f\"Epoch {epoch + 1}/{config['epochs']}, \\\n",
    "            Loss: {round(epoch_loss, 5)}, Validation Loss: {round(epoch_val_loss, 5)} \\\n",
    "            Acc: {round(acc_train, 5)}, Validation Acc: {round(acc_test, 5)}\"\n",
    "            )\n",
    "    \n",
    "    if sound: play_sound(1)\n",
    "\n",
    "    if write_wandb: \n",
    "        wandb.finish()\n",
    "\n",
    "    print('Finished Training')\n",
    "    return epoch_loss, epoch_val_loss, acc_train, acc_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfacher Modell Test  \n",
    "Folgend soll ein einfaches Modell mit einem hidden Layer erstellt werden um die Basisfunktionen von `train_model()` zu testen. \n",
    "\n",
    "Initilisierung Gewichte: \n",
    "Da unterschiedliche Problemstellung verschiedene Initialisierungen erfordern, sollen mehrere Methoden ausprobiert werden um die Gewichte zu initialisiern `linear_layer = torch.nn.Linear(2, 3)`:\n",
    "1. `uniform`: torch.nn.init.uniform_(linear_layer.weight)\n",
    "1. `xavier`: torch.nn.init.xavier_uniform_(linear_layer.weight)\n",
    "1. `normal`: torch.nn.init.normal_(linear_layer.weight, mean=0, std=1)\n",
    "1. `kaiming_norm`: torch.nn.init.kaiming_normal_(linear_layer.weight, nonlinearity=\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell Class\n",
    "class MLPNet_hl1(nn.Module):\n",
    "    def __init__(self, hidden_l_1:list, act_fn=F.relu, dropout=0, init_methode:str='kaiming_norm'):\n",
    "        super(MLPNet_hl1, self).__init__()\n",
    "        self.linear1 = nn.Linear(3*32*32, hidden_l_1[0])  # input.shape = (n, 3, 32, 32)\n",
    "        self.linear2 = nn.Linear(hidden_l_1[0], 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = act_fn \n",
    "\n",
    "        for linear_weight in [self.linear1.weight, self.linear2.weight]:          \n",
    "            if init_methode == 'uniform':\n",
    "                torch.nn.init.uniform_(linear_weight)\n",
    "            if init_methode == 'xavier':\n",
    "                torch.nn.init.xavier_uniform_(linear_weight)\n",
    "            if init_methode == 'normal':\n",
    "                torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "            if init_methode == 'kaiming_norm':\n",
    "                torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x) # x.shape = (n, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_lst = ['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "# create model\n",
    "for method in method_lst:\n",
    "    model = MLPNet_hl1(hidden_l_1=[64], init_methode=method)\n",
    "    plot_init_weights(model, figsize=(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilungen der Gewichte mit den Methoden `uniform` und  `xavier` sind sehr ähnlich. Mit Xavier ist die X-Skala unterschiedlich, da die Gewichte so skalliert werden dass die Varianz des Output der Varianz des Inputs entspricht.  \n",
    "Bei `normal` und `kaiming_norm` besteht das gleiche Prinzip, die skallierung der Gewichte, zudem wird die Aktivierungsfunktion berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Hyperparameters  \n",
    "    config = {\n",
    "        \"name\": \"MLP-test1\", \n",
    "        \"epochs\": 5,   \n",
    "        \"train_batch_size\": 32, \n",
    "        \"test_batch_size\": 32,\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"lr\": 1e-3, \n",
    "        \"optimizer\": 'SGD',\n",
    "        \"loss_func\": 'CrossEntropyLoss',\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"image_size\": 32,\n",
    "        \"cross_validation\": False,\n",
    "        \"is_test_batch\": False,\n",
    "        \"start_time\": datetime.now().strftime(\"%d.%m.%Y_%H%M\"),\n",
    "        \"num_workers\": 0,\n",
    "        \"normalize\":\"zero_one\",\n",
    "        \"init_w_method\":\"kaiming_norm\",  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "        \"hidden_layer_sizes\": [128], \n",
    "        \"kernel_sizes\": [],\n",
    "        \"pool_sizes\": [],\n",
    "        \"dropout\": 0,\n",
    "        \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "        \"norm_std\": (0.5, 0.5, 0.5),\n",
    "        \"num_classes\": 10,\n",
    "        \"save_eval_image\": True\n",
    "    }\n",
    "\n",
    "    # create dataloader\n",
    "    _, _, train_loader, test_loader = preprocessing_cifar10(train_batch_size=config[\"train_batch_size\"],\n",
    "                                                            test_batch_size=config[\"test_batch_size\"],\n",
    "                                                            normalize=config['normalize'],\n",
    "                                                            download=False,  \n",
    "                                                            print_info=False)\n",
    "    # create model\n",
    "    MLPNet_hl1(hidden_l_1=config[\"hidden_layer_sizes\"], \n",
    "            init_methode=config[\"init_w_method\"])\n",
    "\n",
    "    # train model\n",
    "    train_model(model, train_loader, test_loader, config=config,\n",
    "                group='MLP_test',\n",
    "                tags=['MLP'],\n",
    "                print_info=False, plot_eval=True, sound=True, \n",
    "                write_wandb=True\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schritt 4: Evaluation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Bei der Evaluation ist darauf zu achten, dass das Vorgehen stets möglichst reflektiert erfolgt und versucht wird, die Ergebnisse zu interpretieren. Am Schluss soll auch ein Fazit gezogen werden, darüber welche Variante am besten funktioniert.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**1. Training mit SGD, ohne REG, ohne BN:**  \n",
    "Untersuche verschiedene Modelle unterschiedlicher Komplexität, welche geeignet sein könnten, um das Klassifikationsproblem zu lösen. Verwende Stochastic Gradient Descent - ohne Beschleunigung, ohne Regularisierung (REG) und ohne Batchnorm (BN).  \n",
    "\n",
    "a. Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein sorgfältiges Hyper-Parameter-Tuning durch (Lernrate, Batch-Grösse). Achte stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl Epochen, Lernrate und Batch-Grösse.\n",
    "\n",
    "b. Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning) verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding.\n",
    "\n",
    "c. Fasse die Ergebnisse zusammen in einem geeigneten Plot, bilde eine Synthese und folgere, welche Modell-Komplexität Dir am sinnvollsten erscheint.  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition von MLP Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singel MLP Model class\n",
    "class MLPNet_hlXX(nn.Module):\n",
    "    def __init__(self, hidden_l_1:list, act_fn=F.relu, dropout=0, init_methode:str='kaiming_norm'):\n",
    "        super(MLPNet_hlXX, self).__init__()\n",
    "        self.init_methode = init_methode\n",
    "        self.num_h_layers = len(hidden_l_1)\n",
    "        print(f'number Layers: {self.num_h_layers}')\n",
    "\n",
    "        self.linear1 = nn.Linear(3*32*32, hidden_l_1[0])  # input.shape = (n, 3, 32, 32)\n",
    "        if self.num_h_layers == 1:            \n",
    "            self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "            self.linear2 = nn.Linear(hidden_l_1[0], 10)\n",
    "            self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.activation = act_fn \n",
    "            \n",
    "        if self.num_h_layers == 2:\n",
    "            self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "            self.linear2 = nn.Linear(hidden_l_1[0], hidden_l_1[1])\n",
    "            self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "            self.linear3 = nn.Linear(hidden_l_1[1], 10)\n",
    "            self.linear3.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.activation = act_fn \n",
    "\n",
    "        if self.num_h_layers == 3:\n",
    "            self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "            self.linear2 = nn.Linear(hidden_l_1[0], hidden_l_1[1])\n",
    "            self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "            self.linear3 = nn.Linear(hidden_l_1[1], hidden_l_1[2])\n",
    "            self.linear3.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.linear4 = nn.Linear(hidden_l_1[2], 10)\n",
    "            self.linear4.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.activation = act_fn \n",
    "\n",
    "        if self.num_h_layers == 4:\n",
    "            self.linear1.weight = self.init_dits_weight(self.linear1.weight)\n",
    "            self.linear2 = nn.Linear(hidden_l_1[0], hidden_l_1[1])\n",
    "            self.linear2.weight = self.init_dits_weight(self.linear2.weight)\n",
    "            self.linear3 = nn.Linear(hidden_l_1[1], hidden_l_1[2])\n",
    "            self.linear3.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.linear3 = nn.Linear(hidden_l_1[2], hidden_l_1[3])\n",
    "            self.linear3.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.linear4 = nn.Linear(hidden_l_1[3], 10)\n",
    "            self.linear4.weight = self.init_dits_weight(self.linear3.weight)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.activation = act_fn \n",
    "\n",
    "    def forward(self, x): # x.shape = (n, 3, 32, 32)\n",
    "        x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "        if self.num_h_layers == 1:            \n",
    "            x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear2(x) # x.shape = (n, 10)\n",
    "\n",
    "        if self.num_h_layers == 2:\n",
    "            x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear3(x) # x.shape = (n, 10)\n",
    "\n",
    "        if self.num_h_layers == 3:\n",
    "            x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear3(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear4(x) # x.shape = (n, 10)\n",
    "\n",
    "        if self.num_h_layers == 4:\n",
    "            x = x.view(-1, 3*32*32) # x.shape = (n, 3072)\n",
    "            x = self.activation(self.linear1(x)) # x.shape = (n, 128)\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear3(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(self.linear4(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear5(x) # x.shape = (n, 10)    \n",
    "        return x\n",
    "    \n",
    "    def init_dits_weight(self, linear_weight):\n",
    "        if self.init_methode == 'uniform':\n",
    "            return torch.nn.init.uniform_(linear_weight)\n",
    "        if self.init_methode == 'xavier':\n",
    "            return torch.nn.init.xavier_uniform_(linear_weight)\n",
    "        if self.init_methode == 'normal':\n",
    "            return torch.nn.init.normal_(linear_weight, mean=0, std=1)\n",
    "        if self.init_methode == 'kaiming_norm':\n",
    "            return torch.nn.init.kaiming_normal_(linear_weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition von CNN Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell Class\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deffinition wandb Trainingloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_mlp_1hl():\n",
    "#         wandb.init(settings=wandb.Settings(silent=\"True\"))\n",
    "#         config = wandb.config\n",
    "#         run_name = f'{config[\"name\"]}_lr_{str(config[\"lr\"])}_layer_{str(config[\"hidden_layer_sizes\"])}_bs_{str(config[\"batchsize\"])}'\n",
    "#         print(run_name)\n",
    "#         wandb.run.name = run_name\n",
    "\n",
    "#         # create dataloader\n",
    "#         _, _, train_loader, test_loader = preprocessing_cifar10(batch_size=config[\"batchsize\"],\n",
    "#                                                                                         norm_mean=config[\"norm_mean\"], \n",
    "#                                                                                         norm_std=config[\"norm_std\"],\n",
    "#                                                                                         download=False,  \n",
    "#                                                                                         print_info=False)\n",
    "#         # create model\n",
    "#         MLPNet_hl1(hidden_l_1=config[\"hidden_layer_sizes\"][0], \n",
    "#                 init_methode=config[\"init_w_method\"])\n",
    "\n",
    "#         # train model\n",
    "#         train_model(model, train_loader, test_loader,\n",
    "#                         loss_func=config[\"loss_func\"], \n",
    "#                         opt=config[\"opt\"], \n",
    "#                         lr=config[\"lr\"], \n",
    "#                         num_epochs=config[\"num_epochs\"], \n",
    "#                         id_name=config[\"name\"],\n",
    "#                         print_info=False, plot_eval=False, sound=False, write_wandb=True)\n",
    "        \n",
    "def train_mlp_hlXX():\n",
    "        wandb.init(settings=wandb.Settings(silent=\"True\"))\n",
    "        config = wandb.config\n",
    "        run_name = f'{config[\"name\"]}_lr_{str(config[\"lr\"])}_layer_{str(config[\"hidden_layer_sizes\"])}_bs_{str(config[\"batchsize\"])}'\n",
    "        print(run_name)\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        # create dataloader\n",
    "        _, _, train_loader, test_loader = preprocessing_cifar10(batch_size=config[\"batchsize\"],\n",
    "                                                                                        norm_mean=config[\"norm_mean\"], \n",
    "                                                                                        norm_std=config[\"norm_std\"],\n",
    "                                                                                        download=False,  \n",
    "                                                                                        print_info=False)\n",
    "        # create model\n",
    "        MLPNet_hlXX(hidden_l_1=config[\"hidden_layer_sizes\"], \n",
    "                init_methode=config[\"init_w_method\"])\n",
    "\n",
    "        # train model\n",
    "        train_model(model, train_loader, test_loader,\n",
    "                        loss_func=config[\"loss_func\"], \n",
    "                        opt=config[\"opt\"], \n",
    "                        lr=config[\"lr\"], \n",
    "                        num_epochs=config[\"num_epochs\"], \n",
    "                        tags=config[\"name\"],\n",
    "                        print_info=True, plot_eval=False, sound=False, write_wandb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test modellklassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_simple_model = False\n",
    "# if train_simple_model:\n",
    "#     send_data_to_wand = False\n",
    "#     exp_name = \"simple mlp (1hl)\"\n",
    "#     project_name = \"del_mc1_test\"\n",
    "\n",
    "#     #--------- Hyperparameters ---------- \n",
    "#     config = {\n",
    "#         \"dataset\": \"CIFAR-10\",\n",
    "#         \"name\": exp_name,\n",
    "#         \"architecture\": \"MLP\", \n",
    "#         \"num_epochs\": 2,   \n",
    "#         \"batchsize\": 64,\n",
    "#         \"lr\": 1e-3, \n",
    "#         \"opt\": 'SGD',\n",
    "#         \"loss_func\": 'CrossEntropyLoss',\n",
    "#         \"hidden_layer_sizes\": [64, 128, 256], \n",
    "#         \"kernel_sizes\": [],\n",
    "#         \"activation\": \"ReLU\",\n",
    "#         \"pool_sizes\": [],\n",
    "#         \"dropout\": 0,\n",
    "#         \"norm_mean\": (0.5, 0.5, 0.5),\n",
    "#         \"norm_std\": (0.5, 0.5, 0.5),\n",
    "#         \"num_classes\": 10,\n",
    "#         \"init_w_method\":\"kaiming_norm\"  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "#     }\n",
    "\n",
    "#     if send_data_to_wand:\n",
    "#         wandb.init(project=project_name, name=exp_name, notes=\"\", config=config)\n",
    "\n",
    "\n",
    "#     # create dataloader\n",
    "#     train_dataset, test_dataset, train_loader, test_loader = preprocessing_cifar10(batch_size=config[\"batchsize\"],\n",
    "#                                                                                 norm_mean=config[\"norm_mean\"], \n",
    "#                                                                                 norm_std=config[\"norm_std\"],\n",
    "#                                                                                 download=False,  \n",
    "#                                                                                 print_info=False)\n",
    "#     # create model\n",
    "#     MLPNet_hlXX(hidden_l_1=config[\"hidden_layer_sizes\"], \n",
    "#             init_methode=config[\"init_w_method\"])\n",
    "\n",
    "#     # train model\n",
    "#     train_model(model, train_loader, test_loader,\n",
    "#                 loss_func=config[\"loss_func\"], \n",
    "#                 opt=config[\"opt\"], \n",
    "#                 lr=config[\"lr\"], \n",
    "#                 num_epochs=config[\"num_epochs\"], \n",
    "#                 id_name=config[\"name\"],\n",
    "#                 print_info=True, plot_eval=True, sound=True, write_wandb=send_data_to_wand)\n",
    "\n",
    "\n",
    "#     # Weights & Biases save Parameter\n",
    "#     if send_data_to_wand:\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLP** Parameter Suche mit wandb  \n",
    "Modelltraining Parametereinstellungen mit wandb sweep [Anleitung](https://docs.wandb.ai/ref/python/sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein gutes Modell müssen die Hyperparamter optimal gesetzt sein. Diese optimalen Parameter zu finden ist nicht einfach. Hier soll Schritt für Schritt das MLP-Modell verbessert werden. Folgende Anpassunge sollen ausgesucht werden:  \n",
    "* Anzahl hidden Layers erweitern\n",
    "* Umfang der hidden Layers vergrössern\n",
    "* Anpassung oder hinzufügen von Aktivierungsfunktionen\n",
    "* Anpassen der Lernrate\n",
    "* Trainingszeit erhöhen\n",
    "\n",
    "Um die Übersicht über all die unterschiedlichen Kombinationen von Hyperparameter nicht zu verlieren, wird das Tracking Tool [weights&biases]() mit der `wandb.sweep()` Configurations Funktion verwendet. Damit werden die Parameter schrittweise angepasst und jeweils ein Modell auf dem Dashboard von weights&biases abgebildet. Verschiedene Grafiken stehen danach zur Interpretation der Modell zur Verfügung.  \n",
    "\n",
    "---\n",
    "Folgend werden Phasen definiert um den Einfluss von unterschiedliche Kombinationen von Parameter zu testen. Die Parameter werden entsprechend im `sweep_configuration` festgehalten:\n",
    "1. Phase: Lernraten, Batchgrösse und Anzahl Layer sollen untersucht werden.\n",
    "1. Phase: Lernraten, Anzahl Layer und Layergrösse sollen untersucht werden\n",
    "1. Phase: Untersuchung der Initialisierung der Gewichte auf Layer\n",
    "1. Phase: Änderungen von Aktivierungsfunktionen\n",
    "1. Phase: Trainingszeiten für vielversprechende Modellparameter erhöhen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:**  \n",
    "Lernraten, Anzahl Layer sollen untersucht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sweep = False  # cuda:167min\n",
    "exp_name = \"mlp_ph1\"\n",
    "num_epochs = 20\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": exp_name,\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\"name\": {\"values\": [exp_name]},                    \n",
    "                    \"loss_func\": {\"values\": ['CrossEntropyLoss']},\n",
    "                    \"opt\": {\"values\": [\"SGD\"]},\n",
    "                    \"init_w_method\": {\"values\": [\"kaiming_norm\"]},  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "                    \"norm_mean\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"norm_std\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"num_epochs\": {\"values\": [num_epochs]},\n",
    "                    \"batchsize\": {\"values\": [64, 128, 256, 512]},\n",
    "                    \"hidden_layer_sizes\": {\"values\": [(32,), (32,32), (32,32,32), (32,32,32,32)]},\n",
    "                    \"lr\": {\"values\": [1e-3, 1e-4]}\n",
    "                    },\n",
    "}\n",
    "\n",
    "if train_sweep:\n",
    "    sweep_id = wandb.sweep(sweep_configuration, project=\"del_mc1_sweeptest\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_hlXX)\n",
    "    play_sound(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 2:** \n",
    "Lernraten, Anzahl Layer und Layergrösse sollen untersucht werden  \n",
    "* batchsize fix 64 (grösser als 128 deutet auf Overfitting)\n",
    "* Layergrössen aufsteigend erweitern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sweep = False  # cuda:112min\n",
    "exp_name = \"mlp_ph2\"\n",
    "num_epochs = 20\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": exp_name,\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\"name\": {\"values\": [exp_name]},                    \n",
    "                    \"loss_func\": {\"values\": ['CrossEntropyLoss']},\n",
    "                    \"opt\": {\"values\": [\"SGD\"]},\n",
    "                    \"init_w_method\": {\"values\": [\"kaiming_norm\"]},  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "                    \"norm_mean\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"norm_std\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"num_epochs\": {\"values\": [num_epochs]},\n",
    "                    \"batchsize\": {\"values\": [64, 128]},\n",
    "                    \"hidden_layer_sizes\": {\"values\": [(32,64), (64,128), (128,256), (32,64,128), (64,128,256)]},\n",
    "                    \"lr\": {\"values\": [1e-3, 1e-4]}\n",
    "                    },\n",
    "}\n",
    "\n",
    "if train_sweep:\n",
    "    sweep_id = wandb.sweep(sweep_configuration, project=\"del_mc1_sweeptest\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_hlXX)\n",
    "    play_sound(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sweep = False\n",
    "exp_name = \"mlp_ph2\"\n",
    "num_epochs = 20\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": exp_name,\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\"name\": {\"values\": [exp_name]},                    \n",
    "                    \"loss_func\": {\"values\": ['CrossEntropyLoss']},\n",
    "                    \"opt\": {\"values\": [\"SGD\"]},\n",
    "                    \"init_w_method\": {\"values\": [\"kaiming_norm\"]},  #['uniform', 'xavier', 'normal' ,'kaiming_norm' ]\n",
    "                    \"norm_mean\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"norm_std\": {\"values\": [(0.5, 0.5, 0.5)]},\n",
    "                    \"num_epochs\": {\"values\": [num_epochs]},\n",
    "                    \"batchsize\": {\"values\": [64]},\n",
    "                    \"hidden_layer_sizes\": {\"values\": [(32,64), (64,128), (128,256), (32,64,128), (64,128,256)]},\n",
    "                    \"lr\": {\"values\": [1e-3, 1e-4]}\n",
    "                    },\n",
    "}\n",
    "\n",
    "if train_sweep:\n",
    "    sweep_id = wandb.sweep(sweep_configuration, project=\"del_mc1_sweeptest\")\n",
    "    wandb.agent(sweep_id, function=train_mlp_hlXX)\n",
    "    play_sound(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN** Parameter Suche mit wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**2. Nutzen der Regularisierung**  \n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:  \n",
    "a. L1/L2 Weight Penalty  \n",
    "b. Dropout\n",
    "\n",
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung verschiedener Regularisierungsstärken. Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist (Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden). Inwiefern wird dieses Ziel im gegebenen Fall erreicht?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**3. Nutzen von Batchnorm BN (ohne REG, mit SGD)**  \n",
    "Evaluiere, ob Batchnorm etwas bringt. Beschreibe kurz, was die Idee von BN ist, wozu es helfen soll.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**4. Nutzen von Adam (ohne BN, ohne / mit REG)**   \n",
    "Evaluiere, ob Du mit Adam bessere Resultate erzielen kannst.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tipp:</b> Blaue Boxen .. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Erfolg:</b> Grüne Boxen (alert-success)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warnung:</b> Rote Boxen (alert-danger)\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e452ba78c6e1c5f07381d012e3449bb8aff006d5d5ee6ce630663b7f08ee76a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
